---
title: "IA_Flat_File_Aggregation"
author: "Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Preamble

## To Do List


## Questions for Data Provider:
- Pending info for mutliple outstanding items from Jeff Kopaska




##Instructions
This code is written in chunks that each accomplish a task moving towards the goal of a file for each state that encompasses all fish observations made/shared with us. The structure of that observation-level data should be one row per individual fish. By joining these data to the effort info, we will be able to filter and aggregate the data in flexible ways, always bringing along info on how much effort it took to catch each the fish obs (or set of fish).

The data explainer sticks .n suffixes on columns where multiple fields from one of the datastes has multiple cols that match that field (i.e., date.1, date.2, date.3). The naming conventions for cols uses "_" and no spaces.

After loading packages, the data from each state will be loaded into the WS and renamed according to the mapping of old colnames to new colnames in the data explainer. Next, the files should be explored a bit, and the script should identify files that will not be used, but instead get removed from the workspace. After this initial exploration fo what's there, the files should be restructured and munged into the obs-level format described above. When this is done, subsequent blocks should conduct some baseline additional QC should be done to verify the product of the munging is as-expected. Finally, the script should tidy up an remaining column or field formatting (e.g., species uses common names, no spaces, but "_"), and drop an unneeded columns. 


A basic guide to columns we expect to see in a observation level data are as follows:

LOCATION INFORMATION:
state - 
county - county associate with the wb in the state data
lake_name - common lang name of the lake
lake_id - usually a local id specific to the state contributing the data
nhdhr.id - This column is usually added towards the end of the script based on state lake_ids using the mwlaxeref (Paul Frater) package from here: https://drive.google.com/drive/u/1/folders/1HURmPTtufVzI0aqn7D8MpKdL5B8atCL5

SURVEY INFORMATION:
date_clean - usually multiple dates are submitted with each fish (e.g., collection date, survey end date). Use the date of the survey as the primary date for each fish observation, generating a date_clean column
survey_type - this is often specified in the data, and sometimes helps to filter out which data are useful for any given purpose (e.g., research survey, fishkill check)
survey_id - in some states this is a provided variable used as a key to each "survey." Ususally a "survey" is multiple gears on a single lake on a single date (often surveys might run multiple consecutive dates, but only one date is reported )
sampling_method - This is a gear field, and often includes wide ranging gears and sometimes very specific gears
total_effort.1 - This should be a numeric field with only the qty of effort
effort_units.1 - paired with total_effort.1, defines units for numeric
nothing_caught - specifies that nothing was caught in this effort (species will also be NA)
target_species - what was the species being targeted in the survey?
effort_ident - This is a field we add, it is a unique key for each effort unit that we have data for(usually a gear within a survey). For example, a data user could get cpue by counting all fish within a group_by(effort_id) or it's equivalent group_by(lake_id, date, survey_type,sampling_method) 

TAXA INFORMATION: 
species.1 - species common name
species_abbrev - State level code sometimes used in data share
length.1 - length of fish observed, numeric
length_unit.1 - units for length.1, also specify resolution if needed (e.g, cm, whole cm)
weight.1 - weight of fish obs, numeric
weight_unit.1 - units of weight.1, also specify resolution if needed (e.g, lb, whole lb)
sample_id.1 - unique id for each fish observation sometimes provided and sometimes useful for connecting to aged fish
age - age in years, numeric
aging_structure - what was used to determine age?
young_of_year - was the fish a YOY (i.e. hatched <365d before surveyed)
sex - sex of fish (male, female, unknown, NA)

SOURCE FILE INFORMATION: These columns come in with each dataset from the data explainer and we leave them in the product so that we could hunt down issues we find a bit more easily. 

original_file_name.1_effort - name of effort file that was used to generate data in this row
original_file_name.1_indivfish - name of individual fish file that was used to generate data in this row
original_file_name.1_[...]

FLAGS AND ISSUES:
flag - this column contains a character string with issues describing each row, each issue separated with a comma. Use mutate(flag = paste(flag, "new issue description", sep = ",")) to add to this column without overwriting other issues already specified.







##Libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(data.table)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)

options(scipen = 999)
```


##Data
This could readily be changed into a function that takes a filepath and returns files into environment.
* note Holly has to change file paths to "D" 
```{r}
#generate a file list to import
files_list <- list.files(path = "D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/IA_Data/ia_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list



#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/IA_Data/ia_raw_disaggregated_data/",
                                          files_list[i])))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  #if the file is not in the data explainer, don't try to rename it:
  if(filei %in% cde$new_file_name) {
    print("renaming with data explainer")
  } else {next}
  
  
  
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    data.table::transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 



```


#Obs Data (new)
##Data Review
```{r}

# We've got a new fish file from Iowa! It looks (kinda) like the old one, but they have added a crap-ton of info 


#fish obs
glimpse(ia_fishobs_30Nov2023)

#do these data contain any no_fish_caught surveys?
ia_fishobs_30Nov2023[total_count.1 == 0, .N] #Coool. 12k zeros! Later we'll see a bunch of these are erroneous.(Line 231 -  Line 303)

#but there is no lake ID
ia_fishobs_30Nov2023[, .N,secondary_lake_id] #this is not a lake id. It is ther Survey.SurveyID field (put here in data explainer to make import easier)
ia_fishobs_30Nov2023[, unique(site_id.1) , ]
ia_fishobs_30Nov2023[ , unique(lake_name.1) , ]
ia_fishobs_30Nov2023[ , length(unique(lat_unspec)) , .(lake_name.1,county) ][V1>1]
#looks like we can get there with a county-lake key

# They had a var called survey.SurveyID that we renamed to Secondary_Lake_ID -- this combines all of the "Survey.visitIDs"(surveyID) in a survey
ia_fishobs_30Nov2023[ , length(unique(secondary_lake_id)) , .(lake_name.1, county, lat_unspec, lon_unspec , date.1, survey_id) ][V1>1]
# and does survey_id (was "SurveyvisitID") parse gears? NO it does not. 
ia_fishobs_30Nov2023[ , length(unique(sampling_method.1)) , .(lake_name.1, county, lat_unspec, lon_unspec , date.1, survey_id, secondary_lake_id) ][V1>1]




a <- ia_fishobs_30Nov2023[lake_name.1 == "Ada Hayden Heritage Park Lake" & date.1 == "10/4/2010"  & survey_type.1 == "General" ]
  a[ , length(unique(secondary_lake_id)) , .(lake_name.1,county, date.1, survey_type.1 ) ][V1>1]

setcolorder(ia_fishobs_30Nov2023, c("lake_name.1", "county", "lat_unspec", "lon_unspec",
                                    "survey_type.1", "secondary_lake_id", "sampling_method.1", "sampling_method.2", "gear_data_notes.1", "date.1","start_time", "year", "site_id.1",
                                    "survey_id","total_effort_1", "total_effort_2", "total_count.1",
                                    "sample_id.1","notes.8","batch_count_ident","species.1" , "individual_count", "length.1", "weight.1", "batch_length", "batch_weight"
                                       ))  


### Erroneous zeros


# beacuse they drew together databases, there's a bit of craziness that happened with the survey_ident having 1 record that  says total_count = 0 (indicating that total_eff_ident has 0 indiv_fish observations), but one batch count record with N>0
ia_fishobs_30Nov2023[ , .N , sample_id.1 ][N==1]

b <- ia_fishobs_30Nov2023[ia_fishobs_30Nov2023[ , .N , sample_id.1 ][N==1], on = .(sample_id.1) , ]


#for each sample_id, show me sum(total_count)for and sum(individual_count)for batchrows and count(unique(total_effort)) and count unique(Fish.Measurement>ID)
ia_fishobs_30Nov2023[ , .(num_tot_eff_vals=length(unique(total_effort_1)), n_total_count_vals = length(unique(total_count.1)), summed_indiv_count = sum(individual_count, na.rm = T), length(unique(notes.8))),  , .(sample_id.1) ]
#any sample that has >1 tot eff?
ia_fishobs_30Nov2023[ , .(num_tot_eff_vals=length(unique(total_effort_1)), n_total_count_vals = length(unique(total_count.1)), summed_indiv_count = sum(individual_count, na.rm = T), length(unique(notes.8))),  , .(sample_id.1) ][num_tot_eff_vals>1]
#any sample with >2 total_count cal? NO, if yes would have indicated that there were more than just the batch count row probs
ia_fishobs_30Nov2023[ , .(num_tot_eff_vals=length(unique(total_effort_1)), n_total_count_vals = length(unique(total_count.1)), summed_indiv_count = sum(individual_count, na.rm = T), length(unique(notes.8))),  , .(sample_id.1) ][n_total_count_vals>2]

#### so to summarize what we think has happened here: the total_count value was assigned to each sample_id.1 based on the fish measurement table. That means that if only batch counted fish were pulled from a gear, it got a total_count that was 0 and a line in the data for that zero. TO rectify this, we want to find all total_count==0, where no batch_count data exist. For these there should be only one row (all fish came from the ind_fish data) 

      #if we eliminate the batch rows, does the problem go away? Yep!
      ia_fishobs_30Nov2023[ batch_count_ident == "" , .(num_tot_eff_vals=length(unique(total_effort_1)), n_total_count_vals = length(unique(total_count.1)), summed_indiv_count = sum(individual_count, na.rm = T), length(unique(notes.8))),  , .(sample_id.1) ][n_total_count_vals>1]
      
      #for sample.id's with only one total_count val, if the summed indiv count col == 0, that sample_id should be assigned a nothing_caught==T (note that total_count for these reocrds is NA)
      ia_fishobs_30Nov2023[   ,  n_sample_id.1_totalcount_vals := length(unique(total_count.1))  , sample_id.1 ]
      ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sample_id.1]
      ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sample_id.1][is.na(V2)] #this line is garbage
      ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sample_id.1][V2==0]# These are our zeros!!!!!!
      ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sample_id.1][sample_id.1 == "000490DE-1F58-4596-922E-F578C4118D8E"] #investigate a known record


##Funky Sample IDs?
#why some with no sample ID? Doesn't matter-- we have no effort data for these, and no species info-- I 
ia_fishobs_30Nov2023[sample_id.1 == "" , unique(species.1) ,]
ia_fishobs_30Nov2023[sample_id.1 == "" , unique(total_effort_1) ,]
ia_fishobs_30Nov2023[sample_id.1 == "" , unique(total_effort_2) ,]
ia_fishobs_30Nov2023[sample_id.1 == "" , unique(sampling_method.1) ,]
ia_fishobs_30Nov2023[sample_id.1 == "" , unique(sampling_method.2) ,]

#dump these:
ia_fishobs_30Nov2023 <- ia_fishobs_30Nov2023[!sample_id.1 == ""]

##Now rename some things for clarity's sake
#fix identifier names:
setnames(ia_fishobs_30Nov2023, old = c("secondary_lake_id", "survey_id", "sample_id.1", "notes.8", "batch_count_ident"),
         new = c("survey_id", "total_effort_ident", "sub_effort_ident", "fish_ident", "batch_count_ident"))
#now peel garbage out of ifsh_ident col
ia_fishobs_30Nov2023[ ,fish_ident := gsub("Fish.MeasurementId:", "", fish_ident) , ]
ia_fishobs_30Nov2023[fish_ident == "", fish_ident := NA]
#check others for ""
ia_fishobs_30Nov2023[sub_effort_ident == ""]
  ia_fishobs_30Nov2023[is.na(sub_effort_ident)]
ia_fishobs_30Nov2023[total_effort_ident == ""]
  ia_fishobs_30Nov2023[is.na(total_effort_ident)]
ia_fishobs_30Nov2023[survey_id == ""]
  ia_fishobs_30Nov2023[is.na(survey_id)]


#okay, now stick in a sub_effort_nothing_caught
      
      #must deal with zero count batches:  
      ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sub_effort_ident][,summary(summed_indiv_count)]#make sure there are no NAs in here
      ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sub_effort_ident][summed_indiv_count == 0, V2]
      #are there any cases where the batch count record is there but was a zero?
      ia_fishobs_30Nov2023[n_sample_id.1_totalcount_vals == 2 & !batch_count_ident == "" & individual_count == 0, .N , sub_effort_ident]#yes
      #do all of these have another row holding them in?
      ia_fishobs_30Nov2023[n_sample_id.1_totalcount_vals == 2 & !batch_count_ident == "" & individual_count == 0, .( n_unique_batch_vals = length(unique(individual_count)) , n_batch_rows = .N ,n_sample_id.1_totalcount_vals) , sub_effort_ident]
      # Yes they do
      
      
      #peel off sub_eff IDs: 
      ia_fishobs_30Nov2023[n_sample_id.1_totalcount_vals == 2 & !batch_count_ident == "" & individual_count == 0, .( n_unique_batch_vals = length(unique(individual_count)) , n_batch_rows = .N ,n_sample_id.1_totalcount_vals) , sub_effort_ident][, sub_effort_ident]
      
      #see if all of these have a non-batch record: 
      match( ia_fishobs_30Nov2023[n_sample_id.1_totalcount_vals == 2 & !batch_count_ident == "" & individual_count == 0, .( n_unique_batch_vals = length(unique(individual_count)) , n_batch_rows = .N ,n_sample_id.1_totalcount_vals) , sub_effort_ident][, sub_effort_ident],
             ia_fishobs_30Nov2023[ batch_count_ident == "" , sub_effort_ident]
      )
      #confirm that there are not any just batch count records fo sub_eff:
      ia_fishobs_30Nov2023[n_sample_id.1_totalcount_vals == 1 & !batch_count_ident == "" & individual_count == 0, .N ,] 
      
      #All of this means we can drop these 0 batch count records
      ia_fishobs_30Nov2023 <- ia_fishobs_30Nov2023[!(!batch_count_ident == "" & individual_count == 0),] 

#back to the sub_eff_ident_nothing_caught:

ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sub_effort_ident][,summary(summed_indiv_count)]#make sure there are no NAs in here
ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1 , .("summed_indiv_count" = sum(individual_count, na.rm = T), unique(total_count.1)) , sub_effort_ident]
ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1, .N , is.na(total_count.1) ]
ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1, .N , total_count.1 == 0 ]
ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals == 1, sub_effort_nothing_caught := total_count.1 == 0]
ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals != 1, sub_effort_nothing_caught := FALSE ]
ia_fishobs_30Nov2023[ n_sample_id.1_totalcount_vals < 1 , .N]
  ia_fishobs_30Nov2023[ , .N ,  n_sample_id.1_totalcount_vals]
  ia_fishobs_30Nov2023[ , .N, sub_effort_nothing_caught]



#now check out the cases where all batch counted fish were caught, and no indiv. fish records exist
  #rebuild this column after dropping junk out:
ia_fishobs_30Nov2023[   ,  n_sample_id.1_totalcount_vals := length(unique(total_count.1))  , sub_effort_ident ]
ia_fishobs_30Nov2023[n_sample_id.1_totalcount_vals > 1 , "" %in% unique(batch_count_ident) == T ,  sub_effort_ident][ , .N , V1]

ia_fishobs_30Nov2023[ , .("n_ind_fish" = length(unique(fish_ident)), "n_batch_counts" = length(unique(batch_count_ident))) , sub_effort_ident  ]

ia_fishobs_30Nov2023[ , .N , is.na(fish_ident) ]
ia_fishobs_30Nov2023[ , .N , is.na(batch_count_ident) ]
ia_fishobs_30Nov2023[ , .N , batch_count_ident == "" ]
ia_fishobs_30Nov2023[ , .N , .(fish_ident_na = is.na(fish_ident), (batch_countid_blank = batch_count_ident == ""), (sub_effort_nothing_caught))]
#drop those goofy imported rows from the ind_fish table:
# thats where sub_nothing_caught == F, fish_id is na, and batch_count is blank
ia_fishobs_30Nov2023[sub_effort_nothing_caught == F & is.na(fish_ident) & batch_count_ident == "", .N , sub_effort_ident][N>1] #there should only be one row per sub if this is how they were generated...
a <- ia_fishobs_30Nov2023[sub_effort_nothing_caught == F & is.na(fish_ident) & batch_count_ident == "", .N , sub_effort_ident][ , sub_effort_ident ] #every one of these sub eff idents should have one or more batch ID too
any(is.na(match(a, 
      ia_fishobs_30Nov2023[batch_count_ident != "", sub_effort_ident]))) # no unmatched survey idents means drop those beeeeatches
ia_fishobs_30Nov2023 <- ia_fishobs_30Nov2023[!(sub_effort_nothing_caught == F & is.na(fish_ident) & batch_count_ident == ""),  ,]
  ia_fishobs_30Nov2023[batch_count_ident == "", batch_count_ident := NA]  
  ia_fishobs_30Nov2023[ , .N , .(fish_ident_na = is.na(fish_ident), (batch_countid_na = is.na(batch_count_ident)), (sub_effort_nothing_caught))]

#so at this point we have eliminated all non true-zero fish and batch entries. 

#can uncount the batch fish
  
  uncount(ia_fishobs_30Nov2023[!is.na(batch_count_ident)], weights = individual_count, .remove = TRUE, .id = "uncount_ident")
ia_fishobs <- rbindlist(list(ia_fishobs_30Nov2023[is.na(batch_count_ident)],
                        uncount(ia_fishobs_30Nov2023[!is.na(batch_count_ident)], weights = individual_count, .remove = TRUE, .id = "uncount_ident")),
                        fill = TRUE)  

#now total_effort_idents exist, but they are no good(have multi-gear contents), but the total efforts within are actual sub_efforts
ia_fishobs[ , length(unique(sampling_method.1)) , total_effort_ident ][ , summary(V1) ]
ia_fishobs[!is.na(total_effort_1) , length(unique(total_effort_1)) , total_effort_ident ][ , summary(V1) ]
ia_fishobs[!is.na(total_effort_2) , length(unique(total_effort_2)) , total_effort_ident ]
#see here, these efforts belong to a sub_effort in a clean way:
ia_fishobs[ , length(unique(total_effort_1)) , sub_effort_ident ][ , summary(V1) ]
ia_fishobs[ , length(unique(total_effort_2)) , sub_effort_ident ][ , summary(V1) ]


#as such, total_effort_1&2 are actually a sub_effort_ident
  setnames(ia_fishobs, old = c("total_effort_1", "total_effort_2"),new = c("sub_effort_1", "sub_effort_2") )
ia_fishobs[ , `:=` (sub_effort_1_units = "hours", sub_effort_2_units = "distance") ,]


#now for each total_effort_ident, let's calc a total_effort_1 & 2
ia_fishobs[   ,   ,  ]


#naming reminder
          # as submitted =   "Survey.SurveyID", "Survey.SurveyVisitID", "Sample.SampleId", "FishmeasurementID", "Fish.BatchcountID"
#          on import = c("secondary_lake_id", "survey_id", "sample_id.1", "notes.8", "batch_count_ident"),
#          new = c("survey_id", "total_effort_ident", "sub_effort_ident", "fish_ident", "batch_count_ident"))



#now calc effort for each total_effort_ident (and generate a new total_effort_ident)
ia_fishobs[ , length(unique(sub_effort_ident)) , .(total_effort_ident)  ][ , summary(V1) ]
ia_fishobs[ , length(unique(sampling_method.1)) , .(total_effort_ident)  ][ , summary(V1) ]

# make a tot eff ident
#check key to make:
ia_fishobs[ , length(unique(lake_name.1)) ,  .(survey_id, sampling_method.1)]
ia_fishobs[ , paste(unique(sampling_method.2)) ,  .(survey_id, sampling_method.1)]

# gotta fix sampling method before we can make new tot eff idents:
gear_corrections <- fread(file = "IA_gear_notes.csv")
#rename these and tidy up those columns  a bit:
#gear data notes
        # For the gear data notes, we should NA everything that has :NA in it, then collapse into a single gear data notes col 
        
        #make loop to do this task (need to add code here to drop blanks too[try word(str, 2,  == "") ] )
        cols = paste(rep("gear_data_notes",28), (1:28), sep = "." )
        
        ia_fishobs[ , .SD, .SDcols = cols ]
        #NAs
        for (col in cols) {
          #col=cols[12]
          # ia_fishobs[str_detect(eval(parse(text = col)),"NA"), (col) := NA ,]
          print(ia_fishobs[str_detect(eval(parse(text = col)),"NA"), .N , by = eval((col)) ])#prints whats getting NA'd
          ia_fishobs[str_detect(eval(parse(text = col)),"NA"), (col) := NA ] #do the NA assignment
        }
        #blanks -- vectorized strategy courtesy of ChatGPT
        for (col in cols) {
          # Create a logical vector indicating where the condition is met
          condition <- ia_fishobs[, word(eval(parse(text = col)), 2, sep = fixed(":")) == ""]
        
          # Print what's getting NA'd
          print(ia_fishobs[condition, .N, by = eval(parse(text = col))])
        
          # Do the NA assignment efficiently
          ia_fishobs[condition, (col) := NA]
        }

#check to make sure guts of these cols jive with the gear_corrections table
        
# now make their names jive:        
        names(gear_corrections)
        ia_fishobs[ , .SD, .SDcols = cols ][1]
        
        # Function to extract the first word from a string
        extract_first_word <- function(x) {
          non_na_values <- na.omit(x)
          if (length(non_na_values) > 0) {
            first_word <- strsplit(as.character(non_na_values[1]), " ")[[1]][1]
            return(first_word)
          }
          return(NULL)
        }
        
        # Loop through each column and update column name
        for (col in cols) {
          first_word <- extract_first_word(ia_fishobs[[col]])
          if (!is.null(first_word)) {
            setnames(ia_fishobs, col, first_word)
          }
        }

      # Function to strip the first word from a string
      strip_first_word <- function(x) {
        if (!is.na(x)) {
          stripped_string <- sub("^[^:]+:", "", as.character(x))
          return(stripped_string)
        }
        return(x)
      }
      
      # Loop through each column and update values
      cols_2 = names(ia_fishobs)[str_detect(names(ia_fishobs), ":")]
      
      for (col in cols_2) {
        ia_fishobs[, (col) := lapply(.SD[[col]], strip_first_word)]
      }
      
      #fix column names
      names(ia_fishobs) <- word(names(ia_fishobs), 1, sep = fixed(":"))
      
      cols_3 <- word(cols_2, 1, sep = fixed(":"))
      
      #bring in the new gear type names:
      names(gear_corrections) %in% c(names(ia_fishobs))
      
      setkeyv(gear_corrections, cols_3)
      # ia_fishobs[ , Sample.GearTypeDesc := as.character(Sample.GearTypeDesc) , ]
      # ia_fishobs[ , Sample.Depth := as.character(Sample.Depth) , ]
      ia_fishobs[,(cols_3):= lapply(.SD, as.character), .SDcols = cols_3]
      setkeyv(ia_fishobs, cols_3)
      
      gear_corrections[,(cols_3):= lapply(.SD, as.character), .SDcols = cols_3]
      gear_corrections[gear_corrections==''] <- NA
      
      
      
      ia_fishobs[gear_corrections, on = cols_3, consolidated_gear := `Consolidated Gear`  , ]       
        
      ia_fishobs[ , .N , consolidated_gear]
      
      ia_fishobs[is.na(consolidated_gear), .N , .(sampling_method.1,sampling_method.2) ]
      ia_fishobs[is.na(consolidated_gear), consolidated_gear := paste(sampling_method.2, "Unspecified", sep = " ") ]
      ia_fishobs[ , .N , consolidated_gear]
      
#Now we have improved gear names, generate new total effort idents:
      
      ia_fishobs[ , length(unique(consolidated_gear)) , sub_effort_ident  ]
      #checkdate
      ia_fishobs[ , length(unique(date.1)) , .(survey_id) ][V1>1]
      ia_fishobs[ ,date.1 := as.IDate(date.1, format = "%m/%d/%Y") ,]
      #set new survey level date
      ia_fishobs[  , survey_start_date := min(date.1)  , survey_id  ]
      
      ia_fishobs[ , .N , .(lake_name.1, survey_start_date, survey_id, consolidated_gear )]
      
      ia_fishobs[ , visit_ID := total_effort_ident , ]
      ia_fishobs[ , total_effort_ident := .GRP , .(lake_name.1, survey_start_date, survey_id, consolidated_gear )]

      ia_fishobs[ , .N , total_effort_ident ]
      
      
      tot_eff_1 <- ia_fishobs[ , .N ,
                               .(lake_name.1, survey_start_date, survey_id, consolidated_gear,total_effort_ident,
                                 sub_effort_ident, sub_effort_1,sub_effort_2, sub_effort_nothing_caught)  ][ ,
                                                                                                             .("total_eff_1" = sum(sub_effort_1),
                                                                                                               "total_eff_2" = sum(sub_effort_2),
                                                                                                               "total_eff_nc" = all(sub_effort_nothing_caught)),
                                                                                                             .(total_effort_ident)]
      
ia_fishobs[tot_eff_1, on = ("total_effort_ident"), `:=` ("total_effort_1" = total_eff_1, "total_effort_2" = total_eff_2, "total_effort_nothing_caught" = total_eff_nc) , ]
ia_fishobs[ , `:=` (total_effort_1_units = "hours", total_effort_2_units = "distance") ,]
rm(tot_eff_1)

#check our work
ia_fishobs[ , length(unique(total_effort_1)) , total_effort_ident ][V1>1]
ia_fishobs[ , length(unique(consolidated_gear)) , total_effort_ident ][V1>1]


#review the nothing caught tot effs
ia_fishobs[total_effort_nothing_caught == T]

### Manage binned lengths:

ia_fishobs[ , .N ,  .(batch_length)]
ia_fishobs[ !is.na(length.1) & batch_length != "", .N , .(batch_length,length.1) ]
#any ranges in herE?
ia_fishobs[ , unique(length.1)]
ia_fishobs[ str_detect(length.1, " "), .N , ]
ia_fishobs[ str_detect(length.1, "-"), .N , ]

ia_fishobs[ , unique(batch_length) , ]

#move age_class to its own column
ia_fishobs[batch_length %in% c("Young of Year","Adult","Sub-Adult"), .N , batch_length ]
ia_fishobs[batch_length %in% c("Young of Year","Adult","Sub-Adult"), `:=` (age_class = batch_length, batch_length = NA) ]
#make blanks into NAs
ia_fishobs[batch_length %in% c(""), batch_length := NA ]

#move these into the decided schema:
# Decided that lengths given as bins will be retained in a length_bin column, length_bin_units

ia_fishobs[!is.na(batch_length), `:=` (length_bin = batch_length, length_bin_unit = "inches") , ]

ia_fishobs[ , batch_length := NULL ]

#check for weight situation (Keep as-is, from cde we see that the weights are total weight in lbs)
ia_fishobs[ , unique(batch_weight) , ]
#name unit explicitly
ia_fishobs[!is.na(batch_weight) , batch_weight_unit := "pounds" , ]



# Dates?
ia_fishobs[ , .N , .(date.1, lake_name.1, county,  sampling_method.1)] #check the date cleaning
ia_fishobs[ , summary(as.IDate(date.1,  format = "%m/%d/%Y" )) , ]#whoa... the years actually seem reasonable...
ia_fishobs[ ,date.1 := as.IDate(date.1, format = "%m/%d/%Y") ,]

#attempt to recover the Lake IDs from the last dataset?
ia_fishobs[ , .N ,  .(date.1, lake_name.1, county,  sampling_method.1, lat_unspec, lon_unspec)][ is.na(lat_unspec), unique(lake_name.1)]

# Ignoring the looping back in of any ages that are in here vs. in previous files. 
# #ages?
# ia_fishobs[!is.na(age), .N , species.1] 
# 
# 
# ia_age_length_21Aug2021[ , .N , species.1 ]
# ia_BLG_age_length_21Aug2021[ , .N , species.1]
# ia_CCF_age_length_21Aug2021[ , .N , ]
# 
# ia_CCF_age_length_21Aug2021[ , species.1 := "CCF" ,]
# ia_BLG_age_length_21Aug2021[ , species.1 := "BLG"  ,]
# 
# ia_BLG_age_length_21Aug2021[  ,  , .(lake_name.1, yea) ]
# ia_fishobs[str_detect() , , ]
# 

```


## Restructure the fish obs
```{r}
#reorganize this beast:
names(ia_fishobs)
names(ia_fishobs)[str_detect(names(ia_fishobs), "notes")]

#notes
cols = paste(rep("notes",7), (1:7), sep = "." )


ia_fishobs[ , unique(notes.7)] # junk
ia_fishobs[ , unique(notes.6)] #KEEPER - Team Sort
ia_fishobs[ , unique(notes.5)] #KEEPER - Flag
ia_fishobs[ , unique(notes.4)] #KEEEPR - Flag
ia_fishobs[ , unique(notes.3)] #KEEPER - Flag
ia_fishobs[ , unique(notes.2)] #KEEPER - Flag and request clarification
ia_fishobs[ , unique(notes.1)] #KEEPER - Team Sort



# QC flags retained
ia_fishobs[ notes.2 == "Sample.IsSampleQualityAdequate:0", flag := "Agency flagged sample quality not adequate",
            ]
ia_fishobs[ notes.3 == "Sample.SamplingIssue:1", flag := ifelse(is.na(flag),
                                                                          "Agency flagged sample issue" ,
                                                                          fpaste(flag, "Agency flagged sample issue", sep = ";" )
                                                                          ),
            ]
ia_fishobs[ notes.4 != "Sample.IssueCategory:", flag := ifelse(is.na(flag),
                                                                          notes.4 ,
                                                                          paste(flag, notes.4, sep = ";" )
                                                                          ),
            ]
ia_fishobs[ notes.5 == "Sample.GearIssue:1", flag := ifelse(is.na(flag),
                                                                          notes.5 ,
                                                                          paste(flag, notes.5, sep = ";" )
                                                                          ),
            ]
#drop what needs dropping
cols = c("notes.7", "notes.5", "notes.4", "notes.3", "notes.2")
ia_fishobs[ , (cols) := NULL , ]


#what cols go out to the team? (We'll carry these forward & round up at the end)
ia_fishobs[ , .("records" = .N, "surveys" = length(unique(total_effort_ident))) , notes.6]
ia_fishobs[ , .("records" = .N, "surveys" = length(unique(total_effort_ident))) , notes.1]


# garbage bin notes

cols = paste(rep("garbage_bin_notes",12), (1:12), sep = "." )

ia_fishobs[ , unique(garbage_bin_notes.12)]# junk
ia_fishobs[ , unique(garbage_bin_notes.11)]# junk
ia_fishobs[ , unique(garbage_bin_notes.10)]# junk
ia_fishobs[ , unique(garbage_bin_notes.9)] # team sort
ia_fishobs[ , unique(garbage_bin_notes.8)] # junk
ia_fishobs[ , unique(garbage_bin_notes.7)] # junk
ia_fishobs[ , unique(garbage_bin_notes.6)] # junk
ia_fishobs[ , unique(garbage_bin_notes.5)] # junk
ia_fishobs[ , unique(garbage_bin_notes.4)] # Flag & request clarification
ia_fishobs[ , unique(garbage_bin_notes.3)] # junk
ia_fishobs[ , unique(garbage_bin_notes.2)] # junk
ia_fishobs[ , unique(garbage_bin_notes.1)] # junk

# QC flags retained
ia_fishobs[ garbage_bin_notes.4 == "Survey.State:Rejected", flag := ifelse(is.na(flag),
                                                                          "Agency flagged survey rejected" ,
                                                                          fpaste(flag, "Agency flagged survey rejected", sep = ";" )
                                                                          ),
            ]

#drop junk
cols = cols[!cols %in% c("garbage_bin_notes.9")]
ia_fishobs[ , (cols) := NULL]






#gear data notes
# For the gear data notes, we should NA everything that has :NA in it, then collapse into a single gear data notes col 

#This is moved way up in data review section
# #make loop to do this task (need to add code here to drop blanks too[try word(str, 2,  == "") ] )
# cols = paste(rep("gear_data_notes",28), (1:28), sep = "." )
# 
# ia_fishobs[ , .SD, .SDcols = cols ]
# #NAs
# for (col in cols) {
#   #col=cols[12]
#   # ia_fishobs[str_detect(eval(parse(text = col)),"NA"), (col) := NA ,]
#   print(ia_fishobs[str_detect(eval(parse(text = col)),"NA"), .N , by = eval((col)) ])#prints whats getting NA'd
#   ia_fishobs[str_detect(eval(parse(text = col)),"NA"), (col) := NA ] #do the NA assignment
# }
# #blanks -- vectorized strategy courtesy of ChatGPT
# for (col in cols) {
#   # Create a logical vector indicating where the condition is met
#   condition <- ia_fishobs[, word(eval(parse(text = col)), 2, sep = fixed(":")) == ""]
# 
#   # Print what's getting NA'd
#   print(ia_fishobs[condition, .N, by = eval(parse(text = col))])
# 
#   # Do the NA assignment efficiently
#   ia_fishobs[condition, (col) := NA]
# }


#shove all into one column as a long string
ia_fishobs[, gear_data_notes_1 := apply(.SD, 1, function(row) {
  non_na_elements <- na.omit(row)
  paste(names(non_na_elements), ":", non_na_elements, collapse = "; ")
}), .SDcols = cols_3]

# Collapse non-NA values into a new field by row
ia_fishobs[ , unique(gear_data_notes_1)][100:115] #review new string column
# ia_fishobs[ , (cols) := NULL ]

#This data product is brought back in up above. 
# 
# cols <- c(cols, "sampling_method.1", "sampling_method.2")
# 
# #cols to ship to team as a df:
#         #what cols go out to the team? (We'll carry these forward & round up at the end)
#         dat <- ia_fishobs[ , .("records" = .N, "surveys" = length(unique(total_effort_ident))) , by= eval((cols))]
#         
#         # Function to extract the first word from a string
#         extract_first_word <- function(x) {
#           non_na_values <- na.omit(x)
#           if (length(non_na_values) > 0) {
#             first_word <- strsplit(as.character(non_na_values[1]), " ")[[1]][1]
#             return(first_word)
#           }
#           return(NULL)
#         }
#         
#         # Loop through each column and update column name
#         for (col in names(dat)[1:28]) {
#           first_word <- extract_first_word(dat[[col]])
#           if (!is.null(first_word)) {
#             setnames(dat, col, first_word)
#           }
#         }
# 
#       # Function to strip the first word from a string
#       strip_first_word <- function(x) {
#         if (!is.na(x)) {
#           stripped_string <- sub("^[^:]+:", "", as.character(x))
#           return(stripped_string)
#         }
#         return(x)
#       }
#       
#       # Loop through each column and update values
#       for (col in names(dat)[1:28]) {
#         dat[, (col) := lapply(.SD[[col]], strip_first_word)]
#       }
#       
#       #fix column names
#       names(dat) <- word(names(dat), 1, sep = fixed(":"))
#       
#       # Print the updated data.table#write to file
#       
#       fwrite(dat, file = "Data_and_Scripts/Data/output/IA_gear_notes.csv")
#       
#       rm(dat, a, cols, col, condition)
# 
# cols = paste(rep("gear_data_notes",28), (1:28), sep = "." )

ia_fishobs[ , (cols_3) := NULL]
      
      
# mark recap notes

names(ia_fishobs)[str_detect(names(ia_fishobs), "notes")]

#notes
cols = paste(rep("mark_recap_data_notes",11), (1:11), sep = "." )


ia_fishobs[ , unique(mark_recap_data_notes.1)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.2)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.3)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.4)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.5)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.6)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.7)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.8)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.9)]  # junk
ia_fishobs[ , unique(mark_recap_data_notes.10)] # junk
ia_fishobs[ , unique(mark_recap_data_notes.11)] # junk

ia_fishobs[ , (cols) := NULL ]


# aging data notes

names(ia_fishobs)[str_detect(names(ia_fishobs), "notes")]

#notes
cols = paste(rep("aging_data_notes",4), (1:4), sep = "." )


ia_fishobs[ , unique(aging_data_notes.1)]  # junk, redundant
  ia_fishobs[ word(aging_data_notes.1, 2, sep = ":") != "", .N , .("ageatcap" = word(aging_data_notes.1, 2, sep = ":"), length.1)  ][ , .N , .(is.na(length.1), !is.na(ageatcap)) ] #one record--who cares
ia_fishobs[ , unique(aging_data_notes.2)]  # junk, not true--we've dumped all the BCAL ages from the data
ia_fishobs[ , unique(aging_data_notes.3)]  # junk
ia_fishobs[ , unique(aging_data_notes.4)]  # junk

ia_fishobs[ , (cols) := NULL ]



#check progress on notes:
names(ia_fishobs)[str_detect(names(ia_fishobs), "notes")]

#round up everything headed to team meeting
#gear data notes are done, now draw in the flags and deletes:

#what cols go out to the team? (We'll carry these forward & round up at the end)
        dat <- ia_fishobs[ , .("records" = .N, "surveys" = length(unique(total_effort_ident))) , by= .(notes.1, notes.6, garbage_bin_notes.9)]
        
        # Function to extract the first word from a string
        extract_first_word <- function(x) {
          non_na_values <- na.omit(x)
          if (length(non_na_values) > 0) {
            first_word <- strsplit(as.character(non_na_values[1]), " ")[[1]][1]
            return(first_word)
          }
          return(NULL)
        }
        
        # Loop through each column and update column name
        for (col in names(dat)[1:3]) {
          first_word <- extract_first_word(dat[[col]])
          if (!is.null(first_word)) {
            setnames(dat, col, first_word)
          }
        }

      # Function to strip the first word from a string
      strip_first_word <- function(x) {
        if (!is.na(x)) {
          stripped_string <- sub("^[^:]+:", "", as.character(x))
          return(stripped_string)
        }
        return(x)
      }
      
      # Loop through each column and update values
      for (col in names(dat)[1:3]) {
        dat[, (col) := lapply(.SD[[col]], strip_first_word)]
      }
      
      #fix column names
      names(dat) <- word(names(dat), 1, sep = fixed(":"))
      
      # Print the updated data.table#write to file
      
      # fwrite(dat, file = "Data_and_Scripts/Data/output/IA_comments_notes.csv")
      
      rm(dat, cols, col)

names(ia_fishobs)      
   # implement Jack's flags and deletions
    notes_corrections <- fread(file = "IA_comments_notes.csv")
    
#tidy up the iowa data to match for filter on jack's key
        cols_4 <- c("notes.1", "notes.6", "garbage_bin_notes.9")
    
    ia_fishobs[ , .SD , .SDcols = cols_4 ]

        # Function to extract the first word from a string
        extract_first_word <- function(x) {
          non_na_values <- na.omit(x)
          if (length(non_na_values) > 0) {
            first_word <- strsplit(as.character(non_na_values[1]), " ")[[1]][1]
            return(first_word)
          }
          return(NULL)
        }
        
        
        # Loop through each column and update column name
        for (col in cols_4) {
          first_word <- extract_first_word(ia_fishobs[[col]])
          if (!is.null(first_word)) {
            setnames(ia_fishobs, col, first_word)
          }
        }

      # Function to strip the first word from a string
      strip_first_word <- function(x) {
        if (!is.na(x)) {
          stripped_string <- sub("^[^:]+:", "", as.character(x))
          return(stripped_string)
        }
        return(x)
      }
      
      #fix column names
      names(ia_fishobs) <- word(names(ia_fishobs), 1, sep = fixed(":"))
      
      cols_5 <- c("Survey.Notes", "Sample.Notes", "Survey.Comment")
      # Loop through each column and update values
      for (col in cols_5) {
        ia_fishobs[, (col) := lapply(.SD[[col]], strip_first_word)]
      }
      
      
      # Fill NAs, set Keys
      ia_fishobs[,(cols_5):= lapply(.SD, as.character), .SDcols = cols_5]
      notes_corrections[,(cols_5):= lapply(.SD, as.character), .SDcols = cols_5]
      
      #bring in the new gear type names:
      names(notes_corrections) %in% c(names(ia_fishobs))
      
      
      setkeyv(notes_corrections, cols_5)
      # ia_fishobs[ , Sample.GearTypeDesc := as.character(Sample.GearTypeDesc) , ]
      # ia_fishobs[ , Sample.Depth := as.character(Sample.Depth) , ]
      setkeyv(ia_fishobs, cols_5)
      
      notes_corrections[notes_corrections==''] <- NA
      
      #avoiding name amtching or issues
      setnames(notes_corrections, "flag" , "new_flag")
      
      ia_fishobs[notes_corrections,  on = cols_5 , `:=` (delete = delete, target_species = `Target Species`, new_flag = new_flag)  ]   
      
      
      #execute these marks:
      ia_fishobs[ ,.N , .(delete, target_species, new_flag) ]
        ia_fishobs <- ia_fishobs[is.na(delete)]
      
      ia_fishobs[ new_flag == "gear issue", flag := ifelse(is.na(flag),
                                                                          "gear issue" ,
                                                                          fpaste(flag, "gear issue", sep = ";" )
                                                                          ),
            ]
      #shove all into one column as a long string
      ia_fishobs[, notes_1 := apply(.SD, 1, function(row) {
        non_na_elements <- na.omit(row)
        paste(names(non_na_elements), ":", non_na_elements, collapse = "; ")
      }), .SDcols = cols_5]
      
      ia_fishobs[ , .N , .(notes_1) ]
      
      ia_fishobs[ ,(cols_5) := NULL]
      ia_fishobs[ ,c("delete", "new_flag") := NULL]

#clean up some column names 
ia_fishobs <- ia_fishobs %>% 
  clean_names()

#fixing species names
ia_fishobs <- ia_fishobs %>% 
  mutate(species_clean = tolower(gsub(" ", "_", species_1))) %>% 
  mutate(species_clean = case_when(species_1 == "" ~ NA,
                                   TRUE ~ species_clean))
ia_fishobs %>% 
  group_by(species_clean) %>% 
  count() %>% 
  print(n = nrow(.))

#Exclude rivers and streams
ia_fishobs[ ,.N , lake_type]
ia_fishobs <- ia_fishobs[!(lake_type %in% c("Trout Stream", "River"))]

#cleaning up some unit columns
ia_fishobs <- ia_fishobs %>% 
  mutate(weight_unit_1_clean = str_replace(weight_unit_1, ".*Pounds", "Pounds"),
         length_unit_1_clean =  str_replace(length_unit_1, ".*Inches", "Inches"))
ia_fishobs %>% 
  group_by(weight_unit_1,
           length_unit_1,
           weight_unit_1_clean,
           length_unit_1_clean) %>% 
  count()

#what do these dates look like?
dates <- ia_fishobs %>% 
  distinct(total_effort_ident, sub_effort_ident, .keep_all = T) %>% 
  select(survey_id, total_effort_ident, sub_effort_ident, date_1, start_time, survey_start_date, sampling_method_1)
different <- ia_fishobs %>% 
  distinct(total_effort_ident, sub_effort_ident, .keep_all = T) %>% 
  filter(date_1 != survey_start_date) %>% 
  select(survey_id, total_effort_ident, sub_effort_ident, date_1, start_time, survey_start_date, sampling_method_1)
#survey start date will be applyed to the "date_survey" to denote when the first sampling happened in a lake-year
#date_1 is the sub_effort_ident
#total effort ident date will be generated by taking the first date from a total effort ident grouping
#no sample date to be retained

#sampling methods
sampling_methods <- ia_fishobs %>% 
  distinct(total_effort_ident, .keep_all = T) %>% 
  select(total_effort_ident, consolidated_gear, sampling_method_1, sampling_method_2)
#I will take sampling_method_1 as the first sampling method 
#I will then take consolidated gear as sampling method 2 - more refined year type


#Denver's attempt to clean up columns
ia_fishobs_trial <- ia_fishobs %>% 
  mutate(lake_name = lake_name_1,
         lake_id = as.character(NA),
         nhdhr_id = as.character(NA),
         month = month(date_1),
         survey_type = survey_type_1,
         survey_type_2 = as.character(NA),
         survey_type_3 = as.character(NA),
         survey_type_4 = as.character(NA),
         sampling_method_2 = consolidated_gear,
         sampling_method = sampling_method_1,
         gear_data_notes = gear_data_notes_1,
         target_species_2 = as.character(NA),
         total_effort_3 = as.numeric(NA),
         total_effort_3_units = as.character(NA),
         water_temp = as.numeric(NA),
         water_temp_units = as.character(NA),
         water_clarity = as.numeric(NA),
         water_clarity_units = as.character(NA),
         lat_end = as.numeric(NA),
         lon_end = as.numeric(NA),
         site_id = site_id_1,
         species_1 = species_clean,
         length_unit_1 = length_unit_1_clean,
         aging_structure_2 = as.character(NA),
         weight_unit_1 = weight_unit_1_clean,
         batch_weight = as.character(batch_weight),
         original_file_names = original_file_name_1,
         ind_fish_ident = fish_ident,
         lakesize = c_lakesize,
         lakesize_units = c_lakesize_units,
         area_group = as.character(NA),
         waterbody_type = lake_type,
         location_notes_1 = as.character(NA),
         obs_id = as.character(row_number())
         ) %>% 
  #fixing dates
  mutate(date_survey = survey_start_date,
         date_sub_effort_ident = date_1,
         date_sample = as.Date(NA)) %>% 
  group_by(total_effort_ident) %>% 
  mutate(date_total_effort_ident = first(date_1)) %>% 
  select(state,
         county,
         lake_name,
         lake_id,
         nhdhr_id,
         date_survey,
         date_total_effort_ident,
         date_sub_effort_ident,
         date_sample,
         year,
         month,
         survey_id,
         survey_type,
         survey_type_2,
         survey_type_3,
         survey_type_4,
         sampling_method,
         sampling_method_2,
         gear_data_notes,
         target_species,
         target_species_2,
         total_effort_ident,
         total_effort_1,
         total_effort_2,
         total_effort_3,
         total_effort_1_units,
         total_effort_2_units,
         total_effort_3_units,
         total_effort_nothing_caught,
         water_temp,
         water_temp_units,
         water_clarity,
         water_clarity_units,
         lat_start,
         lon_start,
         lat_end,
         lon_end,
         site_id,
         sub_effort_ident,
         sub_effort_1,
         sub_effort_1_units,
         sub_effort_2,
         sub_effort_2_units,
         sub_effort_nothing_caught,
         species_1,
         length_1,
         length_unit_1,
         length_bin,
         length_bin_unit,
         age,
         aging_structure_1,
         aging_structure_2,
         weight_1,
         weight_unit_1,
         batch_weight,
         batch_weight_unit,
         sex,
         age_class,
         flag,
         original_file_names,
         ind_fish_ident,
         lakesize,
         lakesize_units,
         area_group,
         lat_unspec,
         lon_unspec,
         waterbody_type,
         location_notes_1,
         notes_1,
         obs_id) %>% 
  ungroup()
glimpse(ia_fishobs_trial)
ia_fishobs_trial <- as.data.table(ia_fishobs_trial)


#attempt to recover lake IDs?
#get old IA file:
ia_dat_old <-  fread("D:\\Shared drives\\Hansen Lab\\RESEARCH PROJECTS\\Fish Survey Data\\IA_Data\\ia_raw_disaggregated_data\\Archived\\ia_fishlengths_24Oct2023.csv")

ialakecodekey <- ia_dat_old[ , .("Lat" = mean(Station_Lat), "Lon" = mean(Station_Long)) , .(County, LakeName, LakeID)]

ialakecodekey[ , LakeName := word(LakeName, 1, sep = fixed(" (")) ,]

ia_fishobs_trial[ , lake_id := as.character(lake_id) ,]

ia_fishobs_trial[ialakecodekey, on = .(county=County, lake_name=LakeName), lake_id := LakeID ]

ia_fishobs_trial[ , length(unique(lake_name)) , is.na(lake_id)]

#now mwlaxeref
ia_fishobs_trial[ , nhdhr_id := mwlaxeref::ia_to_nhdhr(ia_fishobs_trial, from_colname = "lake_id", )$nhdhr.id , ]

ia_fishobs_trial[ , length(unique(paste(lake_name))) , .("Has_state_lake_ID" = !is.na(lake_id), "HasNHD_ID" = !is.na(nhdhr_id)) ]

#check sturcture of dates
dates <- ia_fishobs_trial %>% 
  distinct(total_effort_ident, sub_effort_ident, .keep_all = T) %>% 
  select(survey_id, total_effort_ident, sub_effort_ident, date_survey, date_total_effort_ident, date_sub_effort_ident, sampling_method_2)
#total effort ident 13 is a good example to show how the dates work from survey level (first sampling within a lake within a year) - total effort ident (date used to create that total effort ident) - sub effort ident (day for each sub sampling within a total effort ident)

#whats the deal with the survey involving total effort ident 13?
check <- ia_fishobs_trial %>% 
  filter(survey_id == "CB6851EB-3CE8-4333-9554-41A13F2430A9") %>% 
  distinct(total_effort_ident, sub_effort_ident, .keep_all = T)
#this shows that sampling method is the same for all and appears that they should all be the same gear/effort_id but the more refined gear used to make the is different thus one of the sub efforts get a different total effort ident

#fish obs unpacked and tidied, clean up ws
rm(ia_fishobs_30Nov2023, b, cde, ia_age_length_21Aug2021, ia_BLG_age_length_21Aug2021,ia_CCF_age_length_21Aug2021, ia_diet_21Aug2021, ia_lake_characteristics_21Aug2021, names, unusedbits, samplestationlocationmap_drawrectangletofilterlocations, ia_renamer, addcols, dropcols, filei, files_list, first_word, i, maxn, n, dates, check)

```



# Import/Export files

```{r}


#save to disk:




ia_catch_eff <- as_arrow_table(ia_fishobs_trial)

write_dataset(dataset = ia_catch_eff, path = "Data_and_Scripts/Data/output/ia_file_arrow")

ia_data <- open_dataset("Data_and_Scripts/Data/output/ia_file_arrow")

glimpse(ia_data)

```

# Prep Age Data
```{r}
# #We've got three age files, each of them is uniquely messy. This first block is built to start cleaning them
# 
# glimpse(ia_age_length_21Aug2021)
# toString(names(ia_age_length_21Aug2021))
# ia_age_length_21Aug2021[ , .N, is.na(date.1) ]
# ia_age_length_21Aug2021[ , .N , survey_type.1]
# ia_age_length_21Aug2021[ , .N , aging_data_notes.3]
# ia_age_length_21Aug2021[ , .N , .(garbage_bin_notes.4, species.1)]
# 
# 
# #drop a bunch of backcalc cols and other crud
# cols <- as.character(expression(sample_id.1, sample_id.2, sex, lake_id, lake_name.1, county, date.1, garbage_bin_notes.1, garbage_bin_notes.2, garbage_bin_notes.3, year, sampling_method.1, species.1, length.1,length_unit.1, weight.1, weight_unit.1, age, aging_structure.1, garbage_bin_notes.4, original_file_name.1))
#    
# ia_age_length_21Aug2021 <- ia_age_length_21Aug2021[ , .SD , .SDcols = cols]
# #there, now each fish is a row. Bring in other data
# 
# 
# #these guys have multiple rows for each fish. We want only the age observed, not the backcalc ages
# head(ia_BLG_age_length_21Aug2021)
# glimpse(ia_BLG_age_length_21Aug2021)
# 
# ia_BLG_age_length_21Aug2021[ , .N , sample_id.1 ]
# 
# #how to select that: we don't need to do much. each row has the basic info, just take the first of each:
# 
# ia_BLG_age_length_21Aug2021[ , .SD[1] , sample_id.1 ]#in each subset print row 1
# ia_BLG_age_length_21Aug2021 <- ia_BLG_age_length_21Aug2021[ , .SD[1] , sample_id.1 ] # in each subset print row 1
# 
# #nab out only useful cols:
# toString(names(ia_BLG_age_length_21Aug2021))
# cols <- as.character(expression(sample_id.1, garbage_bin_notes.1, lake_name.1, aging_structure.1, species.1, length.1, weight.1, sex, age, original_file_name.1, length_unit.1, weight_unit.1))
# ia_BLG_age_length_21Aug2021 <- ia_BLG_age_length_21Aug2021[ , .SD , .SDcols = cols]




# This code in borrowed from the growth munging file.   
# #date data
# ia[, unique(year)]
# 
# #backfill year?
# ia[is.na(year) & !is.na(date.1)]
# #date align with year?
# ia[!is.na(date.1) & !year==year(date.1)]
# 
# #lost in garbage can? yes there are some month and season data there, 
# ia[is.na(date.1), date.2 := word(garbage_bin_notes.1, -1, sep = ":"), ]
# ia[is.na(date.1), .N , date.2]
# ia[date.2 == "7", date.2 := "July"]
# ia[date.2 == "6", date.2 := "June"]
# ia[date.2 == "8", date.2 := "August"]
# ia[date.2 == "5", date.2 := "May"]
# ia[date.2 == "NA", date.2 := NA  ]
# ia[is.na(date.1)& is.na(date.2), date.2 := word(garbage_bin_notes.3, -1, sep = ":"), ]
# ia[date.2 == "." , date.2 := NA, ]
# ia[ ,date.2 := tolower(date.2) , ]
# 
# ia[is.na(date.1), .N , date.2]
# 
# ia[ , date.1 :=  as.IDate(date.1) , ]
# 
# ia[, date_clean :=  as.IDate(date.1)]
# 
# hist(ia[!is.na(date_clean) ,yday(date_clean)])
# 
# #now populate some dates where only mo or season provided (set to 15th if mo given, or season date approximated from histogram here^)
# 
# ia_datefill <- transpose(keep.names = "oldname", data.table(fall = "2 Oct",
#                                                           july = "15 July",
#                                                           spring = "20 April",
#                                                           summer = "19 July",
#                                                           june = "15 June",
#                                                           august = "15 August",
#                                                           may = "15 May"))
# ia[ , date.3 := 
#       ia_datefill[match(ia[ ,date.2],ia_datefill[,oldname]) ,
#                 V1 , ]
#     , ]
# 
# ia[!is.na(year) , date.4 := paste(date.3, year)]
# 
# ia[ , .N, date.4]
# 
# ia[str_detect(date.4, "NA"), date.4 := NA]
# 
# ia[is.na(date_clean) & !is.na(date.4), date_clean := as.IDate(date.4, format = "%d %B %Y") ]
# 
# # check coverage
# hist(ia[ ,yday(date_clean)])
# ia[!is.na(date_clean), .N , ]/ia[ , .N , ]
# 
# rm(ia_datefill)
# 
# # make other dates character strings
# datecols <- colnames(ia)[str_detect(colnames(ia), "date\\.")]
# ia[    , (datecols) := lapply(.SD, as.character)    ,   .SDcols = datecols]
# 
# #in this chunk you join the effort data to the fish-as-rows or obs-level dataset. I start by prepping these multiple files for a merge. After each operation, be sure to check your work! Again I have left MI in here to give you an idea of how one previous example went. 
# 
# #grabbing effort from catch file


``` 
          
          
          
# Data tidying                        
```{r}        
#this is a bit of tidying code that Denver bypassed in his schema review
# clean up these names and match overall schema
#need to 1. fix current cols to match 2. add missing cols 3.drop all extras 
ia_fishobs <- clean_names(ia_fishobs) #clean up names a touch

# then open into excel an align with the schema (see these IA example files)
# fwrite(as.data.table(names(ia_fishobs)), file = "Data_and_scripts/Data/output/ia_names.csv")

# use the exported csv to align with the column names in the column schema GSheet (https://docs.google.com/spreadsheets/d/1zfevASMxRMxMYNWm3Hq1yR2Qk0zcJV_JnO0LBeODaPQ/edit#gid=0) . 
# - if you delete a column from your state's names in the sheet, the code below will drop those cols automatically from the data product.
# - put an NA in the table in each place where your data have no equivalent col and the code below will fill those with NAs in the data product
# - keep the order of the table in line with the column schema GSheet and that will automatically reorder your data to match the schema GSheet
# after aligning the names with the schema bring that back in (note that the )
ia_renamer <- fread( file = "Data_and_scripts/Data/input/ia_renamer.csv") #read in 

#drop extras
dropcols <- names(ia_fishobs)[!(names(ia_fishobs) %in% ia_renamer[,  iowa])]
ia_fishobs[ , (dropcols) := NULL , ]

#rename ia to fit the schema
ia_renamer[match(names(ia_fishobs),ia_renamer[,iowa]), schema]

setnames(ia_fishobs, 
         old = names(ia_fishobs),
         new = ia_renamer[match(names(ia_fishobs),ia_renamer[,iowa]), schema])


#missing cols
ia_renamer[,schema] %in% names(ia_fishobs)
names(ia_fishobs) %in% ia_renamer[,schema]

#add misssing names:
addcols <- ia_renamer[is.na(iowa) , schema ,]
ia_fishobs[ ,(addcols) := NA , ]

newcolorder <- ia_renamer[ ,schema]


setcolorder(ia_fishobs, newcolorder)


#generate an obs_id

ia_fishobs[ , .N , obs_id ]
ia_fishobs[ , obs_id := .I , ]


#attempt to recover lake IDs?
#get old IA file:
ia_dat_old <-  fread("E:\\Shared drives\\Hansen Lab\\RESEARCH PROJECTS\\Fish Survey Data\\IA_Data\\ia_raw_disaggregated_data\\Archived\\ia_fishlengths_24Oct2023.csv")

ialakecodekey <- ia_dat_old[ , .("Lat" = mean(Station_Lat), "Lon" = mean(Station_Long)) , .(County, LakeName, LakeID)]

ialakecodekey[ , LakeName := word(LakeName, 1, sep = fixed(" (")) ,]

ia_fishobs[ , lake_id := as.character(lake_id) ,]

ia_fishobs[ialakecodekey, on = .(county=County, lake_name=LakeName), lake_id := LakeID ]

ia_fishobs[ , length(unique(lake_name)) , is.na(lake_id)]

#now mwlaxeref
ia_fishobs[ , nhdhr_id := mwlaxeref::ia_to_nhdhr(ia_fishobs, from_colname = "lake_id", )$nhdhr.id , ]

ia_fishobs[ , length(unique(paste(lake_name))) , .("Has_state_lake_ID" = !is.na(lake_id), "HasNHD_ID" = !is.na(nhdhr_id)) ]


### dataset cleanup and tidying. MI work left here as an idea of a previous state's work



                                               
                        #check the product:
                        colnames(mi_catch_eff_merge)
                        
                       mi_catch_eff_merge[ str_detect(lake_name.1, "ike" ), .(count = .N, missinglengths = sum(is.na(length.1)), meanL = mean(length.1)), .(lake_name.1, lake_id, date.1_effort, date.1_mergedcatch, survey_id, sampling_method_abbrev, species.1) ]

            
                        #did we retain all of the surveys? Looks like 498 unique survey IDs, and both the product and input reflect this:
                        mi_catch_eff_merge[ , .N , .(survey_id)]
                        merge(merge(mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id], 
                                    mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id], all = T),
                              mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ], all = T)
                       
                       
                        #here's all of our effort:
                        mi_catch_eff_merge[ , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        #no catch data exist (or matched) for these data:
                        mi_catch_eff_merge[ is.na(species.1)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        #no_taxa_found
                        mi_catch_eff_merge[   , nothing_caught  := is.na(species.1) ,  ]
                        
                        
                        #no effort data were submitted for these fish:
                        mi_catch_eff_merge[ is.na(date.1_effort)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]

                        
       #clean up the column names and tidy the data table up a bit                 
       
              sort(colnames(mi_catch_eff_merge))                 
              
              #county     
              mi_catch_eff_merge[ is.na(county_mergedcatch)|is.na(county_effort) , .N ,   ]
              mi_catch_eff_merge[ !(is.na(county_mergedcatch)&is.na(county_effort)) & county_mergedcatch != county_effort  , .N ,   ]# no cases where the county doesn't match
              mi_catch_eff_merge[  , .N , .(is.na(county_mergedcatch), is.na(county_effort))  ]#there are some cases where we haven't got a county at all, otherwise the effort county covers all. 
              mi_catch_eff_merge[  , county_mergedcatch := NULL]
              setnames(mi_catch_eff_merge, "county_effort", "county")
              
              #data_type
              setnames(mi_catch_eff_merge, "data_type", "data_type_effort")
                   
              #date.1
              mi_catch_eff_merge[ is.na(date.1_mergedcatch)|is.na(date.1_effort) , .N ,   ]
              mi_catch_eff_merge[ !(is.na(date.1_mergedcatch)&is.na(date.1_effort)) & date.1_mergedcatch != date.1_effort  , .N ,   ]# no cases where the dates don't match
              mi_catch_eff_merge[  , .N , .(is.na(date.1_mergedcatch), is.na(date.1_effort))  ]#there are some cases where we haven't got a date at all, otherwise the effort info covers all. 
              mi_catch_eff_merge[  , date.1_mergedcatch := NULL]
              setnames(mi_catch_eff_merge, "date.1_effort", "date.1")
              
              #date recieved
              setnames(mi_catch_eff_merge, "date_recieved", "date_recieved_effort")
              
              #effort units
              mi_catch_eff_merge[ is.na(effort_units.1_mergedcatch)|is.na(effort_units.1_effort) , .N ,   ]
              mi_catch_eff_merge[ !(is.na(effort_units.1_mergedcatch)&is.na(effort_units.1_effort)) & effort_units.1_mergedcatch != effort_units.1_effort  , .N ,   ]# no cases where the units don't match
              mi_catch_eff_merge[  , .N , .(is.na(effort_units.1_mergedcatch), is.na(effort_units.1_effort))  ]#we have efforts for all. slide into single column. 
              
              mi_catch_eff_merge[is.na(effort_units.1_effort), .N , ]
              mi_catch_eff_merge[is.na(effort_units.1_effort) , effort_units.1_effort := effort_units.1_mergedcatch , ]
              mi_catch_eff_merge[ , effort_units.1_mergedcatch := NULL , ]
               setnames(mi_catch_eff_merge, "effort_units.1_effort", "effort_units.1")
              
              #filenumber
              setnames(mi_catch_eff_merge, "file_number", "file_number_effort")
              
              #state
              mi_catch_eff_merge[ ,state := "Michigan"  ] 
              mi_catch_eff_merge[ , `:=` (state_catch = NULL, state_catchlengths = NULL)  , ]
              
              #total effort
              mi_catch_eff_merge[total_effort_1.1_effort != total_effort_1.1_mergedcatch, .N,  ]
              plot(total_effort_1.1_effort ~ total_effort_1.1_mergedcatch, data = mi_catch_eff_merge  )
              abline(1,0)
              
              #effort vals
              mi_catch_eff_merge[ total_effort_1.1_effort != total_effort_1.1_mergedcatch , .N , .(total_effort_1.1_mergedcatch, total_effort_1.1_effort)]
              #I think that the merged catch values were assigned to indiv fish (like "this fish was caught in ONE net lift") and the effort file has survey X gear total efforts
              mi_statustrends_catchlengthclass_03July2023[ , summary(total_effort_1.1) , ] 
                mi_statustrends_catchlengthclass_03July2023[total_effort_1.1 > 1 , summary(total_count), sampling_method_abbrev]
              # mi_statustrends_catch_16Mar2021[ , summary(total_effort_1.1) , ]    #this won run b/c no col for effort in that       
              mi_statustrends_effort_16Mar2021[ , summary(total_effort_1.1) ,]          
              # well--- I don't know what to make of all this, but for now I'll be keeping both of these "total_effort" variables, and leaning on the one originating in the effort file
              
              
              
              #year
              
              mi_catch_eff_merge[ year_effort != year_mergedcatch , ,]
              mi_catch_eff_merge[ , .N , .(is.na(year_effort), is.na(year_mergedcatch))]
              mi_catch_eff_merge[is.na(year_effort), year_effort := year_mergedcatch , ]
              mi_catch_eff_merge[ ,  year_mergedcatch := NULL , ]
              setnames(mi_catch_eff_merge, "year_effort", "year")
              
              
              # most of this is waste-of-time junk. Let's move the big ones left and leave this mess hang out there to the right.
              notgarbage <-  c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                               "sampling_method_abbrev", "total_effort_1.1_effort", "effort_units.1", "nothing_caught",  #gear
                               "species.1", "ident", "length.1", "length_unit.1", "ident_l" #fish
                               )
              
              setcolorder(mi_catch_eff_merge, notgarbage)
               
              
        #expand these data to cover all interested species in each surveyXgear
                        
                        #check behavior now:
                        mi_catch_eff_merge[ species.1 == "WAE" , .N  , c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        #we can see that to gen a catch or CPUE dataset we can cast wide (like we did in MN)
        
        #generate a species obs matrix
                        #clean dates:
                        mi_catch_eff_merge[ , unique(date.1) , ]
                        
                        #execute
                        mi_catch_eff_merge[ , date_clean := as.IDate(date.1) ,]
                        mi_catch_eff_merge[ , summary(date_clean) , ]
                        mi_catch_eff_merge[ is.na(date_clean) , .N , .(survey_id, lake_name.1)] #missing effort data here, thus the gap
                        
                        
                        #tag codes with "taxon"
                        mi_catch_eff_merge[ , species.1 := paste("taxon_",species.1, sep = "")  ,]
                        
                        #we called this "wide complete" in MN
                        wide_complete <- dcast(mi_catch_eff_merge[ ,.N , by = c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                                                                           "sampling_method_abbrev", "total_effort_1.1_effort", "effort_units.1", "nothing_caught", 
                                                                           "species.1"
                                                                           )] , ... ~ species.1 , value.var = "N", fill = 0)
                        
                        wide_complete[ , c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                                                                           "sampling_method_abbrev", "total_effort_1.1_effort", "effort_units.1", "taxon_WAE") , ]
                        
                        wide_complete[taxon_NA >0]
                        
                        
          # saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts/Data/output/mi_flat_effort_indivfish_merge.rds")                
                        
                        
                        
                        
```                        
                        
 


# Review & QC datasets
```{r}
#here I do some very basic checks on what the data structure and general outputs look like (i.e., s this thing behaving like the obs-level file I think it is?). MI work left here as an idea of a previous state's work. In my opinion, it is not our job to QC the actual observations at this point (like, is a WAE really going to be 500mm at age zero), but instead to use this QC as a check on the operations performed in this script.  

ia_data <- open_dataset(sources = file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Fish Survey Data", "Parquet files", "ia_file_arrow"))
glimpse(ia_data)

ia_data %>% 
  distinct(total_effort_ident, sampling_method) %>% 
  group_by(total_effort_ident, sampling_method) %>% 
  count() %>% 
  group_by(total_effort_ident) %>% 
  count() %>% 
  filter(n > 2) %>% 
  collect() %>% 
  print( n = nrow(.)) 
#total effort idents with more than one sampling method
#the problem above is fixed when we look at the more specific sampling method
ia_data %>% 
  distinct(total_effort_ident, sampling_method_2) %>% 
  group_by(total_effort_ident, sampling_method_2) %>% 
  count() %>% 
  group_by(total_effort_ident) %>% 
  count() %>% 
  filter(n > 2) %>% 
  collect() %>% 
  print( n = nrow(.)) 

ia_data %>% 
  group_by(species_1) %>% 
  count() %>% 
  collect() %>% 
  print(n = nrow(.))
#this all looks pretty good besides the 2 records of sucker,_redhorse_mixed with the comma, oy good enough 


#export a list of lake locations/nhds
ia_data %>% 
  group_by(waterbody_type) %>% 
  count() %>% 
  collect()


ia_data %>%
  # filter(waterbody_type %in% c(""))
  group_by(state, county, lake_name, lake_id, nhdhr_id, waterbody_type) %>% 
  # slice(which(!is.na(lat_unspec) & !is.na(lon_unspec))) %>%
  summarize(mean_lat = mean(lat_unspec, na.rm = TRUE),
            mean_lon = mean(lon_unspec, na.rm = TRUE)) %>%
  collect() %>% 
  {. ->> ia_lake_locs_25Jan}

#looking at weird total effort idents 
ia_data %>% 
  filter(total_effort_ident == "2657") %>% 
  collect() %>% 
   distinct(total_effort_ident, sampling_method, .keep_all = T) %>% 
  glimpse()
#these clearly all have different sampling method but the more specific sampling method is the same - the one used in the creation of the total effort ident

check <- ia_data %>% 
  filter(survey_id == "CB6851EB-3CE8-4333-9554-41A13F2430A9") %>% 
  collect() %>% 
  distinct(total_effort_ident, sub_effort_ident, sampling_method, .keep_all = T) 
  


                        #effort per surveyXgear?
                        mi_catch_eff_merge[ , .(effort = first(total_effort_1.1_effort), units = first(effort_units.1)) ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")]
                        
                        #effort per survey type?
                        mi_catch_eff_merge[!is.na(total_effort_1.1_effort) , .(effort = first(total_effort_1.1_effort), units = first(effort_units.1), number = .N) ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")][ ,.(effort = sum(effort), counts = sum(number), grandCPUE = sum(number)/sum(effort)) , .(sampling_method_abbrev, units)]
                        
                        #effort per survey type (Walleye ONLY)?
                        mi_catch_eff_merge[!is.na(total_effort_1.1_effort) & species.1=="walleye" , .(effort = first(total_effort_1.1_effort), units = first(effort_units.1), number = .N) ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")][ ,.(effort = sum(effort), counts = sum(number), grandCPUE = sum(number)/sum(effort)) , .(sampling_method_abbrev, units)]
                        
                        
                        
                        #how many Walleye in surveys where we had effort data?
                        mi_catch_eff_merge[ species.1== "walleye"  , .("n_fish" = .N) , .(sampling_method_abbrev) ][, sum(n_fish)]
                        mi_catch_eff_merge[ , .N , species.1]
                        
                        
                        #whats the effort look like?
                        mi_catch_eff_merge[ ,.N, total_effort_1.1_effort ]
                        
                        # data coverage
                        # how many surveys were we missing effort data for? One survey, 3 gears. Survey 4042 on Twin Lake
                        mi_catch_eff_merge[ is.na(total_effort_1.1_effort), .N , ]
                        mi_catch_eff_merge[ is.na(total_effort_1.1_effort), c("numberofspp" = length(unique(species.1))) , c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        
                        
                        # how many surveys were we missing catch data from?
                        # how many surveys missing catchlength for?
                        mi_catch_eff_merge[ , .N, is.na(original_file_name.1_catch)]
                        mi_catch_eff_merge[ , .N, .(catchNA = is.na(original_file_name.1_catch),
                                                    catchlengthNA = is.na(original_file_name.1_catchlengths),
                                                    effortNA = is.na(original_file_name.1_effort))]
                        mi_catch_eff_merge[is.na(original_file_name.1_catch)]
                        
                        
                        
                        glimpse(mi_catch_eff_merge)
                        
                        
                        #Naming scheme updated to match overall approach:
                        
                      
                        

```





