---
title: "MI_Flat_File_Aggregation"
author: "Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
---
# Preamble

## To-do List
1. Ages were excluded from this file
2. We limited species in this because the files had different species extents
4. Dates in string format changed to match schema
5. Seems that the current output might be low on # of lakes in there. 
6. In my dates exploration, I found a survey start and end dates, all surveys expect for 4 happen over a 7 day time frame
    -there is one gill netting case spans from May to Oct.
    -given the end date does not seem relevant to retain, I only keep survey start dates





##Libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(data.table)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)
library(mwlaxeref)

options(scipen = 999)
```


##Data
This could readily be changed into a function that takes a filepath and returns files into environment.
* note Holly has to change file paths to "D" 
```{r}
#generate a file list to import
files_list <- list.files(path = "D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MI_Data/mi_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list

#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MI_Data/mi_raw_disaggregated_data/",
                                          files_list[i])))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    data.table::transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 



```


#Data Review
```{r}
#effort:
mi_statustrends_effort_16Mar2021[ , .N , .(lake_id, lake_name.1, date.1 , survey_type.1, sampling_method_abbrev)  ]

#some lake IDs seem to have >1 name:
mi_statustrends_effort_16Mar2021[  ,length(unique(lake_id)), .(lake_name.1)][V1>1] # lake names not unique (they usually aren't)
mi_statustrends_effort_16Mar2021[  ,length(unique(lake_name.1)), .(lake_id)][V1>1] # lake ids not unique (they *should* be though)

#which lakes have multiple names for a single ID?
mi_statustrends_effort_16Mar2021[  ,length(unique(lake_name.1)), .(lake_id)][V1>1][,lake_id]
mi_statustrends_effort_16Mar2021[lake_id %in% 
                                   mi_statustrends_effort_16Mar2021[  ,length(unique(lake_name.1)), .(lake_id)][V1>1][,lake_id], , ]
# it appears here that a full survey (all gears were deployed in each part of these lakes. For that reason, My vote is that we keep them separate and use lake_id + lake_name for our key. 

# view each data level for a single survey
mi_statustrends_effort_16Mar2021[survey_id ==2446]
mi_statustrends_catch_16Mar2021[survey_id == 2446]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446, ]
mi_statustrends_lenage_20May2021[survey_id == 2446, ]

# do the counts match across all levels?
mi_statustrends_catch_16Mar2021[survey_id == 2446, sum(total_count.1)]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446, sum(total_count.1)]
nrow(mi_statustrends_lenage_20May2021[survey_id == 2446, ])

#no. not even close. I suspect this is bc of a species seletion that happened in the catch data:
mi_statustrends_catch_16Mar2021[survey_id == 2446, length(unique(species.1))]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446, length(unique(species.1))]
mi_statustrends_lenage_20May2021[survey_id == 2446, length(unique(species.1)) ]
#view species lists:
mi_statustrends_catch_16Mar2021[survey_id == 2446,sum(total_count.1) , species.1]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446,sum(total_count.1) , species.1]
mi_statustrends_lenage_20May2021[survey_id == 2446, .N , species.1 ] # these numbers will not be the same. This is because these aged fish are a subset.
#view species lists:
mi_statustrends_catch_16Mar2021[survey_id == 4079,sum(total_count.1) , species.1]
mi_statustrends_catchlengthclass_03July2023[survey_id == 4079,sum(total_count.1) , species.1]
mi_statustrends_lenage_20May2021[survey_id == 4079, .N , species.1 ] # these numbers will not be the same. This is because these aged fish are a subset.



# not exactly clear WHY the numbers still don't jive between the inch grp file and the catch file, BUT the catch file seems to have problems with the n per species X gear where there are zeros in an inchgroup (see survey 2446)


#survey_ids matched across all surveys?
mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id]
mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id]
mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ]
mi_statustrends_lenage_20May2021[ , .( lenage_nrows = .N) , survey_id] 


  
data_coverage <- 
merge(
  merge(
  merge(
    merge(mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id], 
          mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id], all = T),
    mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ], all = T), 
  mi_statustrends_lenage_20May2021[ , .( lenage_nrows = .N) , survey_id], all = T),
  snt_lakes_info_unique_id[,.(crosswalk_nrows = .N) , Survey_Number ],by.x = "survey_id", by.y = "Survey_Number", all = T
)

#retaining more dates?
survey_dates <- mi_statustrends_catch_16Mar2021 %>% 
  select(survey_id, date.1, sample_time_notes.1) %>% 
  mutate(start = as.Date(date.1),
         end = str_replace(sample_time_notes.1, "SAMPLE_END_DATE:", ""),
         end = as.Date(end, format = "%m/%d/%Y"),
         duration = end - start) %>% 
  distinct(survey_id, .keep_all = T)
survey_dates %>% 
  summarise(min.dur = min(duration), 
            max.dur = max(duration),
            over.week = sum(duration > 7))
#only 4 surveys that lasted longer than a week - two longest are survey id 4854 and 9316
mi_statustrends_catch_16Mar2021 %>% 
  filter(survey_id %in% c("4854", "9316")) %>% 
  group_by(survey_id, sampling_method_abbrev, date.1, sample_time_notes.1) %>% 
  count()
```


# Effort Merge
```{r}
#Start with a merge of catch and catch-length?

#file prep
#dates
# mi_statustrends_effort_16Mar2021[ , unique(date.1) , ]
mi_statustrends_effort_16Mar2021[ , date.1 := as.character(date.1) , ]
# mi_statustrends_catch_16Mar2021[ , unique(date.1) , ]
mi_statustrends_catch_16Mar2021[ , date.1 := as.character(as.IDate(date.1, format = "%m/%d/%Y")) , ]
#species        
#species is empty in effort
mi_statustrends_effort_16Mar2021[ , .N , species.1 ]
mi_statustrends_effort_16Mar2021[ , species.1 := NULL ,]

#view species lists:
mi_statustrends_catch_16Mar2021[ ,sum(total_count.1) , species.1]
mi_statustrends_catchlengthclass_03July2023[ ,sum(total_count.1) , species.1]
mi_statustrends_lenage_20May2021[ , .N , species.1 ] # these numbers will not be the same. This is because these aged fish are a subset.

#dump the species not of interest to us here:
mi_statustrends_catchlengthclass_03July2023 <- mi_statustrends_catchlengthclass_03July2023[species.1 %in% mi_statustrends_catch_16Mar2021[ , unique(species.1)]]

#column name diffs
colnames(mi_statustrends_catchlengthclass_03July2023)[colnames(mi_statustrends_catchlengthclass_03July2023)== "sampling_method.1" ] <- "sampling_method_abbrev"

#add lake_id to lengthclass file:
mi_statustrends_catchlengthclass_03July2023[mi_statustrends_effort_16Mar2021[ ,.N , .(lake_id, lake_name.1, survey_id)], on = c("lake_name.1", "survey_id"), lake_id := lake_id ] 
mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)]#still missing names

#any more available in catch file? NOPE
mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)][,survey_id]%in%
  mi_statustrends_catch_16Mar2021[ ,.N , .(lake_id, lake_name.1, survey_id)][ , survey_id]

#if drawn from effort sans survey ID (Risky, ambiguous join) can we get a 1:1 for lake name? or is there multi lakes for each name?
mi_statustrends_effort_16Mar2021[ lake_name.1 %in%
                                    mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)][,lake_name.1] , .N ,
                                  .(lake_id, county, lake_name.1, year, survey_id, survey_type.1) ]
#if drawn from catch sans survey ID (Risky, ambiguous join) can we get a 1:1 for lake name? or is there multi lakes for each name?
mi_statustrends_catch_16Mar2021[ lake_name.1 %in%
                                   mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)][,lake_name.1] , .N ,
                                 .(lake_id, county, lake_name.1, year, survey_id, survey_type.1) ]

#Conclusion-- don't do that. only grab the full keyed lake IDs from effort:
mi_statustrends_catchlengthclass_03July2023[mi_statustrends_catch_16Mar2021[ ,.N , .(lake_id, lake_name.1, survey_id)], on = c("lake_name.1", "survey_id"), lake_id := lake_id ]
#uncount catch
#uncount the catch files into an indiv-as-row format:
#with lengths scope
mi_statustrends_catchlengthclass_03July2023[ ,summary(total_count.1) , ]
mi_statustrends_catchlengthclass_03July2023[ , .N , total_count.1 == 0] #do we lose anything if we drop these?

#here we check if the surveyXsample methods are all covere in the >0 total_count.1 data (same dataset)
mi_statustrends_catchlengthclass_03July2023[total_count.1 == 0 , .N , .(survey_id, sampling_method_abbrev)]
sum(!mi_statustrends_catchlengthclass_03July2023[total_count.1 == 0 , paste(survey_id, sampling_method_abbrev)] %in% mi_statustrends_catchlengthclass_03July2023[total_count.1>0, paste(survey_id,sampling_method_abbrev)])
#how about in the effort data? There's no effort data that we would lose if we drop those zeros. 
sum(!mi_statustrends_catchlengthclass_03July2023[total_count.1 == 0 , paste(survey_id, sampling_method_abbrev)] %in% mi_statustrends_effort_16Mar2021[, paste(survey_id,sampling_method_abbrev)])

#execute the uncount, dropping those zero total counts in the mix (This line is generating a warning in the next line)
mi_statustrends_catchlengthclass_03July2023_uncount <- 
  uncount(mi_statustrends_catchlengthclass_03July2023[total_count.1!=0], total_count.1, .remove = T, .id = "ident_l")

#add a surveyxgear ident for indiv fish
mi_statustrends_catchlengthclass_03July2023_uncount[ , ident := seq_len(.N) , .(lake_name.1, lake_id, survey_id, sampling_method_abbrev, species.1) ]

#no lengths scope          
mi_statustrends_catch_16Mar2021[ , summary(total_count.1)]
#execute
mi_statustrends_catch_16Mar2021_uncount <- 
  uncount(mi_statustrends_catch_16Mar2021, total_count.1, .remove = T, .id = "ident")



#how many length data cover unknown surveys in catch data?

#here we check if the surveyXsample methods are all covered in the simple catch data
mi_statustrends_catchlengthclass_03July2023_uncount[ , .N , .(survey_id, sampling_method_abbrev)]
sum(!mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))] %in% mi_statustrends_catch_16Mar2021_uncount[, unique(paste(survey_id,sampling_method_abbrev))])

#how about in the effort data? There's no effort data that we would lose if we drop those zeros, but we do have 3 surveyXgears unique to the inchclass data. 
sum(!mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))] %in% mi_statustrends_effort_16Mar2021[, paste(survey_id,sampling_method_abbrev)])

# we can see that 3 surveys in inchclass data are not represented in catch or effort data
mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))][!mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))] %in% mi_statustrends_effort_16Mar2021[, paste(survey_id,sampling_method_abbrev)]]

mi_statustrends_effort_16Mar2021[survey_id == 4042]

mi_statustrends_catchlengthclass_03July2023_uncount[ survey_id == 4042, .N , .(species.1, sampling_method_abbrev) ]

#are there any surveys without any catch? Yes, 120 of them. (can we assume catch = zero there? Likely yes )
sum(!
      mi_statustrends_effort_16Mar2021[ , 
                                        unique(paste(survey_id, sampling_method_abbrev))
      ] %in%
      mi_statustrends_catch_16Mar2021_uncount[ ,
                                               unique(paste(survey_id, sampling_method_abbrev)) ,
      ]
)

mi_statustrends_catch_16Mar2021[ , summary(total_count.1) ,]

# and how many surveyX gears are not shown in the effort file? ZERO!                
sum(!mi_statustrends_catch_16Mar2021_uncount[ ,
                                              unique(paste(survey_id, sampling_method_abbrev)) ,
] %in%
  mi_statustrends_effort_16Mar2021[ , 
                                    unique(paste(survey_id, sampling_method_abbrev))
  ]

)
#this tells us we can skip expansion here. These no catch are already captured in the effort file

#merge catch files:                
names(mi_statustrends_catch_16Mar2021_uncount)
colnames(mi_statustrends_catchlengthclass_03July2023_uncount)


#set key columns
keycols <- c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev", "species.1", "ident")
setkeyv(mi_statustrends_catch_16Mar2021_uncount, keycols )
setkeyv(mi_statustrends_catchlengthclass_03July2023_uncount, keycols )

#merge together all catch data
mi_catch_merge <- merge(mi_statustrends_catch_16Mar2021_uncount, mi_statustrends_catchlengthclass_03July2023_uncount, by = keycols, all = T, suffixes = c("_catch", "_catchlengths"))


#add ages where we've got em' (ambiguous gears means we can't tie these to catch-- they're going to go into the mi flat file without gear assigned)
# #add an ident
# mi_statustrends_lenage_20May2021[ , ident := seq_len(.N) , .(lake_id, survey_id, sampling_method_abbrev, species.1) ]
# #set key columns
# keycols <- c("lake_id", "survey_id", "sampling_method_abbrev", "species.1", "ident")
# setkeyv(mi_statustrends_catch_16Mar2021_uncount, keycols )
# setkeyv(mi_statustrends_catchlengthclass_03July2023_uncount, keycols )
# 
# #merge together all catch data
# mi_catch_merge <- merge(mi_statustrends_catch_16Mar2021_uncount, mi_statustrends_catchlengthclass_03July2023_uncount, by = keycols, all = T, suffixes = c("_catch", "_catchlengths"))


#merge effort into this

#set key columns
keycols <- c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")
setkeyv(mi_catch_merge, keycols )
setkeyv(mi_statustrends_effort_16Mar2021, keycols )

#drop survey level info that will otherwise get duplicated upon  merge() (DONT DO THIS HERE, WILL LOSE DATA-- SEE data_coverage table)
data_coverage[ ,.N , .("effortNA" = is.na(effort_nrows), "catchlenNA" = is.na(catchlen_nrows))]
data_coverage[ ,.N , .("effortNA" = is.na(effort_nrows), "catchNA" = is.na(catch_nrows))]


#do merge
mi_catch_eff_merge <- merge(mi_catch_merge, mi_statustrends_effort_16Mar2021, by = keycols, all = T, suffixes = c("_mergedcatch", "_effort"))




### dataset cleanup and tidying                       

#check the product:
colnames(mi_catch_eff_merge)

mi_catch_eff_merge[ str_detect(lake_name.1, "ike" ), .(count = .N, missinglengths = sum(is.na(length.1)), meanL = mean(length.1)), .(lake_name.1, lake_id, date.1_effort, date.1_mergedcatch, survey_id, sampling_method_abbrev, species.1) ]


#did we retain all of the surveys? Looks like 498 unique survey IDs, and both the product and input reflect this:
mi_catch_eff_merge[ , .N , .(survey_id)]
merge(merge(mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id], 
            mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id], all = T),
      mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ], all = T)


#here's all of our effort:
mi_catch_eff_merge[ , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
#no catch data exist (or matched) for these data:
mi_catch_eff_merge[ is.na(species.1)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
#no_taxa_found
mi_catch_eff_merge[   , nothing_caught  := is.na(species.1) ,  ]


#no effort data were submitted for these fish:
mi_catch_eff_merge[ is.na(date.1_effort)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]


#clean up the column names and tidy the data table up a bit                 

sort(colnames(mi_catch_eff_merge))                 

#county     
mi_catch_eff_merge[ is.na(county_mergedcatch)|is.na(county_effort) , .N ,   ]
mi_catch_eff_merge[ !(is.na(county_mergedcatch)&is.na(county_effort)) & county_mergedcatch != county_effort  , .N ,   ]# no cases where the county doesn't match
mi_catch_eff_merge[  , .N , .(is.na(county_mergedcatch), is.na(county_effort))  ]#there are some cases where we haven't got a county at all, otherwise the effort county covers all. 
mi_catch_eff_merge[  , county_mergedcatch := NULL]
setnames(mi_catch_eff_merge, "county_effort", "county")

#data_type
setnames(mi_catch_eff_merge, "data_type", "data_type_effort")

#date.1
mi_catch_eff_merge[ is.na(date.1_mergedcatch)|is.na(date.1_effort) , .N ,   ]
mi_catch_eff_merge[ !(is.na(date.1_mergedcatch)&is.na(date.1_effort)) & date.1_mergedcatch != date.1_effort  , .N ,   ]# no cases where the dates don't match
mi_catch_eff_merge[  , .N , .(is.na(date.1_mergedcatch), is.na(date.1_effort))  ]#there are some cases where we haven't got a date at all, otherwise the effort info covers all. 
mi_catch_eff_merge[  , date.1_mergedcatch := NULL]
setnames(mi_catch_eff_merge, "date.1_effort", "date.1")

#date recieved
setnames(mi_catch_eff_merge, "date_recieved", "date_recieved_effort")

#effort units
mi_catch_eff_merge[ is.na(effort_units.1_mergedcatch)|is.na(effort_units.1_effort) , .N ,   ]
mi_catch_eff_merge[ !(is.na(effort_units.1_mergedcatch)&is.na(effort_units.1_effort)) & effort_units.1_mergedcatch != effort_units.1_effort  , .N ,   ]# no cases where the units don't match
mi_catch_eff_merge[  , .N , .(is.na(effort_units.1_mergedcatch), is.na(effort_units.1_effort))  ]#we have efforts for all. slide into single column. 

mi_catch_eff_merge[is.na(effort_units.1_effort), .N , ]
mi_catch_eff_merge[is.na(effort_units.1_effort) , effort_units.1_effort := effort_units.1_mergedcatch , ]
mi_catch_eff_merge[ , effort_units.1_mergedcatch := NULL , ]
setnames(mi_catch_eff_merge, "effort_units.1_effort", "effort_units.1")

#filenumber
setnames(mi_catch_eff_merge, "file_number", "file_number_effort")

#state
mi_catch_eff_merge[ ,state := "Michigan"  ] 
mi_catch_eff_merge[ , `:=` (state_catch = NULL, state_catchlengths = NULL)  , ]

#total effort
mi_catch_eff_merge[total_effort_1_effort != total_effort_1_mergedcatch, .N,  ]
plot(total_effort_1_effort ~ total_effort_1_mergedcatch, data = mi_catch_eff_merge  )
abline(1,0)

#effort vals
mi_catch_eff_merge[ total_effort_1_effort != total_effort_1_mergedcatch , .N , .(total_effort_1_mergedcatch, total_effort_1_effort)]
#I think that the merged catch values were assigned to indiv fish (like "this fish was caught in ONE net lift") and the effort file has survey X gear total efforts
mi_statustrends_catchlengthclass_03July2023[ , summary(total_effort_1) , ] 
mi_statustrends_catchlengthclass_03July2023[total_effort_1 > 1 , summary(total_count.1), sampling_method_abbrev]
# mi_statustrends_catch_16Mar2021[ , summary(total_effort_1.1) , ]    #this won run b/c no col for effort in that       
mi_statustrends_effort_16Mar2021[ , summary(total_effort_1) ,]          
# well--- I don't know what to make of all this, but for now I'll be keeping both of these "total_effort" variables, and leaning on the one originating in the effort file



#year

mi_catch_eff_merge[ year_effort != year_mergedcatch , ,]
mi_catch_eff_merge[ , .N , .(is.na(year_effort), is.na(year_mergedcatch))]
mi_catch_eff_merge[is.na(year_effort), year_effort := year_mergedcatch , ]
mi_catch_eff_merge[ ,  year_mergedcatch := NULL , ]
setnames(mi_catch_eff_merge, "year_effort", "year")


# most of this is waste-of-time junk. Let's move the big ones left and leave this mess hang out there to the right.
notgarbage <-  c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                 "sampling_method_abbrev", "total_effort_1_effort", "effort_units.1", "nothing_caught",  #gear
                 "species.1", "ident", "length.1", "length_unit.1", "ident_l" #fish
                 )

setcolorder(mi_catch_eff_merge, notgarbage)


#expand these data to cover all interested species in each surveyXgear

#check behavior now:
mi_catch_eff_merge[ species.1 == "WAE" , .N  , c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
#we can see that to gen a catch or CPUE dataset we can cast wide (like we did in MN)

#generate a species obs matrix
#clean dates:
mi_catch_eff_merge[ , unique(date.1) , ]

#execute
mi_catch_eff_merge[ , date_clean := as.IDate(date.1) ,]
mi_catch_eff_merge[ , summary(date_clean) , ]
mi_catch_eff_merge[ is.na(date_clean) , .N , .(survey_id, lake_name.1)] #missing effort data here, thus the gap


#tag codes with "taxon"
mi_catch_eff_merge[ , species.1 := paste("taxon_",species.1, sep = "")  ,]

#we called this "wide complete" in MN
wide_complete <- dcast(mi_catch_eff_merge[ ,.N , by = c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                                                        "sampling_method_abbrev", "total_effort_1_effort", "effort_units.1", "nothing_caught", 
                                                        "species.1")
                                           ] ,
                       ... ~ species.1 ,
                       value.var = "N",
                       fill = 0)

wide_complete[ , c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                   "sampling_method_abbrev", "total_effort_1_effort", "effort_units.1", "taxon_WAE") , ]

wide_complete[taxon_NA >0]

# now add in an effort identifier column:
            mi_catch_eff_merge[ , effort_ident := .GRP ,.(county,lake_id, lake_name.1, date.1, year, survey_id,  #survey 
                                                        sampling_method_abbrev, total_effort_1_effort, effort_units.1, nothing_caught) ]            
# now use Denver Links cleanup of these:
#changing species names to standard format                       
mi_catch_eff_merge <- mi_catch_eff_merge %>% 
  #renaming species abbreviations column - changed to common name in the following step
  rename(species.abbrev = species.1) %>% 
  #agency species code updated with common name
  mutate(species.1 = case_when(species.abbrev == "taxon_BCR" ~ "black_crappie",
                             species.abbrev == "taxon_BLG" ~ "bluegill",
                             species.abbrev == "taxon_CIS" ~ "cisco",
                             species.abbrev == "taxon_LMB" ~ "largemouth_bass",
                             species.abbrev == "taxon_NOP" ~ "northern_pike",
                             species.abbrev == "taxon_SMB" ~ "smallmouth_bass",
                             species.abbrev == "taxon_WAE" ~ "walleye",
                             species.abbrev == "taxon_YEP" ~ "yellow_perch",
                             species.abbrev == "taxon_NA" ~ NA,
                             TRUE ~ species.abbrev)) %>% 
  #Gear type code updated with spelled-out common name
  mutate(sampling_method = case_when(sampling_method_abbrev == "BOOMSHK" ~ "boomshocking",
                                            sampling_method_abbrev == "GLGNET" ~ "great_lakes_gill_net",
                                            sampling_method_abbrev == "IGNET" ~ "inland_gill_net",
                                            sampling_method_abbrev == "LMFYKE" ~ "large_mesh_fyke_net",
                                            sampling_method_abbrev == "SEINE" ~ "seine",
                                            sampling_method_abbrev == "SMFYKE" ~ "small_mesh_fyke_net",
                                            sampling_method_abbrev == "TRAPNET" ~ "trap_net")) %>% 
  #unit length 
  mutate(length_unit.1 = case_when(length_unit.1 == "col_name_Inch_Group" ~ "inch_group",
                                   TRUE ~ length_unit.1)) %>% 
  #effort unit 
  mutate(effort_units.1 = case_when(effort_units.1 == "HAULS" ~ "hauls",
                                    effort_units.1 == "MINUT" ~ "minute",
                                    effort_units.1 == "LIFTS" ~ "lifts")) %>%
  rename(. , original_file_name.1_effort = original_file_name.1) %>% 
  #selects for columns to be used during analysis - limits memory usage and allows creates more clarity
  #effort files for survey type, lat/long, and target species contained more data than the merged catch files
  select(state, county, lake_id, lake_name.1, date.1, year, survey_type.1_effort, survey_id, lat_unspec_effort, lon_unspec_effort, sampling_method, sampling_method_abbrev, target_species_effort, species.1, species.abbrev, length.1, length_unit.1, total_effort_1_effort, effort_units.1, effort_ident, nothing_caught, original_file_name.1_catch,original_file_name.1_catchlengths,original_file_name.1_effort)                        

mi_catch_eff_merge[state == "Michigan", state := "mi"]

local_to_nhdhr(mi_catch_eff_merge[!is.na(lake_id)], from_colname = "lake_id", states = "mi"  ) %>% count(nhdhr.id)

mi_catch_eff_merge[!is.na(lake_id)  ,   nhdhr.id := local_to_nhdhr(mi_catch_eff_merge[!is.na(lake_id)], from_colname = "lake_id" , states = "mi" )$nhdhr.id ,  ]

#If there are NAs in the lake Ids you feed into these local to NHD functions, you'll end up raking a ton of extra nhdhr vals from the NA local ID column. 

#these are the unmatched lake_ids that we're left with.
mi_catch_eff_merge[is.na(nhdhr.id), .N , lake_id ]

#do we need to retain more date columns?
mi_statustrends_catch_16Mar2021_uncount %>% 
  group_by(survey_id, date.1) %>% 
  count()

mi_catch_eff_merge %>% 
  group_by(survey_id, effort_ident, sampling_method, date.1) %>% 
  count() 

#matching to schema
mi_data <- mi_catch_eff_merge %>% 
  mutate(lake_name = lake_name.1,
         date_survey = as.Date(date.1),
         date_total_effort_ident = as.Date(date.1),
         date_sub_effort_ident = as.Date(NA),
         date_sample = as.Date(NA),
         month = month(date.1),
         survey_id = as.character(survey_id),
         survey_type = survey_type.1_effort,
         survey_type_2 = as.character(NA),
         survey_type_3 = as.character(NA),
         survey_type_4 = as.character(NA),
         sampling_method_2 = sampling_method,
         sampling_method = sampling_method_abbrev,
         gear_data_notes = as.character(NA),
         target_species = target_species_effort,
         target_species_2 = as.character(NA),
         total_effort_ident = as.character(effort_ident),
         total_effort_1 = total_effort_1_effort,
         total_effort_2 = as.numeric(NA),
         total_effort_3 = as.numeric(NA),
         total_effort_1_units = effort_units.1,
         total_effort_2_units = as.character(NA),
         total_effort_3_units = as.character(NA),
         total_effort_nothing_caught = nothing_caught,
         water_temp = as.numeric(NA),
         water_temp_units = as.character(NA),
         water_clarity = as.numeric(NA),
         water_clarity_units = as.character(NA),
         lat_start = as.numeric(NA),
         lat_end = as.numeric(NA),
         lon_start = as.numeric(NA),
         lon_end = as.numeric(NA),
         site_id = as.character(NA),
         sub_effort_ident = as.character(NA),
         sub_effort_1 = as.numeric(NA),
         sub_effort_1_units = as.character(NA),
         sub_effort_2 = as.numeric(NA),
         sub_effort_2_units = as.character(NA),
         sub_effort_nothing_caught = as.logical(NA),
         length_1 = as.numeric(NA),
         length_unit_1 = as.character(NA),
         length_bin = as.character(length.1),
         length_bin_unit = "lower end of 1 inch length bin",
         age = as.numeric(NA),
         aging_structure_1 = as.character(NA),
         aging_structure_2 = as.character(NA),
         weight_1 = as.numeric(NA),
         weight_unit_1 = as.character(NA),
         batch_weight = as.character(NA),
         batch_weight_unit = as.character(NA),
         sex = as.character(NA),
         age_class = as.character(NA),
         flag = as.character(NA),
         ind_fish_ident = as.character(NA),
         lakesize = as.numeric(NA),
         lakesize_units = as.character(NA),
         area_group = as.character(NA),
         lat_unspec = lat_unspec_effort,
         lon_unspec = lon_unspec_effort,
         waterbody_type = as.character(NA),
         location_notes_1 = as.character(NA),
         notes_1 = as.character(NA),
         obs_id = as.character(row_number())) %>% 
   unite("original_file_names", c(original_file_name.1_catch,
                                      original_file_name.1_catchlengths,
                                      original_file_name.1_effort), 
                                      remove = T, 
                                      na.rm = T,
         sep = " ") %>% 
  select(state,
         county,
         lake_name,
         lake_id,
         nhdhr.id,
         date_survey,
         date_total_effort_ident,
         date_sub_effort_ident,
         date_sample,
         year,
         month,
         survey_id,
         survey_type,
         survey_type_2,
         survey_type_3,
         survey_type_4,
         sampling_method,
         sampling_method_2,
         gear_data_notes,
         target_species,
         target_species_2,
         total_effort_ident,
         total_effort_1,
         total_effort_2,
         total_effort_3,
         total_effort_1_units,
         total_effort_2_units,
         total_effort_3_units,
         total_effort_nothing_caught,
         water_temp,
         water_temp_units,
         water_clarity,
         water_clarity_units,
         lat_start,
         lon_start,
         lat_end,
         lon_end,
         site_id,
         sub_effort_ident,
         sub_effort_1,
         sub_effort_1_units,
         sub_effort_2,
         sub_effort_2_units,
         sub_effort_nothing_caught,
         species.1,
         length_1,
         length_unit_1,
         length_bin,
         length_bin_unit,
         age,
         aging_structure_1,
         aging_structure_2,
         weight_1,
         weight_unit_1,
         batch_weight,
         batch_weight_unit,
         sex,
         age_class,
         flag,
         original_file_names,
         ind_fish_ident,
         lakesize,
         lakesize_units,
         area_group,
         lat_unspec,
         lon_unspec,
         waterbody_type,
         location_notes_1,
         notes_1,
         obs_id)
glimpse(mi_data)

mi_data <- mi_data %>% 
  clean_names()
glimpse(mi_data)
```                        
                        
 

# Review & QC datasets
```{r}
mi_data %>% 
  distinct(total_effort_1, .keep_all = T) %>% 
  group_by(lake_name, lake_id, survey_id, total_effort_1_units) %>% 
  count()

#effort per survey type?
mi_data %>% 
  filter(!is.na(total_effort_1)) %>% 
  group_by(lake_name, lake_id, survey_id, total_effort_1, total_effort_1_units) %>% 
  summarise(n = n()) %>% 
  summarise(effort = sum(total_effort_1), counts = sum(n), grandCPUE = sum(n)/sum(total_effort_1))
        
                        
                        #effort per survey type (Walleye ONLY)?
mi_data %>% 
  filter(!is.na(total_effort_1) & species_1 == "walleye") %>% 
  group_by(lake_name, lake_id, survey_id, total_effort_1, total_effort_1_units) %>% 
  summarise(n = n()) %>% 
  summarise(effort = sum(total_effort_1), counts = sum(n), grandCPUE = sum(n)/sum(total_effort_1))
                        
                        
                        
#how many Walleye in surveys where we had effort data?
mi_data %>% 
  group_by(sampling_method) %>% 
  summarise(walleye = sum(species_1 == "walleye"))

mi_data %>% 
  group_by(species_1) %>% 
  count()
                        
                        
#whats the effort look like?
mi_data %>% 
  group_by(total_effort_1) %>% 
  count() %>% 
  print(n = nrow(.))
                        
                        # data coverage
                        # how many surveys were we missing effort data for? One survey, 3 gears. Survey 4042 on Twin Lake
mi_data %>% 
  filter(is.na(total_effort_1)) %>% 
  distinct(total_effort_ident, .keep_all = T) %>% 
  group_by(lake_name, survey_id, sampling_method, total_effort_1) %>% 
  count()
                        
                        
                        # how many surveys were we missing catch data from?
                        # how many surveys missing catchlength for?
mi_data %>% 
  group_by(original_file_names) %>% 
  count()

mi_data %>% 
  filter(!(grepl("mi_snt_catch_data_mar2021", original_file_names))) %>% 
  count()

mi_data %>% 
  summarise(catchNA = sum(!(grepl("mi_snt_catch_data_mar2021", original_file_names))),
            catchlengthNA = sum(!(grepl("mi_snt_catch_inchgrp_clean_0722", original_file_names))),
            effortNA = sum(!(grepl("mi_snt_effort_data_mar2021", original_file_names))))

```


# Import/Export files

```{r}


#save to disk:

# saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# mi_catch_eff_merge <- readRDS(file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")




mi_data <- as_arrow_table(mi_data)

write_dataset(dataset = mi_data, path = "Data_and_Scripts/Data/output/mi_file_arrow")

mi_data <- open_dataset("Data_and_Scripts/Data/output/mi_file_arrow")

glimpse(mi_data)
```



