---
title: "MI_Flat_File_Aggregation"
author: "Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
---
# Preamble

#a few notes
- Ages were excluded from this file
- We limited species in this because the files had different species extents
- In my dates exploration, I found a survey start and end dates, all surveys expect for 4 happen over a 7 day time frame
    -there is one gill netting case spans from May to Oct.
    -given the end date does not seem relevant to retain, I only keep survey start dates



##Libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(data.table)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)
library(mwlaxeref)

options(scipen = 999)
```


##Data
This could readily be changed into a function that takes a filepath and returns files into environment.
* note Holly has to change file paths to "D" 
```{r}
#generate a file list to import
files_list <- list.files(path = "D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MI_Data/mi_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list

#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MI_Data/mi_raw_disaggregated_data/",
                                          files_list[i])))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    data.table::transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 



```


#Data Review
```{r}
#effort:
mi_statustrends_effort_16Mar2021[ , .N , .(lake_id, lake_name.1, date.1 , survey_type.1, sampling_method_abbrev)  ]

#some lake IDs seem to have >1 name:
mi_statustrends_effort_16Mar2021[  ,length(unique(lake_id)), .(lake_name.1)][V1>1] # lake names not unique (they usually aren't)
mi_statustrends_effort_16Mar2021[  ,length(unique(lake_name.1)), .(lake_id)][V1>1] # lake ids not unique (they *should* be though)

#which lakes have multiple names for a single ID?
mi_statustrends_effort_16Mar2021[  ,length(unique(lake_name.1)), .(lake_id)][V1>1][,lake_id]
mi_statustrends_effort_16Mar2021[lake_id %in% 
                                   mi_statustrends_effort_16Mar2021[  ,length(unique(lake_name.1)), .(lake_id)][V1>1][,lake_id], , ]
# it appears here that a full survey (all gears were deployed in each part of these lakes. For that reason, My vote is that we keep them separate and use lake_id + lake_name for our key. 

# view each data level for a single survey
mi_statustrends_effort_16Mar2021[survey_id ==2446]
mi_statustrends_catch_16Mar2021[survey_id == 2446]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446, ]
mi_statustrends_lenage_20May2021[survey_id == 2446, ]

# do the counts match across all levels?
mi_statustrends_catch_16Mar2021[survey_id == 2446, sum(total_count.1)]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446, sum(total_count.1)]
nrow(mi_statustrends_lenage_20May2021[survey_id == 2446, ])

#no. not even close. I suspect this is bc of a species seletion that happened in the catch data:
mi_statustrends_catch_16Mar2021[survey_id == 2446, length(unique(species.1))]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446, length(unique(species.1))]
mi_statustrends_lenage_20May2021[survey_id == 2446, length(unique(species.1)) ]
#view species lists:
mi_statustrends_catch_16Mar2021[survey_id == 2446,sum(total_count.1) , species.1]
mi_statustrends_catchlengthclass_03July2023[survey_id == 2446,sum(total_count.1) , species.1]
mi_statustrends_lenage_20May2021[survey_id == 2446, .N , species.1 ] # these numbers will not be the same. This is because these aged fish are a subset.
#view species lists:
mi_statustrends_catch_16Mar2021[survey_id == 4079,sum(total_count.1) , species.1]
mi_statustrends_catchlengthclass_03July2023[survey_id == 4079,sum(total_count.1) , species.1]
mi_statustrends_lenage_20May2021[survey_id == 4079, .N , species.1 ] # these numbers will not be the same. This is because these aged fish are a subset.



# not exactly clear WHY the numbers still don't jive between the inch grp file and the catch file, BUT the catch file seems to have problems with the n per species X gear where there are zeros in an inchgroup (see survey 2446)


#survey_ids matched across all surveys?
mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id]
mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id]
mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ]
mi_statustrends_lenage_20May2021[ , .( lenage_nrows = .N) , survey_id] 


  
data_coverage <- 
merge(
  merge(
  merge(
    merge(mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id], 
          mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id], all = T),
    mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ], all = T), 
  mi_statustrends_lenage_20May2021[ , .( lenage_nrows = .N) , survey_id], all = T),
  snt_lakes_info_unique_id[,.(crosswalk_nrows = .N) , Survey_Number ],by.x = "survey_id", by.y = "Survey_Number", all = T
)

#retaining more dates?
survey_dates <- mi_statustrends_catch_16Mar2021 %>% 
  select(survey_id, date.1, sample_time_notes.1) %>% 
  mutate(start = as.Date(date.1),
         end = str_replace(sample_time_notes.1, "SAMPLE_END_DATE:", ""),
         end = as.Date(end, format = "%m/%d/%Y"),
         duration = end - start) %>% 
  distinct(survey_id, .keep_all = T)
survey_dates %>% 
  summarise(min.dur = min(duration), 
            max.dur = max(duration),
            over.week = sum(duration > 7))
#only 4 surveys that lasted longer than a week - two longest are survey id 4854 and 9316
mi_statustrends_catch_16Mar2021 %>% 
  filter(survey_id %in% c("4854", "9316")) %>% 
  group_by(survey_id, sampling_method_abbrev, date.1, sample_time_notes.1) %>% 
  count()
```


# Effort Merge
```{r}
#Start with a merge of catch and catch-length?

#file prep
#dates
# mi_statustrends_effort_16Mar2021[ , unique(date.1) , ]
mi_statustrends_effort_16Mar2021[ , date.1 := as.character(date.1) , ]
# mi_statustrends_catch_16Mar2021[ , unique(date.1) , ]
mi_statustrends_catch_16Mar2021[ , date.1 := as.character(as.IDate(date.1, format = "%m/%d/%Y")) , ]
#species        
#species is empty in effort
mi_statustrends_effort_16Mar2021[ , .N , species.1 ]
mi_statustrends_effort_16Mar2021[ , species.1 := NULL ,]

#view species lists:
mi_statustrends_catch_16Mar2021[ ,sum(total_count.1) , species.1]
mi_statustrends_catchlengthclass_03July2023[ ,sum(total_count.1) , species.1]
mi_statustrends_lenage_20May2021[ , .N , species.1 ] # these numbers will not be the same. This is because these aged fish are a subset.

#dump the species not of interest to us here:
mi_statustrends_catchlengthclass_03July2023 <- mi_statustrends_catchlengthclass_03July2023[species.1 %in% mi_statustrends_catch_16Mar2021[ , unique(species.1)]]

#column name diffs
colnames(mi_statustrends_catchlengthclass_03July2023)[colnames(mi_statustrends_catchlengthclass_03July2023)== "sampling_method.1" ] <- "sampling_method_abbrev"

#add lake_id to lengthclass file:
mi_statustrends_catchlengthclass_03July2023[mi_statustrends_effort_16Mar2021[ ,.N , .(lake_id, lake_name.1, survey_id)], on = c("lake_name.1", "survey_id"), lake_id := lake_id ] 
mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)]#still missing names

#any more available in catch file? NOPE
mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)][,survey_id]%in%
  mi_statustrends_catch_16Mar2021[ ,.N , .(lake_id, lake_name.1, survey_id)][ , survey_id]

#if drawn from effort sans survey ID (Risky, ambiguous join) can we get a 1:1 for lake name? or is there multi lakes for each name?
mi_statustrends_effort_16Mar2021[ lake_name.1 %in%
                                    mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)][,lake_name.1] , .N ,
                                  .(lake_id, county, lake_name.1, year, survey_id, survey_type.1) ]
#if drawn from catch sans survey ID (Risky, ambiguous join) can we get a 1:1 for lake name? or is there multi lakes for each name?
mi_statustrends_catch_16Mar2021[ lake_name.1 %in%
                                   mi_statustrends_catchlengthclass_03July2023[is.na(lake_id), .N , .(lake_name.1, survey_id)][,lake_name.1] , .N ,
                                 .(lake_id, county, lake_name.1, year, survey_id, survey_type.1) ]

#Conclusion-- don't do that. only grab the full keyed lake IDs from effort:
mi_statustrends_catchlengthclass_03July2023[mi_statustrends_catch_16Mar2021[ ,.N , .(lake_id, lake_name.1, survey_id)], on = c("lake_name.1", "survey_id"), lake_id := lake_id ]
#uncount catch
#uncount the catch files into an indiv-as-row format:
#with lengths scope
mi_statustrends_catchlengthclass_03July2023[ ,summary(total_count.1) , ]
mi_statustrends_catchlengthclass_03July2023[ , .N , total_count.1 == 0] #do we lose anything if we drop these?

#here we check if the surveyXsample methods are all covere in the >0 total_count.1 data (same dataset)
mi_statustrends_catchlengthclass_03July2023[total_count.1 == 0 , .N , .(survey_id, sampling_method_abbrev)]
sum(!mi_statustrends_catchlengthclass_03July2023[total_count.1 == 0 , paste(survey_id, sampling_method_abbrev)] %in% mi_statustrends_catchlengthclass_03July2023[total_count.1>0, paste(survey_id,sampling_method_abbrev)])
#how about in the effort data? There's no effort data that we would lose if we drop those zeros. 
sum(!mi_statustrends_catchlengthclass_03July2023[total_count.1 == 0 , paste(survey_id, sampling_method_abbrev)] %in% mi_statustrends_effort_16Mar2021[, paste(survey_id,sampling_method_abbrev)])

#execute the uncount, dropping those zero total counts in the mix (This line is generating a warning in the next line)
mi_statustrends_catchlengthclass_03July2023_uncount <- 
  uncount(mi_statustrends_catchlengthclass_03July2023[total_count.1!=0], total_count.1, .remove = T, .id = "ident_l")

#add a surveyxgear ident for indiv fish
mi_statustrends_catchlengthclass_03July2023_uncount[ , ident := seq_len(.N) , .(lake_name.1, lake_id, survey_id, sampling_method_abbrev, species.1) ]

#no lengths scope          
mi_statustrends_catch_16Mar2021[ , summary(total_count.1)]
#execute
mi_statustrends_catch_16Mar2021_uncount <- 
  uncount(mi_statustrends_catch_16Mar2021, total_count.1, .remove = T, .id = "ident")



#how many length data cover unknown surveys in catch data?

#here we check if the surveyXsample methods are all covered in the simple catch data
mi_statustrends_catchlengthclass_03July2023_uncount[ , .N , .(survey_id, sampling_method_abbrev)]
sum(!mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))] %in% mi_statustrends_catch_16Mar2021_uncount[, unique(paste(survey_id,sampling_method_abbrev))])

#how about in the effort data? There's no effort data that we would lose if we drop those zeros, but we do have 3 surveyXgears unique to the inchclass data. 
sum(!mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))] %in% mi_statustrends_effort_16Mar2021[, paste(survey_id,sampling_method_abbrev)])

# we can see that 3 surveys in inchclass data are not represented in catch or effort data
mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))][!mi_statustrends_catchlengthclass_03July2023_uncount[ , unique(paste(survey_id, sampling_method_abbrev))] %in% mi_statustrends_effort_16Mar2021[, paste(survey_id,sampling_method_abbrev)]]

mi_statustrends_effort_16Mar2021[survey_id == 4042]

mi_statustrends_catchlengthclass_03July2023_uncount[ survey_id == 4042, .N , .(species.1, sampling_method_abbrev) ]


#are there any surveys without any catch? Yes, 120 of them. (can we assume catch = zero there? Likely yes )
sum(!
      mi_statustrends_effort_16Mar2021[ , 
                                        unique(paste(survey_id, sampling_method_abbrev))
      ] %in%
      mi_statustrends_catch_16Mar2021_uncount[ ,
                                               unique(paste(survey_id, sampling_method_abbrev)) ,
      ]
)

mi_statustrends_catch_16Mar2021[ , summary(total_count.1) ,]

# and how many surveyX gears are not shown in the effort file? ZERO!                
sum(!mi_statustrends_catch_16Mar2021_uncount[ ,
                                              unique(paste(survey_id, sampling_method_abbrev)) ,
] %in%
  mi_statustrends_effort_16Mar2021[ , 
                                    unique(paste(survey_id, sampling_method_abbrev))
  ]

)
#this tells us we can skip expansion here. These no catch are already captured in the effort file

#merge catch files:                
names(mi_statustrends_catch_16Mar2021_uncount)
colnames(mi_statustrends_catchlengthclass_03July2023_uncount)


#set key columns
keycols <- c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev", "species.1", "ident")
setkeyv(mi_statustrends_catch_16Mar2021_uncount, keycols )
setkeyv(mi_statustrends_catchlengthclass_03July2023_uncount, keycols )

#merge together all catch data
mi_catch_merge <- merge(mi_statustrends_catch_16Mar2021_uncount, mi_statustrends_catchlengthclass_03July2023_uncount, by = keycols, all = T, suffixes = c("_catch", "_catchlengths"))


#add ages where we've got em' (ambiguous gears means we can't tie these to catch-- they're going to go into the mi flat file without gear assigned)
# #add an ident
# mi_statustrends_lenage_20May2021[ , ident := seq_len(.N) , .(lake_id, survey_id, sampling_method_abbrev, species.1) ]
# #set key columns
# keycols <- c("lake_id", "survey_id", "sampling_method_abbrev", "species.1", "ident")
# setkeyv(mi_statustrends_catch_16Mar2021_uncount, keycols )
# setkeyv(mi_statustrends_catchlengthclass_03July2023_uncount, keycols )
# 
# #merge together all catch data
# mi_catch_merge <- merge(mi_statustrends_catch_16Mar2021_uncount, mi_statustrends_catchlengthclass_03July2023_uncount, by = keycols, all = T, suffixes = c("_catch", "_catchlengths"))


#merge effort into this

#set key columns
keycols <- c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")
setkeyv(mi_catch_merge, keycols )
setkeyv(mi_statustrends_effort_16Mar2021, keycols )

#drop survey level info that will otherwise get duplicated upon  merge() (DONT DO THIS HERE, WILL LOSE DATA-- SEE data_coverage table)
data_coverage[ ,.N , .("effortNA" = is.na(effort_nrows), "catchlenNA" = is.na(catchlen_nrows))]
data_coverage[ ,.N , .("effortNA" = is.na(effort_nrows), "catchNA" = is.na(catch_nrows))]


#do merge
mi_catch_eff_merge <- merge(mi_catch_merge, mi_statustrends_effort_16Mar2021, by = keycols, all = T, suffixes = c("_mergedcatch", "_effort"))




### dataset cleanup and tidying                       

#check the product:
colnames(mi_catch_eff_merge)

mi_catch_eff_merge[ str_detect(lake_name.1, "ike" ), .(count = .N, missinglengths = sum(is.na(length.1)), meanL = mean(length.1)), .(lake_name.1, lake_id, date.1_effort, date.1_mergedcatch, survey_id, sampling_method_abbrev, species.1) ]


#did we retain all of the surveys? Looks like 498 unique survey IDs, and both the product and input reflect this:
mi_catch_eff_merge[ , .N , .(survey_id)]
merge(merge(mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id], 
            mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id], all = T),
      mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ], all = T)


#here's all of our effort:
mi_catch_eff_merge[ , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
#no catch data exist (or matched) for these data:
mi_catch_eff_merge[ is.na(species.1)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
#no_taxa_found
mi_catch_eff_merge[   , nothing_caught  := is.na(species.1) ,  ]


#no effort data were submitted for these fish:
mi_catch_eff_merge[ is.na(date.1_effort)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]


#clean up the column names and tidy the data table up a bit                 

sort(colnames(mi_catch_eff_merge))                 

#county     
mi_catch_eff_merge[ is.na(county_mergedcatch)|is.na(county_effort) , .N ,   ]
mi_catch_eff_merge[ !(is.na(county_mergedcatch)&is.na(county_effort)) & county_mergedcatch != county_effort  , .N ,   ]# no cases where the county doesn't match
mi_catch_eff_merge[  , .N , .(is.na(county_mergedcatch), is.na(county_effort))  ]#there are some cases where we haven't got a county at all, otherwise the effort county covers all. 
mi_catch_eff_merge[  , county_mergedcatch := NULL]
setnames(mi_catch_eff_merge, "county_effort", "county")

#data_type
setnames(mi_catch_eff_merge, "data_type", "data_type_effort")

#date.1
mi_catch_eff_merge[ is.na(date.1_mergedcatch)|is.na(date.1_effort) , .N ,   ]
mi_catch_eff_merge[ !(is.na(date.1_mergedcatch)&is.na(date.1_effort)) & date.1_mergedcatch != date.1_effort  , .N ,   ]# no cases where the dates don't match
mi_catch_eff_merge[  , .N , .(is.na(date.1_mergedcatch), is.na(date.1_effort))  ]#there are some cases where we haven't got a date at all, otherwise the effort info covers all. 
mi_catch_eff_merge[  , date.1_mergedcatch := NULL]
setnames(mi_catch_eff_merge, "date.1_effort", "date.1")

#date recieved
setnames(mi_catch_eff_merge, "date_recieved", "date_recieved_effort")

#effort units
mi_catch_eff_merge[ is.na(effort_units.1_mergedcatch)|is.na(effort_units.1_effort) , .N ,   ]
mi_catch_eff_merge[ !(is.na(effort_units.1_mergedcatch)&is.na(effort_units.1_effort)) & effort_units.1_mergedcatch != effort_units.1_effort  , .N ,   ]# no cases where the units don't match
mi_catch_eff_merge[  , .N , .(is.na(effort_units.1_mergedcatch), is.na(effort_units.1_effort))  ]#we have efforts for all. slide into single column. 

mi_catch_eff_merge[is.na(effort_units.1_effort), .N , ]
mi_catch_eff_merge[is.na(effort_units.1_effort) , effort_units.1_effort := effort_units.1_mergedcatch , ]
mi_catch_eff_merge[ , effort_units.1_mergedcatch := NULL , ]
setnames(mi_catch_eff_merge, "effort_units.1_effort", "effort_units.1")

#filenumber
setnames(mi_catch_eff_merge, "file_number", "file_number_effort")

#state
mi_catch_eff_merge[ ,state := "Michigan"  ] 
mi_catch_eff_merge[ , `:=` (state_catch = NULL, state_catchlengths = NULL)  , ]

#total effort
mi_catch_eff_merge[total_effort_1_effort != total_effort_1_mergedcatch, .N,  ]
plot(total_effort_1_effort ~ total_effort_1_mergedcatch, data = mi_catch_eff_merge  )
abline(1,0)

#effort vals
mi_catch_eff_merge[ total_effort_1_effort != total_effort_1_mergedcatch , .N , .(total_effort_1_mergedcatch, total_effort_1_effort)]
#I think that the merged catch values were assigned to indiv fish (like "this fish was caught in ONE net lift") and the effort file has survey X gear total efforts
mi_statustrends_catchlengthclass_03July2023[ , summary(total_effort_1) , ] 
mi_statustrends_catchlengthclass_03July2023[total_effort_1 > 1 , summary(total_count.1), sampling_method_abbrev]
# mi_statustrends_catch_16Mar2021[ , summary(total_effort_1.1) , ]    #this won run b/c no col for effort in that       
mi_statustrends_effort_16Mar2021[ , summary(total_effort_1) ,]          
# well--- I don't know what to make of all this, but for now I'll be keeping both of these "total_effort" variables, and leaning on the one originating in the effort file



#year

mi_catch_eff_merge[ year_effort != year_mergedcatch , ,]
mi_catch_eff_merge[ , .N , .(is.na(year_effort), is.na(year_mergedcatch))]
mi_catch_eff_merge[is.na(year_effort), year_effort := year_mergedcatch , ]
mi_catch_eff_merge[ ,  year_mergedcatch := NULL , ]
setnames(mi_catch_eff_merge, "year_effort", "year")


# most of this is waste-of-time junk. Let's move the big ones left and leave this mess hang out there to the right.
notgarbage <-  c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                 "sampling_method_abbrev", "total_effort_1_effort", "effort_units.1", "nothing_caught",  #gear
                 "species.1", "ident", "length.1", "length_unit.1", "ident_l" #fish
                 )

setcolorder(mi_catch_eff_merge, notgarbage)


#expand these data to cover all interested species in each surveyXgear

#check behavior now:
mi_catch_eff_merge[ species.1 == "WAE" , .N  , c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
#we can see that to gen a catch or CPUE dataset we can cast wide (like we did in MN)

#generate a species obs matrix
#clean dates:
mi_catch_eff_merge[ , unique(date.1) , ]

#execute
mi_catch_eff_merge[ , date_clean := as.IDate(date.1) ,]
mi_catch_eff_merge[ , summary(date_clean) , ]
mi_catch_eff_merge[ is.na(date_clean) , .N , .(survey_id, lake_name.1)] #missing effort data here, thus the gap


#tag codes with "taxon"
mi_catch_eff_merge[ , species.1 := paste("taxon_",species.1, sep = "")  ,]

#we called this "wide complete" in MN
wide_complete <- dcast(mi_catch_eff_merge[ ,.N , by = c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                                                        "sampling_method_abbrev", "total_effort_1_effort", "effort_units.1", "nothing_caught", 
                                                        "species.1")
                                           ] ,
                       ... ~ species.1 ,
                       value.var = "N",
                       fill = 0)

wide_complete[ , c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                   "sampling_method_abbrev", "total_effort_1_effort", "effort_units.1", "taxon_WAE") , ]

wide_complete[taxon_NA >0]

# now add in an effort identifier column:
            mi_catch_eff_merge[ , effort_ident := .GRP ,.(county,lake_id, lake_name.1, date.1, year, survey_id,  #survey 
                                                        sampling_method_abbrev, total_effort_1_effort, effort_units.1, nothing_caught) ]            
# now use Denver Links cleanup of these:
#changing species names to standard format                       
mi_catch_eff_merge <- mi_catch_eff_merge %>% 
  #renaming species abbreviations column - changed to common name in the following step
  rename(species.abbrev = species.1) %>% 
  #agency species code updated with common name
  mutate(species.1 = case_when(species.abbrev == "taxon_BCR" ~ "black_crappie",
                             species.abbrev == "taxon_BLG" ~ "bluegill",
                             species.abbrev == "taxon_CIS" ~ "cisco",
                             species.abbrev == "taxon_LMB" ~ "largemouth_bass",
                             species.abbrev == "taxon_NOP" ~ "northern_pike",
                             species.abbrev == "taxon_SMB" ~ "smallmouth_bass",
                             species.abbrev == "taxon_WAE" ~ "walleye",
                             species.abbrev == "taxon_YEP" ~ "yellow_perch",
                             species.abbrev == "taxon_NA" ~ NA,
                             TRUE ~ species.abbrev)) %>% 
  #Gear type code updated with spelled-out common name
  mutate(sampling_method = case_when(sampling_method_abbrev == "BOOMSHK" ~ "boomshocking",
                                            sampling_method_abbrev == "GLGNET" ~ "great_lakes_gill_net",
                                            sampling_method_abbrev == "IGNET" ~ "inland_gill_net",
                                            sampling_method_abbrev == "LMFYKE" ~ "large_mesh_fyke_net",
                                            sampling_method_abbrev == "SEINE" ~ "seine",
                                            sampling_method_abbrev == "SMFYKE" ~ "small_mesh_fyke_net",
                                            sampling_method_abbrev == "TRAPNET" ~ "trap_net")) %>% 
  #unit length 
  mutate(length_unit.1 = case_when(length_unit.1 == "col_name_Inch_Group" ~ "inch_group",
                                   TRUE ~ length_unit.1)) %>% 
  #effort unit 
  mutate(effort_units.1 = case_when(effort_units.1 == "HAULS" ~ "hauls",
                                    effort_units.1 == "MINUT" ~ "minute",
                                    effort_units.1 == "LIFTS" ~ "lifts")) %>%
  rename(. , original_file_name.1_effort = original_file_name.1) %>% 
  #selects for columns to be used during analysis - limits memory usage and allows creates more clarity
  #effort files for survey type, lat/long, and target species contained more data than the merged catch files
  select(state, county, lake_id, lake_name.1, date.1, year, survey_type.1_effort, survey_id, lat_unspec_effort, lon_unspec_effort, sampling_method, sampling_method_abbrev, target_species_effort, species.1, species.abbrev, length.1, length_unit.1, total_effort_1_effort, effort_units.1, effort_ident, nothing_caught, original_file_name.1_catch,original_file_name.1_catchlengths,original_file_name.1_effort)                        

#mi_catch_eff_merge[state == "Michigan", state := "mi"]

local_to_nhdhr(mi_catch_eff_merge[!is.na(lake_id)], from_colname = "lake_id", states = "mi"  ) %>%
  count(nhdhr.id)

mi_catch_eff_merge[!is.na(lake_id)  ,   nhdhr.id := local_to_nhdhr(mi_catch_eff_merge[!is.na(lake_id)], from_colname = "lake_id" , states = "mi" )$nhdhr.id ,  ]

#If there are NAs in the lake Ids you feed into these local to NHD functions, you'll end up raking a ton of extra nhdhr vals from the NA local ID column. 

#these are the unmatched lake_ids that we're left with.
mi_catch_eff_merge[is.na(nhdhr.id), .N , lake_id ]

#do we need to retain more date columns?
mi_statustrends_catch_16Mar2021_uncount %>% 
  group_by(survey_id, date.1) %>% 
  count()

mi_catch_eff_merge %>% 
  group_by(survey_id, effort_ident, sampling_method, date.1) %>% 
  count() 

#matching gear schema across states for the hive
mi_catch_eff_merge <- mi_catch_eff_merge %>% 
  rename(sampling_method_agency = sampling_method) %>% 
  mutate(sampling_method_simple = case_when(sampling_method_agency == "boomshocking" ~ "boat_electrofishing",
                                 sampling_method_agency == "great_lakes_gill_net" ~ "gill_net",
                                 sampling_method_agency == "inland_gill_net" ~ "gill_net",
                                 sampling_method_agency == "large_mesh_fyke_net" ~ "frame_net",
                                 sampling_method_agency == "seine" ~ "seine",
                                 sampling_method_agency == "small_mesh_fyke_net" ~ "frame_net",
                                 sampling_method_agency == "trap_net" ~ "frame_net",
                                 TRUE ~ NA),
         sampling_method = case_when(sampling_method_agency == "boomshocking" ~ "boat_electrofishing_boomshocking",
                                 sampling_method_agency == "great_lakes_gill_net" ~ "gill_net_great_lakes",
                                 sampling_method_agency == "inland_gill_net" ~ "gill_net_inland",
                                 sampling_method_agency == "large_mesh_fyke_net" ~ "fyke_net_large_mesh",
                                 sampling_method_agency == "seine" ~ "seine",
                                 sampling_method_agency == "small_mesh_fyke_net" ~ "fyke_net_small_mesh",
                                 sampling_method_agency == "trap_net" ~ "trap_net",
                                 TRUE ~ NA))

#cleaning up effort units
mi_catch_eff_merge <- mi_catch_eff_merge %>% 
  mutate(effort_units.1 = case_when(effort_units.1 == "minute" ~ "minutes",
                                    TRUE ~ effort_units.1))

#adding flags where they are needed
mi_catch_eff_merge <- mi_catch_eff_merge %>% 
  mutate(flag = case_when(survey_id == "4042" ~ "no effort data",
                          TRUE ~ NA)) %>% 
  group_by(sampling_method) %>% 
  mutate(mean_effort = mean(total_effort_1_effort, na.rm = TRUE),
         sd_effort = sd(total_effort_1_effort, na.rm = TRUE),
         z_score = (total_effort_1_effort - mean_effort) / sd_effort,
    flag = case_when(total_effort_1_effort <= 0 ~ "invalid effort (zero or negative)",
                     z_score > 5 ~ "high effort",
                     TRUE ~ flag)) %>% 
  group_by()

#adding lat and lon centroids

#lat and lon info coming from usgs data repo (https://www.sciencebase.gov/catalog/item/6206d3c2d34ec05caca53071) lake metadata file
lat_lon_usgs <- read_csv("lake_metadata.csv") %>% 
  filter(state == "MI") %>% 
  select(site_id,
         centroid_lon,
         centroid_lat) %>% 
  mutate(nhdhr.id = sub("^nhdhr_", "", site_id))

mi_catch_eff_merge <- mi_catch_eff_merge %>% 
  #adds lats/lons to data using nhdid
  left_join(lat_lon_usgs, "nhdhr.id") %>%
  #not all lakes have nhdids and/or there is no usgs lat/lon for them
  #here we take the usgs centroid where it exists and fill the lakes that do not have that data with survey level info from the agency 
  mutate(latitude_lake_centroid = centroid_lat,
         longitude_lake_centroid = centroid_lon,
         latitude_lake_centroid = case_when(is.na(latitude_lake_centroid) ~ lat_unspec_effort,
                                            TRUE ~ latitude_lake_centroid),
         longitude_lake_centroid = case_when(is.na(longitude_lake_centroid) ~ lon_unspec_effort,
                                             TRUE ~ longitude_lake_centroid))
rm(lat_lon_usgs)
#there is one survey_id (4042) that has fish but no effort, lake level info is retained by the effort file. Thus, we are not able to generate a lat/lon for that survey (three total effort idents in one lake)


#matching to schema
mi_data <- mi_catch_eff_merge %>% 
  mutate(lake_name = lake_name.1,
         date_survey = as.Date(date.1),
         date_total_effort_ident = as.Date(date.1),
         date_sub_effort_ident = as.Date(NA),
         date_sample = as.Date(NA),
         month = month(date.1),
         survey_id = as.character(survey_id),
         survey_type = survey_type.1_effort,
         survey_type_2 = as.character(NA),
         survey_type_3 = as.character(NA),
         survey_type_4 = as.character(NA),
         sampling_method_2 = sampling_method_agency,
         gear_data_notes = as.character(NA),
         target_species = target_species_effort,
         target_species_2 = as.character(NA),
         total_effort_ident = as.character(effort_ident),
         total_effort_1 = total_effort_1_effort,
         total_effort_2 = as.numeric(NA),
         total_effort_3 = as.numeric(NA),
         total_effort_1_units = effort_units.1,
         total_effort_2_units = as.character(NA),
         total_effort_3_units = as.character(NA),
         total_effort_nothing_caught = nothing_caught,
         water_temp = as.numeric(NA),
         water_temp_units = as.character(NA),
         water_clarity = as.numeric(NA),
         water_clarity_units = as.character(NA),
         lat_start = as.numeric(NA),
         lat_end = as.numeric(NA),
         lon_start = as.numeric(NA),
         lon_end = as.numeric(NA),
         site_id = as.character(NA),
         sub_effort_ident = as.character(NA),
         sub_effort_1 = as.numeric(NA),
         sub_effort_1_units = as.character(NA),
         sub_effort_2 = as.numeric(NA),
         sub_effort_2_units = as.character(NA),
         sub_effort_nothing_caught = as.logical(NA),
         length_1 = as.numeric(NA),
         length_unit_1 = as.character(NA),
         length_bin = as.character(length.1),
         length_bin_unit = "lower end of 1 inch length bin",
         age = as.numeric(NA),
         aging_structure_1 = as.character(NA),
         aging_structure_2 = as.character(NA),
         weight_1 = as.numeric(NA),
         weight_unit_1 = as.character(NA),
         batch_weight = as.character(NA),
         batch_weight_unit = as.character(NA),
         sex = as.character(NA),
         age_class = as.character(NA),
         ind_fish_ident = as.character(NA),
         lakesize = as.numeric(NA),
         lakesize_units = as.character(NA),
         area_group = as.character(NA),
         lat_unspec = lat_unspec_effort,
         lon_unspec = lon_unspec_effort,
         waterbody_type = as.character(NA),
         location_notes_1 = as.character(NA),
         notes_1 = as.character(NA),
         obs_id = as.character(row_number())) %>% 
   unite("original_file_names", c(original_file_name.1_catch,
                                      original_file_name.1_catchlengths,
                                      original_file_name.1_effort), 
                                      remove = T, 
                                      na.rm = T,
         sep = " ") %>% 
  select(state,
         county,
         lake_name,
         lake_id,
         nhdhr.id,
         latitude_lake_centroid,
         longitude_lake_centroid,
         date_survey,
         date_total_effort_ident,
         date_sub_effort_ident,
         date_sample,
         year,
         month,
         survey_id,
         survey_type,
         survey_type_2,
         survey_type_3,
         survey_type_4,
         sampling_method_simple,
         sampling_method,
         sampling_method_2,
         gear_data_notes,
         target_species,
         target_species_2,
         total_effort_ident,
         total_effort_1,
         total_effort_2,
         total_effort_3,
         total_effort_1_units,
         total_effort_2_units,
         total_effort_3_units,
         total_effort_nothing_caught,
         water_temp,
         water_temp_units,
         water_clarity,
         water_clarity_units,
         lat_start,
         lon_start,
         lat_end,
         lon_end,
         site_id,
         sub_effort_ident,
         sub_effort_1,
         sub_effort_1_units,
         sub_effort_2,
         sub_effort_2_units,
         sub_effort_nothing_caught,
         species.1,
         length_1,
         length_unit_1,
         length_bin,
         length_bin_unit,
         age,
         aging_structure_1,
         aging_structure_2,
         weight_1,
         weight_unit_1,
         batch_weight,
         batch_weight_unit,
         sex,
         age_class,
         flag,
         original_file_names,
         ind_fish_ident,
         lakesize,
         lakesize_units,
         area_group,
         lat_unspec,
         lon_unspec,
         waterbody_type,
         location_notes_1,
         notes_1,
         obs_id)
glimpse(mi_data)

mi_data <- mi_data %>% 
  clean_names()
glimpse(mi_data)
```                        
                        
 
# Review & QC datasets
```{r}
mi_data %>% 
  distinct(total_effort_1, .keep_all = T) %>% 
  group_by(lake_name, lake_id, survey_id, total_effort_1_units) %>% 
  count()

#effort per survey type?
mi_data %>% 
  filter(!is.na(total_effort_1)) %>% 
  group_by(lake_name, lake_id, survey_id, total_effort_1, total_effort_1_units) %>% 
  summarise(n = n()) %>% 
  summarise(effort = sum(total_effort_1), counts = sum(n), grandCPUE = sum(n)/sum(total_effort_1))
        
                        
                        #effort per survey type (Walleye ONLY)?
mi_data %>% 
  filter(!is.na(total_effort_1) & species_1 == "walleye") %>% 
  group_by(lake_name, lake_id, survey_id, total_effort_1, total_effort_1_units) %>% 
  summarise(n = n()) %>% 
  summarise(effort = sum(total_effort_1), counts = sum(n), grandCPUE = sum(n)/sum(total_effort_1))
                        
                        
                        
#how many Walleye in surveys where we had effort data?
mi_data %>% 
  group_by(sampling_method) %>% 
  summarise(walleye = sum(species_1 == "walleye", na.rm = T))

mi_data %>% 
  group_by(species_1, total_effort_nothing_caught) %>% 
  count()
                        
                        
#whats the effort look like?
mi_data %>% 
  group_by(total_effort_1) %>% 
  count() %>% 
  print(n = nrow(.))
                        
                        # data coverage
                        # how many surveys were we missing effort data for? One survey, 3 gears. Survey 4042 on Twin Lake
mi_data %>% 
  filter(is.na(total_effort_1)) %>% 
  distinct(total_effort_ident, .keep_all = T) %>% 
  group_by(lake_name, survey_id, sampling_method, total_effort_1) %>% 
  count()
                        
                        
                        # how many surveys were we missing catch data from?
                        # how many surveys missing catchlength for?
mi_data %>% 
  group_by(original_file_names) %>% 
  count()

mi_data %>% 
  filter(!(grepl("mi_snt_catch_data_mar2021", original_file_names))) %>% 
  count()

mi_data %>% 
  summarise(catchNA = sum(!(grepl("mi_snt_catch_data_mar2021", original_file_names))),
            catchlengthNA = sum(!(grepl("mi_snt_catch_inchgrp_clean_0722", original_file_names))),
            effortNA = sum(!(grepl("mi_snt_effort_data_mar2021", original_file_names))))

#how is the state name? 
mi_data %>% 
  group_by(state) %>% 
  count()
```


# Import/Export files

```{r}


#save to disk:

# saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# mi_catch_eff_merge <- readRDS(file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")




mi_data <- as_arrow_table(mi_data)

write_dataset(dataset = mi_data, path = "Data_and_Scripts/Data/output/mi_file_arrow")

mi_data <- open_dataset("Data_and_Scripts/Data/output/mi_file_arrow")

glimpse(mi_data)
```



