---
title: "SD_Flat_File_Aggregation"
author: "Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Update 7/22/24 (Denver)
I ran into an issue in the parquet file where total effort ident "1" had no survey level info. I did some exploration and found it was due to poor survey id crosswalks between the individual fish object and the effort object. The survey id linkage was 1.) null in fish obs or 2.) survey id in fish was not found in the effort object. The surveys causing issue were removed. They must be removed because they were creating an erroneous total effort ident grouping for the surveys that just didn't have any info

## To do list:



##Libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)
library(tidyverse)
library(data.table)
library(mwlaxeref)

options(scipen = 999)
```


##Data
This could readily be changed into a function that takes a filepath and returns files into environment.
* note Holly has to change file paths to "D" 
```{r}
#generate a file list to import
files_list <- list.files(path = "D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/SD_Data/sd_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list



#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/SD_Data/sd_raw_disaggregated_data/",
                                          files_list[i]),
         drop = c("V1")))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  #if the file is not in the data explainer, don't try to rename it:
  if(filei %in% cde$new_file_name) {
    print("renaming with data explainer")
  } else {next}
  
  
  
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    data.table::transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 



```

#Data Ag
This section has some data review and implementation of basic commands to get the data in fish per row format
```{r}

#clean out some file import stuff-
rm(cde,names, unusedbits, cols, filei, files_list, i, maxn, n)

# review each dataset that we have, strategizing about how you'll use them to develop a obs-level file
# consider things like species scope (do I need to restrict all input data to just the 8ish game species?), file organization (is this file already in a obs-level format or is it a count of each species/size that I should uncount()?), linking keys (is there a fish obs ID in the age data that I can use to link to the fish observations data?), and what things (posisbly whole datasets) are unneeded for our work, here. I have left the Michigan work in here to give you an idea of what I did:

# indiv fish records are here (bind brings together two datasets split to share): 
sd_catch_indivfish_2Nov2023 <- rbindlist(list(sd_catch1_indivfish_2Nov2023,sd_catch2_indivfish_2Nov2023))
  rm(sd_catch1_indivfish_2Nov2023, sd_catch2_indivfish_2Nov2023)
  names(sd_catch_indivfish_2Nov2023)


sd_catch_indivfish_2Nov2023[ , .N , .(survey_id, species.1) ]
sd_catch_indivfish_2Nov2023[ , .N , .(species.1) ] %>% print( n = nrow(.))

sd_catch_indivfish_2Nov2023[species.1 %in% c("<Null>", "None", "Uncoded"), species.1 := "Unknown" ]


#then there's the batch count data:
names(sd_catch3_batchcounts_2Nov2023)

sd_catch3_batchcounts_2Nov2023[ , .N , .(species.1) ] %>% print( n = nrow(.))
sd_catch3_batchcounts_2Nov2023[species.1 %in% c("<Null>", "None", "Uncoded"), species.1 := "Unknown" ]


sd_catch3_batchcounts_2Nov2023[, .N , .(survey_id, species.1) ]
  sd_catch3_batchcounts_2Nov2023[, .N , .(survey_id, species.1) ][N>1]
#based on this format I would not expect to see multi batch counts for one survey...
sd_catch3_batchcounts_2Nov2023[sd_catch3_batchcounts_2Nov2023[, .N , .(survey_id, species.1) ][N>1] , on = .(survey_id, species.1) , ][order(survey_id,species.1)]
#some are because of nulls   
#remove null records 
  sd_catch3_batchcounts_2Nov2023[total_count.1 == 0 , .N  ,]
  sd_catch3_batchcounts_2Nov2023[total_count.1 == 1 , .N  ,]
  sd_catch3_batchcounts_2Nov2023 <-  sd_catch3_batchcounts_2Nov2023[ total_count.1 > 0 ,  ]
  

  #okay, so there are two (or more) different vals for batch counts, let's check to make sure that the effort data are one gear per lake kinda thing on the survey_id field

  
#check effort data--is there more than one record per survey_id?
sd_effort_lakesurveysdata_2Nov2023[ , .N , .(survey_id) ][N>1]
sd_effort_lakesurveysdata_2Nov2023[survey_id == "{E02AD9C6-0877-475B-9BAA-1973CCA1DCFB}"] #only one case..we're going to drop that second record
sd_effort_lakesurveysdata_2Nov2023 <- sd_effort_lakesurveysdata_2Nov2023[ !garbage_bin_notes.1 ==  "OBJECTID *:374903" , ,]
#okay, but note that the survey_id is not unique at the lakeXgear level
sd_effort_lakesurveysdata_2Nov2023[ , length(unique(survey_id)) , .( lake_name.1, date(date.1)) ]#
# so that suggests that the survey_id is equivalent to our sub_effort_ident (i.e. we will need to generate a total_effort_ident filed that sums all sub_effort_idents in a lakeXgear)

#back to the batchcounts--we can see here that for a single "survey_id" (aka sub_effort_ident) we have mutliple batchcounts. That is okay, but it begs the Q of why? Are there three staff each counting their own tub of fish? Are these batches associated with a length bin or something (some data we can see that would key apart the mutliple batches in a single gear)? We'll ask Amy Gebhard about this!
#expand the batchcounts

sd_batchcounts_uncounted <- uncount(sd_catch3_batchcounts_2Nov2023, weights = total_count.1, .remove = T, .id = "uncount_ident")
rm(sd_catch3_batchcounts_2Nov2023)


sd_catch_indivfish_2Nov2023 <- rbindlist(list(sd_batchcounts_uncounted,sd_catch_indivfish_2Nov2023), fill = TRUE)
  rm(sd_batchcounts_uncounted)

#clean up species names to ease merge to age
sd_catch_indivfish_2Nov2023[ ,  species.1 := tolower(species.1)]


sd_length_age_2Nov2023[ , species.1 := gsub(" ","_", tolower(species.1)) ,]
  sd_length_age_2Nov2023[ , .N , species.1][order(species.1)]

#check length age species
sd_length_age_2Nov2023[ , .N , .(species.1) ] %>% print( n = nrow(.))
sd_length_age_2Nov2023[species.1 %in% c("<null>", "none", "uncoded"), species.1 := "unknown" ]
  sd_length_age_2Nov2023[ is.na(species.1), species.1 := "unknown"]
# sd_length_age_2Nov2023 <-    sd_length_age_2Nov2023[!(species.1 == "<null>" | is.na(species.1))]
  sd_catch_indivfish_2Nov2023[species.1 == "<null>" | is.na(species.1), .N]
# sd_catch_indivfish_2Nov2023 <-    sd_catch_indivfish_2Nov2023[!(species.1 == "<null>" | is.na(species.1))]

  
sd_catch_indivfish_2Nov2023[ , species.1 := gsub("_$", "" , gsub("__", "_", gsub(" ","_",gsub(paste(c("[(]", "[)]"), collapse = "|") , "_" , tolower(species.1))))) ,  ]  
  sd_catch_indivfish_2Nov2023[ , .N , species.1] %>% print(n = nrow(.))
  sd_catch_indivfish_2Nov2023[ , sort(unique(species.1))]

# the "lamprey family" aged critter does not have a parallel in the catch file
any(sd_length_age_2Nov2023[,unique(species.1) , ] %in% sd_catch_indivfish_2Nov2023[ , unique(species.1) ,] == F)
  sd_length_age_2Nov2023[,unique(species.1) , ][sd_length_age_2Nov2023[,unique(species.1) , ] %in% sd_catch_indivfish_2Nov2023[ , unique(species.1) ,] == F]

#check the survey_ids & scope the merge
  sd_length_age_2Nov2023[ ,unique(survey_id) ,] %in% sd_effort_lakesurveysdata_2Nov2023[ , unique(survey_id) ,]
  
#looks like the aged fish exist ONLY in that table (not in the catch tables!) Thats badass! Here are two randomly pulled examples
  sd_catch_indivfish_2Nov2023[survey_id == "{2F223C1F-2ADC-4A3B-BFB6-AC7E23C545BB}" & species.1 =="walleye" ,  , ]
    sd_length_age_2Nov2023[survey_id == "{2F223C1F-2ADC-4A3B-BFB6-AC7E23C545BB}" & species.1 =="walleye" ,  , ]
  sd_catch_indivfish_2Nov2023[survey_id == "{136411C8-5A7B-4A3A-AF99-DFBB0CDC11CA}" & species.1 =="black_crappie" ,  , ]
    sd_length_age_2Nov2023[survey_id == "{136411C8-5A7B-4A3A-AF99-DFBB0CDC11CA}" & species.1 =="black_crappie" ,  , ]
#this seems to apply even to the cases where there seems to be a str pulled from a fish but no age reported. DAYUM Amy, this looks super clean
  sd_catch_indivfish_2Nov2023[survey_id == "{710D141C-0109-4B79-B160-4527E93F23BC}" & species.1 =="yellow_perch" ,  , ]
    sd_length_age_2Nov2023[survey_id == "{710D141C-0109-4B79-B160-4527E93F23BC}" & species.1 =="yellow_perch" ,  , ]
#look into these structure-pulled-unaged-records and tidy up some nulls
  sd_catch_indivfish_2Nov2023[!is.na(aging_structure.1) , .N , .(aging_structure.1, uncount_ident)] #recall that all the batched fish had an uncount ident assigned to them!
    sd_catch_indivfish_2Nov2023[ aging_structure.1 == "otolith"]
    sd_catch_indivfish_2Nov2023[ aging_structure.1 %in% c("<Null>", "null"), aging_structure.1 := NA]
  
#now thin out the aged fish data a bit and rbind to the catch data
sd_length_age_2Nov2023 <- sd_length_age_2Nov2023[ , .(survey_id, species.1, length.1, length_unit.1, weight.1, weight_unit.1, age, aging_structure.1, sex, original_file_name.1 ) , ]

sd_catch_indivfish_2Nov2023 <- sd_catch_indivfish_2Nov2023[ ,.(survey_id, species.1, length.1, length_unit.1, weight.1, weight_unit.1, aging_structure.1, sex, original_file_name.1, notes.1)]

sd_catch_indivfish_2Nov2023 <- rbindlist(list(sd_catch_indivfish_2Nov2023, sd_length_age_2Nov2023), fill = TRUE)

rm(sd_length_age_2Nov2023)

#check out and pare down the effort data:
sd_effort_lakesurveysdata_2Nov2023[ , .N , .(sampling_method.1, sampling_method_abbrev) ][order(sampling_method_abbrev)] #no need to keep the samping method abbrev


sd_effort_lakesurveysdata_2Nov2023 <- sd_effort_lakesurveysdata_2Nov2023[ , .(survey_id, date.1, lake_id, lake_name.1, location_notes.1, county,  site_id.1, site_id.2, sampling_method.1, total_effort_1, effort_units.1, original_file_name.1) , ]


#check lake list info required?
sd_effort_lakesurveysdata_2Nov2023[ , .N , .(lake_id,lake_name.1)]
  sd_effort_lakesurveysdata_2Nov2023[ , unique(lake_id) ,]
sd_effort_lakesurveysdata_2Nov2023[ is.na(lake_id)] #one missing lake ID
  sd_lake_ID_list_11Jul2022[str_detect(lake_name.1,"lmore")] #not in the lake list that Holly made either
#we dont need the lake list
  rm(sd_lake_ID_list_11Jul2022)

  
#drop records for which we have no way to connect a fish to a location
  sd_catch_indivfish_2Nov2023[ str_detect(survey_id, "ull") ,.N , ]
  # sd_catch_indivfish_2Nov2023 <- sd_catch_indivfish_2Nov2023[ !str_detect(survey_id, "ull") , , ]
  
  
  sd_effort_lakesurveysdata_2Nov2023[str_detect(survey_id, "ull") , , ]
  
#now we merge the effort and catch:
  
  sd_merged_ce <- merge(sd_catch_indivfish_2Nov2023, sd_effort_lakesurveysdata_2Nov2023, by = ("survey_id"), all = TRUE, suffixes = c("indivfish","effort" ))
  
    #missing data
  sd_merged_ce[is.na(lake_id), .N, .(survey_id, lake_id, lake_name.1) ]
  # sd_merged_ce <- sd_merged_ce[ !is.na(date.1) ]
  
  #denver 7/22/24 - the above line identifies an issue - current parquet has missing survey info from 3934 obs (everything listed here besides Elmore Pond)
  #All of the records are listed under the same total effort ident
sd_merged_ce %>% 
  group_by(is.na(date.1)) %>% 
  count()
sd_merged_ce %>% 
  filter(is.na(date.1)) %>% 
  group_by(survey_id) %>% 
  count()
#removing records with NA dates removes 1.)null survey ids 2.)fish that had survey ids not present in lake survey data
#3934 obs from these survey ids is the exact amount of obs within total effort 1 of the parquet
 #this total effort ident was erroneously in previous iterations made due to the lack of data, i will remove them and proceed 

sd_merged_ce <- sd_merged_ce %>% 
  filter(!is.na(date.1))

  
#need a total effort ident in here:  
#can see multiple sampling events for one gear over a short span of dates
sd_merged_ce[ , .N , .(lake_id, lake_name.1, date(date.1), sampling_method.1) ][order(lake_id, date)]
  sd_merged_ce[lake_id == "ANR-Lake-4-000" , .N,  .(lake_id, lake_name.1, date(date.1), sampling_method.1) ][order(lake_id, date)]
#could roll together lakeXgearXyear?
sd_merged_ce[lake_id == "ANR-Lake-4-000" , .N,  .(lake_id, lake_name.1, year(date.1), sampling_method.1) ][order(lake_id, year)]


setnames(sd_merged_ce, old =  c("survey_id", "total_effort_1", "effort_units.1"),  new =  c("sub_effort_ident", "sub_effort_1", "sub_effort_units"))

sd_merged_ce[ ,`:=` (total_effort_ident = .GRP)  , .(lake_id, lake_name.1, year(date.1), sampling_method.1) ]

#any no catch data?
  sd_merged_ce[ , sum(!is.na(species.1)) , .(sub_effort_ident) ] #yes! every sub_effort_ident with a zero here is a no catch
sd_merged_ce[ , sub_nothing_caught := sum(!is.na(species.1))==0 , sub_effort_ident ]

#now calc effort for each total_effort_ident
sd_merged_ce[ , length(unique(sub_effort_ident)) , .(total_effort_ident)  ]
tot_eff_1 <- sd_merged_ce[ , .N , .(total_effort_ident,sub_effort_ident,sub_effort_1)  ][ ,  .("total_eff_1" = sum(sub_effort_1)) ,.(total_effort_ident)]
sd_merged_ce[tot_eff_1, on = ("total_effort_ident"), total_effort_1 := total_eff_1 , ]

#add a total effort nothing caught
sd_merged_ce[ , all(sub_nothing_caught)  , .(total_effort_ident)][ ,summary(V1)]
sd_merged_ce[ , total_effort_nothing_caught := all(sub_nothing_caught)  , .(total_effort_ident)]

#note here that because we have allowed NA species fish records to stay in the database as NAs, then used species as our gen of nothing caught, we have essentially said those records can stay in, but that they dont count towards catch. 



# units from file
sd_effort_lakesurveysdata_2Nov2023[ , summary(total_effort_1) , sampling_method.1 ] %>% print(n = nrow(.))

#this file is found in the SD folder on the drive - currently must be downloaded
gear_units <- fread("Data_and_Scripts/Data/input/SD_gear_unitizer.csv")

sd_merged_ce[gear_units, on = .(sampling_method.1 = sampling_method) , total_effort_1_units :=  sampling_method_2 ]

sd_merged_ce[ , .N , .(sampling_method.1, total_effort_1_units) ]
rm(gear_units, tot_eff_1)


unique(sd_merged_ce$lake_id)

# mwlaxeref::lake_id_xref %>% filter(state == "sd") %>% 
#   group_by(local.id) %>% count() %>% print(n = nrow(.))

```

## Revise after lake loc import sucess
```{r}

#most have matches in the lake locations file
sd_locs <- sd_lakelocations_1Dec2023

sd_merged_ce[, unique(lake_id) , ]%in%sd_locs$lake_id

sd_merged_ce[sd_locs, on = .(lake_id = lake_id ) , `:=` ("lat_unspec" = lat_unspec, "lon_unspec" = lon_unspec, "lakesize" = c_lakesize, "lakesize_units" = c_lakesize_units) ]

# export a few unmatched ones for stste verification:
fwrite(sd_merged_ce[is.na(lat_unspec), .N , .(lake_id, lake_name.1, county, location_notes.1)], file = "Data_and_Scripts/Data/output/sd_lakes_loc_unknown.csv")

#now try a lakenameXcounty combo:
sd_merged_ce[sd_locs, on = .(lake_name.1 = lake_name.1, county = county ) , `:=` ("lat_2" = lat_unspec, "lon_2" = lon_unspec) ]
#move these into position, remove old, summarize
sd_merged_ce[is.na(lon_unspec), `:=` ("lat_unspec" = lat_2, "lon_unspec" = lon_2) ]
sd_merged_ce[ ,`:=` (lat_2= NULL, lon_2 = NULL)]
sd_merged_ce[ , .N , is.na(lat_unspec) ]

#adding crosswalk from MSU team
crosswalk_msu <- read_csv("MGLP_FISH_LAKES_12Aug24.csv") %>% 
  filter(STATE == "SD") %>% 
  rename(lake_id = STATE_ID,
         nhdhr_id = NHDHR_ID,
         lake_name_msu = LAKE_NAME,
         county_msu = COUNTY) %>% 
  select(lake_id,
         nhdhr_id,
         lake_name_msu,
         county_msu,
         LAT,
         LON)

sd_merged_ce <- sd_merged_ce %>% 
  left_join(crosswalk_msu, by = "lake_id", relationship = "many-to-one")

sd_merged_ce %>% 
  distinct(lake_id, .keep_all = T) %>%
  group_by(is.na(lat_unspec), is.na(LAT)) %>% 
  count()
#looks like I would gain 9 lat/lons by taking msu provided coords as a supplement

sd_merged_ce <- sd_merged_ce %>% 
  mutate(lat_unspec = case_when(is.na(lat_unspec) ~ LAT,
                                TRUE ~ lat_unspec),
         lon_unspec = case_when(is.na(lon_unspec) ~ LON,
                                TRUE ~ lon_unspec))

sd_merged_ce %>% 
  distinct(lake_id, .keep_all = T) %>%
  group_by(is.na(lat_unspec), is.na(LAT)) %>% 
  count()
#9 more lat/lons

sd_merged_ce %>% 
  distinct(lake_id, .keep_all = T) %>% 
  group_by(lake_name.1, lake_name_msu, county, county_msu) %>% 
  count() %>% 
  filter(lake_name.1 != lake_name_msu | county != county_msu) %>% 
  print(n = nrow(.))
#great agreement in naming across the board - only difference is an '

rm(crosswalk_msu)

#now do a similar thing with USGS 
lat_lon_usgs <- read_csv("lake_metadata.csv") %>% 
  filter(state == "SD") %>%
  select(site_id, 
         centroid_lat,
         centroid_lon) %>% 
  rename(nhdhr_id = site_id)

sd_merged_ce <- sd_merged_ce %>% 
  left_join(lat_lon_usgs, by = "nhdhr_id") %>% 
  mutate(latitude_lake_centroid = case_when(is.na(lat_unspec) ~ centroid_lat,
                                           TRUE ~ lat_unspec),
         longitude_lake_centroid = case_when(is.na(lon_unspec) ~ centroid_lon,
                                             TRUE ~ lon_unspec))

sd_merged_ce %>% 
  distinct(lake_id, .keep_all = T) %>% 
  group_by(is.na(latitude_lake_centroid), is.na(lat_unspec)) %>% 
  count()
#we didn't really gain anything from USGS coordinates - how do the coords match up?

coord_match_check<- sd_merged_ce %>% 
  distinct(lake_id, .keep_all = T) %>% 
  select(lake_id, lake_name.1, lat_unspec, centroid_lat, lon_unspec, centroid_lon) %>% 
  mutate(lat_diff = abs(lat_unspec - centroid_lat),
         lon_diff = abs(lon_unspec - centroid_lon))
rm(coord_match_check)
#all lakes that have nhdids are checking out nicely 
rm(lat_lon_usgs)

sd_merged_ce <- sd_merged_ce %>%
  #cleaning up some null columns up
  mutate(weight.1 = case_when(weight.1 == "<Null>" ~ NA,
                              TRUE ~ weight.1),
         sex = case_when(sex == "Null" ~ NA,
                         sex == "<Null>" ~ NA,
                         TRUE ~ sex),
         aging_structure.1 = case_when(aging_structure.1 == "<Null>" ~ NA,
                                       TRUE ~ aging_structure.1),
         notes.1 = case_when(notes.1 == "SerialNumber:<Null>" ~ NA,
                             notes.1 == "SerialNumber:NA" ~ NA,
                             TRUE ~ notes.1)) %>% 
  #cleaning up some unit columns
  mutate(lakesize_units = case_when(!is.na(lakesize) ~ "acres",
                                    TRUE ~ NA),
         length_unit.1 = case_when(!is.na(length.1) ~ "mm",
                                    TRUE ~ NA),
         weight_unit.1 = case_when(!is.na(weight.1) ~ "g",
                                    TRUE ~ NA),
         total_effort_1_units = case_when(total_effort_1_units == "net-night" ~ "net_nights",
                                          total_effort_1_units == "haul" ~ "hauls",
                                          TRUE ~ total_effort_1_units))


#dates
sd_merged_ce <- sd_merged_ce %>% 
  arrange(date.1) %>% 
  mutate(date.1 = format(date.1, "%Y-%m-%d"),
         date.1 = as.Date(date.1),
         date_sub_effort_ident = date.1,
         date_sample = as.Date(NA)) %>% 
  group_by(total_effort_ident) %>% 
  mutate(date_total_effort_ident = first(date.1)) %>% 
  ungroup() %>% 
  group_by(lake_id, year(date.1)) %>% 
  mutate(date_survey = first(date.1)) %>% 
  ungroup()

#gear clean up
sd_merged_ce %>% 
  distinct(sampling_method.1) %>% 
  arrange(sampling_method.1) %>% 
  print(n = nrow(.))

#this file lives in the crosswalk folder in the drive
#here we read in a gear cross walk, the file contains each unique gear and assigns it new labels that are consistent across states
gear_xwalk <- read_csv("gears_by_state.csv") %>% 
  filter(state == "South_Dakota") %>% 
  rename(sampling_method.1 = gear_1) %>% 
  select(sampling_method.1, sampling_method_simple, sampling_method_1, sampling_method_2)

sd_merged_ce <- sd_merged_ce %>% 
  left_join(gear_xwalk)

sd_merged_ce %>% 
  distinct(sampling_method.1, sampling_method_simple, sampling_method_1, sampling_method_2) %>% 
  arrange(sampling_method.1) %>% 
  print(n = nrow(.))

rm(gear_xwalk)

#creating an auto flagging system
flag_xwalk <- sd_merged_ce %>% 
  distinct(total_effort_ident, .keep_all = TRUE) %>% 
  group_by(sampling_method_simple) %>% 
  mutate(mean_effort = mean(total_effort_1, na.rm = TRUE),
         sd_effort = sd(total_effort_1, na.rm = TRUE),
         z_score = (total_effort_1 - mean_effort) / sd_effort,
         flag = NA, #creates a flag column so I can case_when the flag generation 
         flag = case_when(total_effort_1 < 0 ~ paste("invalid effort (negative)", coalesce(flag, "")),
                          total_effort_1 == 0 ~ paste("effort reported as 0", ";", coalesce(flag, "")),
                          z_score > 10 ~ paste("high effort", ";", coalesce(flag, "")),
                          is.na(z_score) ~ coalesce(flag, ""),
                          TRUE ~ coalesce(flag, "")
         ),
         flag = na_if(flag, "")) %>% 
  select(total_effort_ident,
         sampling_method_simple,
         flag,
         total_effort_1,
         mean_effort,
         sd_effort,
         z_score) %>% 
  filter(!is.na(flag))

flag_xwalk_2 <- sd_merged_ce %>% 
  distinct(total_effort_ident, .keep_all = TRUE) %>%
  filter(!(total_effort_ident %in% flag_xwalk$total_effort_ident)) %>% 
  group_by(sampling_method_simple) %>% 
  mutate(mean_effort = mean(total_effort_1, na.rm = TRUE),
         sd_effort = sd(total_effort_1, na.rm = TRUE),
         z_score = (total_effort_1 - mean_effort) / sd_effort,
         flag = NA,
         flag = case_when(total_effort_1 < 0 ~ paste("invalid effort (negative)", coalesce(flag, "")),
                          total_effort_1 == 0 ~ paste("effort reported as 0", ";", coalesce(flag, "")),
                          z_score > 10 ~ paste("high effort", ";", coalesce(flag, "")),
                          is.na(z_score) ~ coalesce(flag, ""),
                          TRUE ~ coalesce(flag, "")
         ),
         flag = na_if(flag, "")) %>% 
  select(total_effort_ident,
         sampling_method_simple,
         flag,
         total_effort_1,
         mean_effort,
         sd_effort,
         z_score) %>% 
  filter(!is.na(flag))

flag_xwalk_effort <- rbind(flag_xwalk, flag_xwalk_2) %>% 
  ungroup() %>% 
  select(total_effort_ident, 
         flag)
rm(flag_xwalk, flag_xwalk_2)

sd_merged_ce <- sd_merged_ce %>% 
  left_join(flag_xwalk_effort,
            relationship = "many-to-one")

sd_merged_ce %>% 
  filter(!is.na(flag)) %>% 
  group_by(total_effort_ident, flag) %>% 
  count()

sd_merged_ce %>% 
  filter(total_effort_ident %in% flag_xwalk_effort$total_effort_ident)
#flag gets applied to 12350 fish records
rm(flag_xwalk_effort)

#flag for fish
length_weight_flag_1 <- sd_merged_ce %>% 
  filter(!is.na(species.1)) %>% 
  group_by(species.1) %>% 
  mutate(mean_length = mean(length.1, na.rm = TRUE),
         weight.1 = as.numeric(weight.1),
         mean_weight = mean(weight.1, na.rm = TRUE),
         sd_length = sd(length.1, na.rm = TRUE),
         sd_weight = sd(weight.1, na.rm = TRUE),
         z_score_length = (length.1 - mean_length) / sd_length,
         z_score_weight = (weight.1 - mean_weight) / sd_weight,
         flag = case_when(z_score_length >= 10 | z_score_length <= -10 ~ paste("unlikely length", ";", coalesce(flag, "")),
                          z_score_weight >= 10 | z_score_weight <= -10 ~ paste("unlikely weight", ";", coalesce(flag, "")),
                          TRUE ~ coalesce(flag, "")),
         flag = case_when(length.1 == 0 | weight.1 == 0 ~ paste("length or weight reported as 0", ";", coalesce(flag, "")),
                          TRUE ~ coalesce(flag, "")),
         flag = na_if(flag, "")) %>% 
  select(total_effort_ident,
         total_effort_1,
         species.1,
         weight.1,
         length.1,
         mean_length,
         mean_weight,
         z_score_length,
         z_score_weight,
         flag) %>% 
  filter(!is.na(flag)) %>% 
  distinct()

sd_merged_ce <- sd_merged_ce %>% 
  select(-flag) %>% 
  mutate(weight.1 = as.numeric(weight.1)) %>% 
  left_join(length_weight_flag_1,
            relationship = "many-to-one")

flag_check <- sd_merged_ce %>% 
  filter(!is.na(flag)) %>% 
  group_by(total_effort_ident, sampling_method_1, species.1, total_effort_1, length.1, weight.1, flag) %>% 
  count()
rm(flag_check)

sd_merged_ce %>% 
  group_by(flag) %>% 
  count()

sd_merged_ce %>% 
  filter(str_detect(flag, "unlikely")) %>% 
  group_by(species.1, length.1, length_unit.1, weight.1, weight_unit.1, flag) %>% 
  count() %>% 
  arrange(flag) %>% 
  print(n = nrow(.))
```

## Match Columns to parquet schema
```{r}
# clean up these names and match overall schema
#need to 1. fix current cols to match 2. add missing cols 3.drop all extras 
sd_merged_ce <- clean_names(sd_merged_ce) #clean up names a touch

sd_merged_ce <- as.data.table(sd_merged_ce)

#making site_id_1
sd_merged_ce[ , site_id_1:= paste(site_id_1,site_id_2, sep = ":") , ]
#making original file name
sd_merged_ce[ , original_file_name := paste(original_file_name_1indivfish, original_file_name_1effort , sep = ":") , ]

#Denver's touch up on column names
sd_merged_ce_trial <- sd_merged_ce %>% 
  mutate(state = "South Dakota",
         lake_name = lake_name_1,
         year = year(date_1),
         month = month(date_1),
         survey_id = as.character(NA),
         survey_type = as.character(NA),
         survey_type_2 = as.character(NA),
         survey_type_3 = as.character(NA),
         survey_type_4 = as.character(NA),
         sampling_method = sampling_method_1_2,
         gear_data_notes = as.character(NA),
         target_species = as.character(NA),
         target_species_2 = as.character(NA),
         total_effort_ident = as.character(total_effort_ident),
         total_effort_2 = as.numeric(NA),
         total_effort_3 = as.numeric(NA),
         total_effort_1_units = total_effort_1_units,
         total_effort_2_units = as.character(NA),
         total_effort_3_units = as.character(NA),
         water_temp = as.numeric(NA),
         water_temp_units = as.character(NA),
         water_clarity = as.numeric(NA),
         water_clarity_units = as.character(NA),
         lat_start = as.numeric(NA),
         lon_start = as.numeric(NA),
         lat_end = as.numeric(NA),
         lon_end = as.numeric(NA),
         site_id = site_id_1,
         sub_effort_1_units = total_effort_1_units,
         sub_effort_2 = as.numeric(NA),
         sub_effort_2_units = as.character(NA),
         sub_effort_nothing_caught = sub_nothing_caught,
         length_bin = as.character(NA),
         length_bin_unit = as.character(NA),
         aging_structure_2 = as.character(NA),
         weight_1 = as.numeric(weight_1),
         batch_weight = as.character(NA),
         batch_weight_unit = as.character(NA),
         age_class = as.character(NA),
         original_file_names = original_file_name,
         ind_fish_ident = as.character(NA),
         area_group = as.character(NA),
         waterbody_type = as.character(NA),
         obs_id = as.character(row_number())) %>% 
  ungroup() %>% 
  select(state, 
         county, 
         lake_name,
         lake_id,
         nhdhr_id,
         latitude_lake_centroid,
         longitude_lake_centroid,
         date_survey,
         date_total_effort_ident,
         date_sub_effort_ident,
         date_sample,
         year,
         month,
         survey_id,
         survey_type,
         survey_type_2,
         survey_type_3,
         survey_type_4,
         sampling_method_simple,
         sampling_method,
         sampling_method_2,
         gear_data_notes,
         target_species,
         target_species_2,
         total_effort_ident,
         total_effort_1, 
         total_effort_2,
         total_effort_3,
         total_effort_1_units,
         total_effort_2_units,
         total_effort_3_units,
         total_effort_nothing_caught,
         water_temp,
         water_temp_units,
         water_clarity,
         water_clarity_units,
         lat_start,
         lon_start,
         lat_end,
         lon_end,
         site_id,
         sub_effort_ident,
         sub_effort_1,
         sub_effort_1_units,
         sub_effort_2,
         sub_effort_2_units,
         sub_effort_nothing_caught,
         species_1,
         length_1,
         length_unit_1,
         length_bin,
         length_bin_unit,
         age,
         aging_structure_1,
         aging_structure_2,
         weight_1,
         weight_unit_1,
         batch_weight,
         batch_weight_unit,
         sex,
         age_class,
         flag,
         original_file_names,
         ind_fish_ident,
         lakesize,
         lakesize_units,
         area_group,
         lat_unspec,
         lon_unspec,
         waterbody_type,
         location_notes_1,
         notes_1,
         obs_id
         )
#things to do
#1. make sure the dates are the appropriate level
#3. ensure I am keeping the correct sampling method (see code below)

#exploring dates and subefforts
dates <- sd_merged_ce_trial %>% 
  distinct(total_effort_ident, sub_effort_ident, site_id, .keep_all = T) %>% 
  select(total_effort_ident, sub_effort_ident, site_id, date_survey, date_total_effort_ident, date_sub_effort_ident)
rm(dates)
#total effort 3992 is a good example of how the dates are structured 
#survey date is the first sampling within a lake-year
#total effort ident date is the first date from the sub effort date
#sub effort ident date is the date the was reported - when the net was lifted


#gear corrections - this is old code as of 9/6/24


#sd_gears <- fread( file = "Data_and_scripts/Data/input/ia_sd_gear_newnames2.csv") #read in 

#sd_gears <- sd_gears[state == "South Dakota"]

#sd_merged_ce_trial[ , .N , sampling_method_2 ]
#sd_merged_ce_trial[ , sampling_method_2 := as.character(sampling_method_2) , ]

#sd_merged_ce_trial[sd_gears, on = .(sampling_method = old_gear), sampling_method_2 := new_gear]

gear <- sd_merged_ce_trial %>% 
  distinct(total_effort_ident, sub_effort_ident, .keep_all = T) %>% 
  group_by(sampling_method_simple, sampling_method, sampling_method_2) %>% 
  count()
rm(gear)

sd_merged_ce_trial %>% 
  group_by(is.na(date_survey), is.na(date_total_effort_ident)) %>% 
  count()

sd_merged_ce_trial %>% 
  group_by(total_effort_1_units, sub_effort_1_units) %>% 
  count() %>% 
  collect()

sd_merged_ce_trial %>% 
  filter(str_detect(flag, "effort")) %>% 
  group_by(total_effort_ident, sampling_method, total_effort_1, total_effort_1_units, flag) %>% 
  count() %>% 
  print(n = nrow(.))

sd_merged_ce_trial %>% 
  filter(str_detect(flag, "unlikely")) %>% 
  group_by(total_effort_ident, species_1, length_1, length_unit_1, weight_1, weight_unit_1, flag) %>% 
  count() %>% 
  print(n = nrow(.))
```


##export as Parquet
```{r}
write_dataset(dataset = sd_merged_ce_trial, path = "Data_and_Scripts/Data/output/sd_file_arrow")

sd_data <- open_dataset(sources = "Data_and_Scripts/Data/output/sd_file_arrow/")

glimpse(sd_data)

```

                
 


# Review & QC datasets
```{r}
#below is some code Mike to match schema


# then open into excel an align with the schema (see these IA example files)
# fwrite(as.data.table(names(sd_merged_ce)), file = "Data_and_scripts/Data/output/sd_names.csv")

# use the exported csv to align with the column names in the column schema GSheet (https://docs.google.com/spreadsheets/d/1zfevASMxRMxMYNWm3Hq1yR2Qk0zcJV_JnO0LBeODaPQ/edit#gid=0) . 
# - if you delete a column from your state's names in the sheet, the code below will drop those cols automatically from the data product.
# - put an NA in the table in each place where your data have no equivalent col and the code below will fill those with NAs in the data product
# - keep the order of the table in line with the column schema GSheet and that will automatically reorder your data to match the schema GSheet
# after aligning the names with the schema bring that back in (note that the )
sd_renamer <- fread( file = "Data_and_scripts/Data/input/sd_renamer.csv") #read in 
setnames(sd_renamer, "fields in all parquet", "schema")
sd_renamer[south_dakota == "", south_dakota := NA ]


#drop extras
dropcols <- names(sd_merged_ce)[!(names(sd_merged_ce) %in% sd_renamer[,  south_dakota])]
sd_merged_ce[ , (dropcols) := NULL , ]



#rename ia to fit the schema
sd_renamer[match(names(sd_merged_ce),sd_renamer[,south_dakota]), schema]

setnames(sd_merged_ce, 
         old = names(sd_merged_ce),
         new = sd_renamer[match(names(sd_merged_ce),sd_renamer[,south_dakota]), schema])


#missing cols
sd_renamer[,schema] %in% names(sd_merged_ce)
names(sd_merged_ce) %in% sd_renamer[,schema]

#add misssing names:
addcols <- sd_renamer[is.na(south_dakota) , schema ,]
sd_merged_ce[ ,(addcols) := NA , ]

newcolorder <- sd_renamer[ ,schema]


setcolorder(sd_merged_ce, newcolorder)


#generate an obs_id

sd_merged_ce[ , .N , obs_id ]
sd_merged_ce[ , obs_id := .I , ]

#set state
sd_merged_ce[ , state := as.character(state) , ]
sd_merged_ce[ , state := "South Dakota"  , ]

#get the surveys and gears and show how many surveys and how many of each species was caught in those surveys 

#total effort review:
sd_data %>% 
  group_by(total_effort_ident,total_effort_1,total_effort_1_units, sampling_method, year(date_total_effort_ident), lake_name, lake_id) %>% 
  count() %>%
  collect()
  

# ensure nothing caught is assigned at the total effort ident level
sd_data %>%
  group_by(total_effort_ident) %>% 
  summarise(n_distinct(total_effort_nothing_caught)) %>% 
  collect()

sd_data %>% 
  distinct(total_effort_ident, sampling_method) %>% 
  group_by(total_effort_ident, sampling_method) %>% 
    count() %>% 
    collect() %>% 
  filter(n >1)
#silly check but its good there aren't any tots effort idents with more than one sampling method

sd_data %>% 
  group_by(total_effort_ident,total_effort_1,total_effort_1_units, sampling_method) %>%
  summarise(walleye = sum(species_1 == "walleye", na.rm = T),
            yellow_perch = sum(species_1 == "yellow_perch", na.rm = T),
            black_crappie = sum(species_1 == "black_crappie", na.rm = T),
            bluegill = sum(species_1 == "bluegill", na.rm = T),
            smallmouth_bass = sum(species_1 == "smallmouth_bass", na.rm = T),
            largemouth_bass = sum(species_1 == "largemouth_bass", na.rm = T),
            northern_pike = sum(species_1 == "northern_pike", na.rm = T),
            lake_whitefish = sum(species_1 == "lake_whitefish", na.rm = T)
            ) %>%
  collect()

  sd_data %>% 
  group_by(state, sampling_method, sampling_method_2) %>%
  summarise(
    n_effort_idents = n_distinct(total_effort_ident),
    n_walleye = sum(species_1 == "walleye", na.rm = T),
    n_yellow_perch = sum(species_1 == "yellow_perch", na.rm = T),
    n_black_crappie = sum(species_1 == "black_crappie", na.rm = T),
    n_bluegill = sum(species_1 == "bluegill", na.rm = T),
    n_smallmouth_bass = sum(species_1 == "smallmouth_bass", na.rm = T),
    n_largemouth_bass = sum(species_1 == "largemouth_bass", na.rm = T),
    n_northern_pike = sum(species_1 == "northern_pike", na.rm = T),
    n_lake_whitefish = sum(species_1 == "lake_whitefish", na.rm = T)
  ) %>%
    collect() %>% 
    {. ->> sd_sample_sz}



       

```


# Import/Export files

```{r}

# 
# #save to disk:
# 
# # saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# # mi_catch_eff_merge <- readRDS(file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# 
# 
# str(mi_catch_eff_merge)
# 
# 
# mi_catch_eff_merge <- as_arrow_table(mi_catch_eff_merge)
# 
# write_dataset(dataset = mi_catch_eff_merge, path = "Data_and_Scripts/Data/output/mi_file_arrow")
# 
# mi_data <- open_dataset("Data_and_Scripts/Data/output/mi_file_arrow")
# 
# glimpse(mi_data)

```



# Old code
```{r}
#found in ag chunk during review

#Holly's exploration for filter workshop
sd_catch_indivfish_2Nov2023 %>%
  group_by(garbage_bin_notes.1)%>%
  summarise(total = n())%>%
  filter(total > 1)%>%
  print(n=nrow(.)) #appears that this "object ID" is an individual ID for each fish

sd_catch_indivfish_2Nov2023 %>%
  group_by(notes.1)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #serial number not unique to the fish, no helpful notes

sd_catch_indivfish_2Nov2023 %>%
  group_by(mark_recap_data_notes.1)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #mark recap data notes just lists the type of tag used

sd_catch_indivfish_2Nov2023 %>%
  group_by(mark_recap_data_notes.2)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #mark recap data notes 2 is just the number on the tag

sd_catch_indivfish_2Nov2023 %>%
  group_by(mark_recap_data_notes.3)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #mark recap notes 3 is tag color, all are null or NA, one is orange


# ## HOlly exploration for filtering workshop
# Notes_SD_Effort <- sd_effort_lakesurveysdata_2Nov2023 %>%
#   group_by(garbage_bin_notes.3)%>%
#   summarise(Number_of_instances = n())%>%
#   mutate(garbage_bin_notes.3 = str_replace_all(garbage_bin_notes.3, "\\\\", "/"))%>%
#   separate_wider_delim(garbage_bin_notes.3, delim = "/", names =c("pt1", "pt2", "pt3", "pt4", "pt5", "pt6", "pt7", "pt8", "pt9", "pt10", "pt11", "pt12", "p13"), too_few = "align_end") #may contain survey purpose info or filtering info
# 
# #write_csv(Notes_SD_Effort, "SD_effort_file_path.csv")
# ####
# sd_effort_lakesurveysdata_2Nov2023 <- sd_effort_lakesurveysdata_2Nov2023[ , .(survey_id, date.1, lake_id, lake_name.1, location_notes.1, county,  site_id.1, site_id.2, sampling_method, total_effort_1, effort_units.1, original_file_name.1) , ]

#Holly's exploration for filter workshop
SD_Gears <- sd_merged_ce %>%
  group_by(sampling_method.1, species.1)%>%
  summarise(Total_Obs = n())

SD_Gears_2 <- sd_merged_ce %>%
  group_by(sampling_method.1)%>%
  summarise(Total_Obs = n())


#write_csv(SD_Gears, "SD_gears_species.csv")
#write_csv(SD_Gears_2, "SD_gears.csv")
####
rm(SD_Gears, SD_Gears_2)
```

