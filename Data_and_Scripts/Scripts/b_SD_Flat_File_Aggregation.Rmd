---
title: "SD_Flat_File_Aggregation"
author: "Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Preamble

##Instructions
This code is written in chunks that each accomplish a task moving towards the goal of a file for each state that encompasses all fish observations made/shared with us. The structure of that observation-level data should be one row per individual fish. By joining these data to the effort info, we will be able to filter and aggregate the data in flexible ways, always bringing along info on how much effort it took to catch each the fish obs (or set of fish).

The data explainer sticks .n suffixes on columns where multiple fields from one of the datastes has multiple cols that match that field (i.e., date.1, date.2, date.3). The naming conventions for cols uses "_" and no spaces.

After loading packages, the data from each state will be loaded into the WS and renamed according to the mapping of old colnames to new colnames in the data explainer. Next, the files should be explored a bit, and the script should identify files that will not be used, but instead get removed from the workspace. After this initial exploration fo what's there, the files should be restructured and munged into the obs-level format described above. When this is done, subsequent blocks should conduct some baseline additional QC should be done to verify the product of the munging is as-expected. Finally, the script should tidy up an remaining column or field formatting (e.g., species uses common names, no spaces, but "_"), and drop an unneeded columns. 


A basic guide to columns we expect to see in a observation level data are as follows:

LOCATION INFORMATION:
state - 
county - county associate with the wb in the state data
lake_name - common lang name of the lake
lake_id - usually a local id specific to the state contributing the data
nhdhr.id - This column is usually added towards the end of the script based on state lake_ids using the mwlaxeref (Paul Frater) package from here: https://drive.google.com/drive/u/1/folders/1HURmPTtufVzI0aqn7D8MpKdL5B8atCL5

SURVEY INFORMATION:
date_clean - usually multiple dates are submitted with each fish (e.g., collection date, survey end date). Use the date of the survey as the primary date for each fish observation, generating a date_clean column
survey_type - this is often specified in the data, and sometimes helps to filter out which data are useful for any given purpose (e.g., research survey, fishkill check)
survey_id - in some states this is a provided variable used as a key to each "survey." Ususally a "survey" is multiple gears on a single lake on a single date (often surveys might run multiple consecutive dates, but only one date is reported )
sampling_method - This is a gear field, and often includes wide ranging gears and sometimes very specific gears
total_effort.1 - This should be a numeric field with only the qty of effort
effort_units.1 - paired with total_effort.1, defines units for numeric
nothing_caught - specifies that nothing was caught in this effort (species will also be NA)
target_species - what was the species being targeted in the survey?
effort_ident - This is a field we add, it is a unique key for each effort unit that we have data for(usually a gear within a survey). For example, a data user could get cpue by counting all fish within a group_by(effort_id) or it's equivalent group_by(lake_id, date, survey_type,sampling_method) 

TAXA INFORMATION: 
species.1 - species common name
species_abbrev - State level code sometimes used in data share
length.1 - length of fish observed, numeric
length_unit.1 - units for length.1, also specify resolution if needed (e.g, cm, whole cm)
weight.1 - weight of fish obs, numeric
weight_unit.1 - units of weight.1, also specify resolution if needed (e.g, lb, whole lb)
sample_id.1 - unique id for each fish observation sometimes provided and sometimes useful for connecting to aged fish
age - age in years, numeric
aging_structure - what was used to determine age?
young_of_year - was the fish a YOY (i.e. hatched <365d before surveyed)
sex - sex of fish (male, female, unknown, NA)

SOURCE FILE INFORMATION: These columns come in with each dataset from the data explainer and we leave them in the product so that we could hunt down issues we find a bit more easily. 

original_file_name.1_effort - name of effort file that was used to generate data in this row
original_file_name.1_indivfish - name of individual fish file that was used to generate data in this row
original_file_name.1_[...]

FLAGS AND ISSUES:
flag - this column contains a character string with issues describing each row, each issue separated with a comma. Use mutate(flag = paste(flag, "new issue description", sep = ",")) to add to this column without overwriting other issues already specified.

## To do list:



1. Amend original file names in data explainer to recognize sheet from which each dataset came, carry these through to the original filenames concat column.






##Libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)
library(tidyverse)
library(data.table)
library(mwlaxeref)

options(scipen = 999)
```


##Data
This could readily be changed into a function that takes a filepath and returns files into environment.
* note Holly has to change file paths to "D" 
```{r}
#generate a file list to import
files_list <- list.files(path = "D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/SD_Data/sd_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list



#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("D:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/SD_Data/sd_raw_disaggregated_data/",
                                          files_list[i]),
         drop = c("V1")))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  #if the file is not in the data explainer, don't try to rename it:
  if(filei %in% cde$new_file_name) {
    print("renaming with data explainer")
  } else {next}
  
  
  
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    data.table::transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 



```

#Data Review
```{r}

#clean out some file import stuff-
rm(cde,names, unusedbits, cols, filei, files_list, i, maxn, n)

# review each dataset that we have, strategizing about how you'll use them to develop a obs-level file
# consider things like species scope (do I need to restrict all input data to just the 8ish game species?), file organization (is this file already in a obs-level format or is it a count of each species/size that I should uncount()?), linking keys (is there a fish obs ID in the age data that I can use to link to the fish observations data?), and what things (posisbly whole datasets) are unneeded for our work, here. I have left the Michigan work in here to give you an idea of what I did:

# indiv fish records are here (bind brings together two datasets split to share): 
sd_catch_indivfish_2Nov2023 <- rbindlist(list(sd_catch1_indivfish_2Nov2023,sd_catch2_indivfish_2Nov2023))
  rm(sd_catch1_indivfish_2Nov2023, sd_catch2_indivfish_2Nov2023)
  names(sd_catch_indivfish_2Nov2023)
  
  
#HOlly's exploration for filter workshop

sd_catch_indivfish_2Nov2023 %>%
  group_by(garbage_bin_notes.1)%>%
  summarise(total = n())%>%
  filter(total > 1)%>%
  print(n=nrow(.)) #appears that this "object ID" is an individual ID for each fish

sd_catch_indivfish_2Nov2023 %>%
  group_by(notes.1)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #serial number not unique to the fish, no helpful notes

sd_catch_indivfish_2Nov2023 %>%
  group_by(mark_recap_data_notes.1)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #mark recap data notes just lists the type of tag used

sd_catch_indivfish_2Nov2023 %>%
  group_by(mark_recap_data_notes.2)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #mark recap data notes 2 is just the number on the tag

sd_catch_indivfish_2Nov2023 %>%
  group_by(mark_recap_data_notes.3)%>%
  summarise(total = n())%>%
  print(n=nrow(.)) #mark recap notes 3 is tag color, all are null or NA, one is orange
  
###

sd_catch_indivfish_2Nov2023[ , .N , .(survey_id, species.1) ]
sd_catch_indivfish_2Nov2023[ , .N , .(species.1) ] %>% print( n = nrow(.))

sd_catch_indivfish_2Nov2023[species.1 %in% c("<Null>", "None", "Uncoded"), species.1 := "Unknown" ]



#then there's the batch count data:
names(sd_catch3_batchcounts_2Nov2023)

sd_catch3_batchcounts_2Nov2023[ , .N , .(species.1) ] %>% print( n = nrow(.))
sd_catch3_batchcounts_2Nov2023[species.1 %in% c("<Null>", "None", "Uncoded"), species.1 := "Unknown" ]


sd_catch3_batchcounts_2Nov2023[, .N , .(survey_id, species.1) ]
  sd_catch3_batchcounts_2Nov2023[, .N , .(survey_id, species.1) ][N>1]
#based on this format I would not expect to see multi batch counts for one survey...
sd_catch3_batchcounts_2Nov2023[sd_catch3_batchcounts_2Nov2023[, .N , .(survey_id, species.1) ][N>1] , on = .(survey_id, species.1) , ][order(survey_id,species.1)]
#some are because of nulls   
#remove null records 
  sd_catch3_batchcounts_2Nov2023[total_count.1 == 0 , .N  ,]
  sd_catch3_batchcounts_2Nov2023[total_count.1 == 1 , .N  ,]
  sd_catch3_batchcounts_2Nov2023 <-  sd_catch3_batchcounts_2Nov2023[ total_count.1 > 0 ,  ]
  

  
#okay, so there are two (or more) different vals for batch counts, let's check to make sure that the effort data are one gear per lake kinda thing on the survey_id field

  
  
#check effort data--is there more than one record per survey_id?
sd_effort_lakesurveysdata_2Nov2023[ , .N , .(survey_id) ][N>1]
sd_effort_lakesurveysdata_2Nov2023[survey_id == "{E02AD9C6-0877-475B-9BAA-1973CCA1DCFB}"] #only one case..we're going to drop that second record
sd_effort_lakesurveysdata_2Nov2023 <- sd_effort_lakesurveysdata_2Nov2023[ !garbage_bin_notes.1 ==  "OBJECTID *:374903" , ,]
#okay, but note that the survey_id is not unique at the lakeXgear level
sd_effort_lakesurveysdata_2Nov2023[ , length(unique(survey_id)) , .( lake_name.1, date(date.1)) ]#
# so that suggests that the survey_id is equivalent to our sub_effort_ident (i.e. we will need to generate a total_effort_ident filed that sums all sub_effort_idents in a lakeXgear)

#back to the batchcounts--we can see here that for a single "survey_id" (aka sub_effort_ident) we have mutliple batchcounts. That is okay, but it begs the Q of why? Are there three staff each counting their own tub of fish? Are these batches associated with a length bin or something (some data we can see that would key apart the mutliple batches in a single gear)? We'll ask Amy Gebhard about this!
#expand the batchcounts

sd_batchcounts_uncounted <- uncount(sd_catch3_batchcounts_2Nov2023, weights = total_count.1, .remove = T, .id = "uncount_ident")
rm(sd_catch3_batchcounts_2Nov2023)


sd_catch_indivfish_2Nov2023 <- rbindlist(list(sd_batchcounts_uncounted,sd_catch_indivfish_2Nov2023), fill = TRUE)
  rm(sd_batchcounts_uncounted)

#clean up species names to ease merge to age
sd_catch_indivfish_2Nov2023[ ,  species.1 := tolower(species.1)]


sd_length_age_2Nov2023[ , species.1 := gsub(" ","_", tolower(species.1)) ,]
  sd_length_age_2Nov2023[ , .N , species.1][order(species.1)]

#check lenth age species
sd_length_age_2Nov2023[ , .N , .(species.1) ] %>% print( n = nrow(.))
sd_length_age_2Nov2023[species.1 %in% c("<null>", "none", "uncoded"), species.1 := "unknown" ]
  sd_length_age_2Nov2023[ is.na(species.1), species.1 := "unknown"]
# sd_length_age_2Nov2023 <-    sd_length_age_2Nov2023[!(species.1 == "<null>" | is.na(species.1))]
  sd_catch_indivfish_2Nov2023[species.1 == "<null>" | is.na(species.1), .N]
# sd_catch_indivfish_2Nov2023 <-    sd_catch_indivfish_2Nov2023[!(species.1 == "<null>" | is.na(species.1))]

  
sd_catch_indivfish_2Nov2023[ , species.1 := gsub("_$", "" , gsub("__", "_", gsub(" ","_",gsub(paste(c("[(]", "[)]"), collapse = "|") , "_" , tolower(species.1))))) ,  ]  
  sd_catch_indivfish_2Nov2023[ , .N , species.1] %>% print(n = nrow(.))
  sd_catch_indivfish_2Nov2023[ , sort(unique(species.1))]

# the "lamprey family" aged critter does not have a parallel in the catch file
any(sd_length_age_2Nov2023[,unique(species.1) , ] %in% sd_catch_indivfish_2Nov2023[ , unique(species.1) ,] == F)
  sd_length_age_2Nov2023[,unique(species.1) , ][sd_length_age_2Nov2023[,unique(species.1) , ] %in% sd_catch_indivfish_2Nov2023[ , unique(species.1) ,] == F]

#check the survey_ids & scope the merge
  sd_length_age_2Nov2023[ ,unique(survey_id) ,] %in% sd_effort_lakesurveysdata_2Nov2023[ , unique(survey_id) ,]
  
#looks like the aged fish exist ONLY in that table (not in the catch tables!) Thats badass! Here are two randomly pulled examples
  sd_catch_indivfish_2Nov2023[survey_id == "{2F223C1F-2ADC-4A3B-BFB6-AC7E23C545BB}" & species.1 =="walleye" ,  , ]
    sd_length_age_2Nov2023[survey_id == "{2F223C1F-2ADC-4A3B-BFB6-AC7E23C545BB}" & species.1 =="walleye" ,  , ]
  sd_catch_indivfish_2Nov2023[survey_id == "{136411C8-5A7B-4A3A-AF99-DFBB0CDC11CA}" & species.1 =="black_crappie" ,  , ]
    sd_length_age_2Nov2023[survey_id == "{136411C8-5A7B-4A3A-AF99-DFBB0CDC11CA}" & species.1 =="black_crappie" ,  , ]
#this seems to apply even to the cases where there seems to be a str pulled from a fish but no age reported. DAYUM Amy, this looks super clean
  sd_catch_indivfish_2Nov2023[survey_id == "{710D141C-0109-4B79-B160-4527E93F23BC}" & species.1 =="yellow_perch" ,  , ]
    sd_length_age_2Nov2023[survey_id == "{710D141C-0109-4B79-B160-4527E93F23BC}" & species.1 =="yellow_perch" ,  , ]
#look into these structure-pulled-unaged-records and tidy up some nulls
  sd_catch_indivfish_2Nov2023[!is.na(aging_structure.1) , .N , .(aging_structure.1, uncount_ident)] #recall that all the batched fish had an uncount ident assigned to them!
    sd_catch_indivfish_2Nov2023[ aging_structure.1 == "otolith"]
    sd_catch_indivfish_2Nov2023[ aging_structure.1 %in% c("<Null>", "null"), aging_structure.1 := NA]
  
#now thin out the aged fish data a bit and rbind to the catch data
sd_length_age_2Nov2023 <- sd_length_age_2Nov2023[ , .(survey_id, species.1, length.1, length_unit.1, weight.1, weight_unit.1, age, aging_structure.1, sex, original_file_name.1 ) , ]

sd_catch_indivfish_2Nov2023 <- sd_catch_indivfish_2Nov2023[ ,.(survey_id, species.1, length.1, length_unit.1, weight.1, weight_unit.1, aging_structure.1, sex, original_file_name.1, notes.1)]

sd_catch_indivfish_2Nov2023 <- rbindlist(list(sd_catch_indivfish_2Nov2023, sd_length_age_2Nov2023), fill = TRUE)

rm(sd_length_age_2Nov2023)

#check out and pare down the effort data:
sd_effort_lakesurveysdata_2Nov2023[ , .N , .(sampling_method.1, sampling_method_abbrev) ][order(sampling_method_abbrev)] #no need to keep the samping method abbrev

# 
# ## HOlly exploration for filtering workshop
# Notes_SD_Effort <- sd_effort_lakesurveysdata_2Nov2023 %>%
#   group_by(garbage_bin_notes.3)%>%
#   summarise(Number_of_instances = n())%>%
#   mutate(garbage_bin_notes.3 = str_replace_all(garbage_bin_notes.3, "\\\\", "/"))%>%
#   separate_wider_delim(garbage_bin_notes.3, delim = "/", names =c("pt1", "pt2", "pt3", "pt4", "pt5", "pt6", "pt7", "pt8", "pt9", "pt10", "pt11", "pt12", "p13"), too_few = "align_end") #may contain survey purpose info or filtering info
# 
# #write_csv(Notes_SD_Effort, "SD_effort_file_path.csv")
# ####
# sd_effort_lakesurveysdata_2Nov2023 <- sd_effort_lakesurveysdata_2Nov2023[ , .(survey_id, date.1, lake_id, lake_name.1, location_notes.1, county,  site_id.1, site_id.2, sampling_method, total_effort_1, effort_units.1, original_file_name.1) , ]

sd_effort_lakesurveysdata_2Nov2023 <- sd_effort_lakesurveysdata_2Nov2023[ , .(survey_id, date.1, lake_id, lake_name.1, location_notes.1, county,  site_id.1, site_id.2, sampling_method.1, total_effort_1, effort_units.1, original_file_name.1) , ]


#check lake list info required?
sd_effort_lakesurveysdata_2Nov2023[ , .N , .(lake_id,lake_name.1)]
  sd_effort_lakesurveysdata_2Nov2023[ , unique(lake_id) ,]
sd_effort_lakesurveysdata_2Nov2023[ is.na(lake_id)] #one missing lake ID
  sd_lake_ID_list_11Jul2022[str_detect(lake_name.1,"lmore")] #not in the lake list that Holly made either
#we dont need the lake list
  rm(sd_lake_ID_list_11Jul2022)

  
#drop records for which we have no way to connect a fish to a location
  sd_catch_indivfish_2Nov2023[ str_detect(survey_id, "ull") ,.N , ]
  # sd_catch_indivfish_2Nov2023 <- sd_catch_indivfish_2Nov2023[ !str_detect(survey_id, "ull") , , ]
  
  
  sd_effort_lakesurveysdata_2Nov2023[str_detect(survey_id, "ull") , , ]
  
#now we merge the effort and catch:
  
  sd_merged_ce <- merge(sd_catch_indivfish_2Nov2023, sd_effort_lakesurveysdata_2Nov2023, by = ("survey_id"), all = TRUE, suffixes = c("indivfish","effort" ))
  
#Holly's exploration for filter workshop
SD_Gears <- sd_merged_ce %>%
  group_by(sampling_method.1, species.1)%>%
  summarise(Total_Obs = n())

SD_Gears_2 <- sd_merged_ce %>%
  group_by(sampling_method.1)%>%
  summarise(Total_Obs = n())


write_csv(SD_Gears, "SD_gears_species.csv")
write_csv(SD_Gears_2, "SD_gears.csv")
####
  
  #missing data
  sd_merged_ce[is.na(lake_id), .N, .(survey_id, lake_id, lake_name.1) ]
  # sd_merged_ce <- sd_merged_ce[ !is.na(date.1) ]
  

  
  
#need a total effort ident in here:  
#can see multiple sampling events for one gear over a short span of dates
sd_merged_ce[ , .N , .(lake_id, lake_name.1, date(date.1), sampling_method.1) ][order(lake_id, date)]
  sd_merged_ce[lake_id == "ANR-Lake-4-000" , .N,  .(lake_id, lake_name.1, date(date.1), sampling_method.1) ][order(lake_id, date)]
#could roll together lakeXgearXyear?
sd_merged_ce[lake_id == "ANR-Lake-4-000" , .N,  .(lake_id, lake_name.1, year(date.1), sampling_method.1) ][order(lake_id, year)]


setnames(sd_merged_ce, old =  c("survey_id", "total_effort_1", "effort_units.1"),  new =  c("sub_effort_ident", "sub_effort_1", "sub_effort_units"))

sd_merged_ce[ ,`:=` (total_effort_ident = .GRP)  , .(lake_id, lake_name.1, year(date.1), sampling_method.1) ]

#any no catch data?
  sd_merged_ce[ , sum(!is.na(species.1)) , .(sub_effort_ident) ] #yes! every sub_effort_ident with a zero here is a no catch
sd_merged_ce[ , sub_nothing_caught := sum(!is.na(species.1))==0 , sub_effort_ident ]

#now calc effort for each total_effort_ident
sd_merged_ce[ , length(unique(sub_effort_ident)) , .(total_effort_ident)  ]
tot_eff_1 <- sd_merged_ce[ , .N , .(total_effort_ident,sub_effort_ident,sub_effort_1)  ][ ,  .("total_eff_1" = sum(sub_effort_1)) ,.(total_effort_ident)]
sd_merged_ce[tot_eff_1, on = ("total_effort_ident"), total_effort_1 := total_eff_1 , ]

#add a total effort nothing caught
sd_merged_ce[ , all(sub_nothing_caught)  , .(total_effort_ident)][ ,summary(V1)]
sd_merged_ce[ , total_effort_nothing_caught := all(sub_nothing_caught)  , .(total_effort_ident)]

#note here that because we have allowed NA species fish records to stay in the database as NAs, then used species as our gen of nothing caught, we have essentially said those records can stay in, but that they dont count towards catch. 



# units from file
sd_effort_lakesurveysdata_2Nov2023[ , summary(total_effort_1) , sampling_method.1 ] %>% print(n = nrow(.))

gear_units <- fread("Data_and_Scripts/Data/input/SD_gear_unitizer.csv")

sd_merged_ce[gear_units, on = .(sampling_method.1 = sampling_method) , total_effort_1_units :=  sampling_method_2 ]

sd_merged_ce[ , .N , .(sampling_method.1, total_effort_1_units) ]
rm(gear_units, tot_eff_1)


unique(sd_merged_ce$lake_id)

# mwlaxeref::lake_id_xref %>% filter(state == "sd") %>% 
#   group_by(local.id) %>% count() %>% print(n = nrow(.))

```

## Revise after lake loc import sucess
```{r}

#most have matches in the lake locations file
sd_locs <- sd_lakelocations_1Dec2023

sd_merged_ce[, unique(lake_id) , ]%in%sd_locs$lake_id

sd_merged_ce[sd_locs, on = .(lake_id = lake_id ) , `:=` ("lat_unspec" = lat_unspec, "lon_unspec" = lon_unspec, "lakesize" = c_lakesize, "lakesize_units" = c_lakesize_units) ]

# export a few unmatched ones for stste verification:
fwrite(sd_merged_ce[is.na(lat_unspec), .N , .(lake_id, lake_name.1, county, location_notes.1)], file = "Data_and_Scripts/Data/output/sd_lakes_loc_unknown.csv")

#now try a lakenameXcounty combo:
sd_merged_ce[sd_locs, on = .(lake_name.1 = lake_name.1, county = county ) , `:=` ("lat_2" = lat_unspec, "lon_2" = lon_unspec) ]
#move these into position, remove old, summarize
sd_merged_ce[is.na(lon_unspec), `:=` ("lat_unspec" = lat_2, "lon_unspec" = lon_2) ]
sd_merged_ce[ ,`:=` (lat_2= NULL, lon_2 = NULL)]
sd_merged_ce[ , .N , is.na(lat_unspec) ]

#cleaning up some unit columns
sd_merged_ce <- sd_merged_ce %>% 
  mutate(lakesize_units = case_when(!is.na(lakesize) ~ "acres",
                                    TRUE ~ NA),
         length_unit.1 = case_when(!is.na(length.1) ~ "mm",
                                    TRUE ~ NA),
         weight_unit.1 = case_when(!is.na(weight.1) ~ "g",
                                    TRUE ~ NA))
#dates
sd_merged_ce <- sd_merged_ce %>% 
  arrange(date.1) %>% 
  mutate(date.1 = format(date.1, "%Y-%m-%d"),
         date.1 = as.Date(date.1),
         date_sub_effort_ident = date.1,
         date_sample = as.Date(NA)) %>% 
  group_by(total_effort_ident) %>% 
  mutate(date_total_effort_ident = first(date.1)) %>% 
  ungroup() %>% 
  group_by(lake_id, year(date.1)) %>% 
  mutate(date_survey = first(date.1)) %>% 
  ungroup()
```

## Match Columns to parquet schema
```{r}
# clean up these names and match overall schema
#need to 1. fix current cols to match 2. add missing cols 3.drop all extras 
sd_merged_ce <- clean_names(sd_merged_ce) #clean up names a touch

sd_merged_ce <- as.data.table(sd_merged_ce)

#making site_id_1
sd_merged_ce[ , site_id_1:= paste(site_id_1,site_id_2, sep = ":") , ]
#making original file name
sd_merged_ce[ , original_file_name := paste(original_file_name_1indivfish, original_file_name_1effort , sep = ":") , ]

#Denver's touch up on column names
sd_merged_ce_trial <- sd_merged_ce %>% 
  mutate(state = "South Dakota",
         lake_name = lake_name_1,
         nhdhr_id = as.character(NA),
         year = year(date_1),
         month = month(date_1),
         survey_id = as.character(NA),
         survey_type = as.character(NA),
         survey_type_2 = as.character(NA),
         survey_type_3 = as.character(NA),
         survey_type_4 = as.character(NA),
         sampling_method = sampling_method_1,
         sampling_method_2 = as.character(NA),
         gear_data_notes = as.character(NA),
         target_species = as.character(NA),
         target_species_2 = as.character(NA),
         total_effort_ident = as.character(total_effort_ident),
         total_effort_2 = as.numeric(NA),
         total_effort_3 = as.numeric(NA),
         total_effort_1_units = as.character(NA),
         total_effort_2_units = as.character(NA),
         total_effort_3_units = as.character(NA),
         water_temp = as.numeric(NA),
         water_temp_units = as.character(NA),
         water_clarity = as.numeric(NA),
         water_clarity_units = as.character(NA),
         lat_start = as.numeric(NA),
         lon_start = as.numeric(NA),
         lat_end = as.numeric(NA),
         lon_end = as.numeric(NA),
         site_id = site_id_1,
         sub_effort_1_units = sub_effort_units,
         sub_effort_2 = as.numeric(NA),
         sub_effort_2_units = as.character(NA),
         sub_effort_nothing_caught = sub_nothing_caught,
         length_bin = as.character(NA),
         length_bin_unit = as.character(NA),
         aging_structure_2 = as.character(NA),
         weight_1 = as.numeric(weight_1),
         batch_weight = as.character(NA),
         batch_weight_unit = as.character(NA),
         age_class = as.character(NA),
         flag = as.character(NA),
         original_file_names = original_file_name,
         ind_fish_ident = as.character(NA),
         area_group = as.character(NA),
         waterbody_type = as.character(NA),
         obs_id = as.character(row_number())) %>% 
  select(state,
         county,
         lake_name,
         lake_id,
         nhdhr_id,
         date_survey,
         date_total_effort_ident,
         date_sub_effort_ident,
         date_sample,
         year,
         month,
         survey_id,
         survey_type,
         survey_type_2,
         survey_type_3,
         survey_type_4,
         sampling_method,
         sampling_method_2,
         gear_data_notes,
         target_species,
         target_species_2,
         total_effort_ident,
         total_effort_1,
         total_effort_2,
         total_effort_3,
         total_effort_1_units,
         total_effort_2_units,
         total_effort_3_units,
         total_effort_nothing_caught,
         water_temp,
         water_temp_units,
         water_clarity,
         water_clarity_units,
         lat_start,
         lon_start,
         lat_end,
         lon_end,
         site_id,
         sub_effort_ident,
         sub_effort_1,
         sub_effort_1_units,
         sub_effort_2,
         sub_effort_2_units,
         sub_effort_nothing_caught,
         species_1,
         length_1,
         length_unit_1,
         length_bin,
         length_bin_unit,
         age,
         aging_structure_1,
         aging_structure_2,
         weight_1,
         weight_unit_1,
         batch_weight,
         batch_weight_unit,
         sex,
         age_class,
         flag,
         original_file_names,
         ind_fish_ident,
         lakesize,
         lakesize_units,
         area_group,
         lat_unspec,
         lon_unspec,
         waterbody_type,
         location_notes_1,
         notes_1,
         obs_id)
#things to do
#1. make sure the dates are the appropriate level
#3. ensure I am keeping the correct sampling method (see code below)

#exploring dates and subefforts
dates <- sd_merged_ce_trial %>% 
  distinct(total_effort_ident, sub_effort_ident, site_id, .keep_all = T) %>% 
  select(total_effort_ident, sub_effort_ident, site_id, date_survey, date_total_effort_ident, date_sub_effort_ident)
rm(dates)
#total effort 3992 is a good example of how the dates are stuctured 
#survey date is the first sampling within a lake-year
#total effort ident date is the first date from the sub effort date
#sub effort ident date is the date the was reported - when the net was lifted


#more gear corrections
sd_gears <- fread( file = "Data_and_scripts/Data/input/ia_sd_gear_newnames2.csv") #read in 

sd_gears <- sd_gears[state == "South Dakota"]

sd_merged_ce_trial[ , .N , sampling_method_2 ]
sd_merged_ce_trial[ , sampling_method_2 := as.character(sampling_method_2) , ]

sd_merged_ce_trial[sd_gears, on = .(sampling_method = old_gear), sampling_method_2 := new_gear]

gear <- sd_merged_ce_trial %>% 
  distinct(total_effort_ident, sub_effort_ident, .keep_all = T) %>% 
  group_by(sampling_method, sampling_method_2) %>% 
  count()
rm(gear)

glimpse(sd_merged_ce_trial)
```


##export as Parquet
```{r}
write_dataset(dataset = sd_merged_ce_trial, path = "Data_and_Scripts/Data/output/sd_file_arrow")

sd_data <- open_dataset(sources = "Data_and_Scripts/Data/output/sd_file_arrow/")

glimpse(sd_data)

```

                
 


# Review & QC datasets
```{r}
#below is some code Mike to match schema


# then open into excel an align with the schema (see these IA example files)
# fwrite(as.data.table(names(sd_merged_ce)), file = "Data_and_scripts/Data/output/sd_names.csv")

# use the exported csv to align with the column names in the column schema GSheet (https://docs.google.com/spreadsheets/d/1zfevASMxRMxMYNWm3Hq1yR2Qk0zcJV_JnO0LBeODaPQ/edit#gid=0) . 
# - if you delete a column from your state's names in the sheet, the code below will drop those cols automatically from the data product.
# - put an NA in the table in each place where your data have no equivalent col and the code below will fill those with NAs in the data product
# - keep the order of the table in line with the column schema GSheet and that will automatically reorder your data to match the schema GSheet
# after aligning the names with the schema bring that back in (note that the )
sd_renamer <- fread( file = "Data_and_scripts/Data/input/sd_renamer.csv") #read in 
setnames(sd_renamer, "fields in all parquet", "schema")
sd_renamer[south_dakota == "", south_dakota := NA ]


#drop extras
dropcols <- names(sd_merged_ce)[!(names(sd_merged_ce) %in% sd_renamer[,  south_dakota])]
sd_merged_ce[ , (dropcols) := NULL , ]



#rename ia to fit the schema
sd_renamer[match(names(sd_merged_ce),sd_renamer[,south_dakota]), schema]

setnames(sd_merged_ce, 
         old = names(sd_merged_ce),
         new = sd_renamer[match(names(sd_merged_ce),sd_renamer[,south_dakota]), schema])


#missing cols
sd_renamer[,schema] %in% names(sd_merged_ce)
names(sd_merged_ce) %in% sd_renamer[,schema]

#add misssing names:
addcols <- sd_renamer[is.na(south_dakota) , schema ,]
sd_merged_ce[ ,(addcols) := NA , ]

newcolorder <- sd_renamer[ ,schema]


setcolorder(sd_merged_ce, newcolorder)


#generate an obs_id

sd_merged_ce[ , .N , obs_id ]
sd_merged_ce[ , obs_id := .I , ]

#set state
sd_merged_ce[ , state := as.character(state) , ]
sd_merged_ce[ , state := "South Dakota"  , ]

#get the surveys and gears and show how many surveys and how many of each species was caught in those surveys 

#total effort review:
sd_data %>% 
  group_by(total_effort_ident,total_effort_1,total_effort_1_units, sampling_method, year(date_total_effort_ident), lake_name, lake_id) %>% 
  count() %>%
  collect()
  

# ensure nothing caught is assigned at the total effort ident level
sd_data %>%
  group_by(total_effort_ident) %>% 
  summarise(n_distinct(total_effort_nothing_caught)) %>% 
  collect()

sd_data %>% 
  distinct(total_effort_ident, sampling_method) %>% 
  group_by(total_effort_ident, sampling_method) %>% 
    count() %>% 
    collect() %>% 
  filter(n >1)
#silly check but its good there aren't any tots effort idents with more than one sampling method

sd_data %>% 
  group_by(total_effort_ident,total_effort_1,total_effort_1_units, sampling_method) %>%
  summarise(walleye = sum(species_1 == "walleye", na.rm = T),
            yellow_perch = sum(species_1 == "yellow_perch", na.rm = T),
            black_crappie = sum(species_1 == "black_crappie", na.rm = T),
            bluegill = sum(species_1 == "bluegill", na.rm = T),
            smallmouth_bass = sum(species_1 == "smallmouth_bass", na.rm = T),
            largemouth_bass = sum(species_1 == "largemouth_bass", na.rm = T),
            northern_pike = sum(species_1 == "northern_pike", na.rm = T),
            lake_whitefish = sum(species_1 == "lake_whitefish", na.rm = T)
            ) %>%
  collect()

  sd_data %>% 
  group_by(state, sampling_method, sampling_method_2) %>%
  summarise(
    n_effort_idents = n_distinct(total_effort_ident),
    n_walleye = sum(species_1 == "walleye", na.rm = T),
    n_yellow_perch = sum(species_1 == "yellow_perch", na.rm = T),
    n_black_crappie = sum(species_1 == "black_crappie", na.rm = T),
    n_bluegill = sum(species_1 == "bluegill", na.rm = T),
    n_smallmouth_bass = sum(species_1 == "smallmouth_bass", na.rm = T),
    n_largemouth_bass = sum(species_1 == "largemouth_bass", na.rm = T),
    n_northern_pike = sum(species_1 == "northern_pike", na.rm = T),
    n_lake_whitefish = sum(species_1 == "lake_whitefish", na.rm = T)
  ) %>%
    collect() %>% 
    {. ->> sd_sample_sz}



       

```


# Import/Export files

```{r}

# 
# #save to disk:
# 
# # saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# # mi_catch_eff_merge <- readRDS(file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# 
# 
# str(mi_catch_eff_merge)
# 
# 
# mi_catch_eff_merge <- as_arrow_table(mi_catch_eff_merge)
# 
# write_dataset(dataset = mi_catch_eff_merge, path = "Data_and_Scripts/Data/output/mi_file_arrow")
# 
# mi_data <- open_dataset("Data_and_Scripts/Data/output/mi_file_arrow")
# 
# glimpse(mi_data)

```




